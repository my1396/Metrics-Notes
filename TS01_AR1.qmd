---
title: "AR(1)"
format: html
---

```{r}
#| include: false
#| echo: false
#| 
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(xts)
# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message=FALSE, fig.align="center", fig.pos = "H")
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
# add labels to multi-panel figures
put.fig.letter <- function(label, location="topleft", x=NULL, y=NULL, 
                           offset=c(0, 0), ...) {
    # offset[1]: negative to move to the left; positive to move to the right
    # offset[2]: negative to move downward; positive to move upward
    if(length(label) > 1) {
        warning("length(label) > 1, using label[1]")
    }
    if(is.null(x) | is.null(y)) {
        coords <- switch(location,
                         topleft = c(0.015,0.98),
                         topcenter = c(0.5525,0.98),
                         topright = c(0.985, 0.98),
                         bottomleft = c(0.015, 0.02), 
                         bottomcenter = c(0.5525, 0.02), 
                         bottomright = c(0.985, 0.02),
                         c(0.015, 0.98) )
    } else {
        coords <- c(x,y)
    }
    this.x <- grconvertX(coords[1] + offset[1], from="nfc", to="user")
    this.y <- grconvertY(coords[2] + offset[2], from="nfc", to="user")
    text(labels=label[1], x=this.x, y=this.y, xpd=T, ...)
}
```

Unlike linear regression, which typically deals with cross-sectional data, time series regression in two ways:

1. Observations on a given unit, observed over a number of time periods, are likely to be **correlated**.
2. **Natural ordering to time**. If one shuffles time series observations, there is a danger of confounding what is their most distinguishing feature: the possible existence of dynamic relationships between variables.


Time series data often display autocorrelation, or serial correlation of the disturbances across periods.

If you plot the residuals and observe that the effect of a given disturbance is carried, at least in part, across periods, then it is a strong signal of serial correlation. It's like the disturbances exhibiting a sort of "memory" over time.


--------------------------------------------------------------------------------

## Reasons for Delayed Effects and Autocorrelation

Consider the dynamic regression model

$$
y_t = \beta_1 + \beta_2x_t + \beta_3x_{t-1} + \gamma y_{t-1} + \varepsilon_t.
$$

Autocorrelation in the error can arise from an autocorrelated omitted variable, or it can arise if a dependent variable $y$ is autocorrelated and this autocorrelation is not adequately explained by the $x$'s and their lags that are included in the equation.

There are several reasons why lagged effects might appear in an empirical model.

- In modeling the response of economic variables to policy stimuli, it is expected that there will be possibly long lags between policy changes and their impacts. The length of lag between changes in monetary policy and its impact on important economic variables such as output and investment has been a subject of analysis for several decades.

- Either the dependent variable or one of the independent variables is based on expectations. Expectations about economic events are usually formed by aggregating new information and past experience. Thus, we might write the expectation of a future value of variable $x$, formed this period, as

    $$
    x_t = \E_t [x^*_{t+1} \mid z_t, x_{t-1}, x_{t-2}, \ldots] = g(z_t, x_{t-1}, x_{t-2}, \ldots) .
    $$
    
    For example, forecasts of prices and income enter demand equations and consumption equations. 

- Certain economic decisions are explicitly driven by a history of related activities.For example, energy demand by individuals is clearly a function not only of current prices and income, but also the accumulated stocks of energy using capital. Even energy demand in the macroeconomy behaves in this fashionâ€“the stock of automobiles and its attendant demand for gasoline is clearly driven by past prices of gasoline and automobiles. Other classic examples are the dynamic relationship between investment decisions and past appropriation decisions and the consumption of addictive goods such as cigarettes and theater performances.

--------------------------------------------------------------------------------

## AR(1) Visualization

The first-order autoregressive process, denoted AR(1), is
$$
\varepsilon_t = \rho \varepsilon_{t-1} + w_t
$$
where $w_t$ is a strictly stationary and ergodic white noise process with 0 mean and variance $\sigma^2_w$. 

```{r}
#| echo: false
#| include: false

n <- 100
eff_seed <- sample(1:2^15, 1)
eff_seed <- 14375
set.seed(eff_seed)
print(sprintf("Seed for session: %s", eff_seed))
w <- rnorm(n, sd=20)
f_name <- "images/white_noise.png"
ppi <- 300
png(f_name, width=9.4*ppi, height=4.61*ppi, res=ppi)
plot(w, type='l', main=TeX("White noise, $\\sigma=20$"), 
     ylab=TeX("$w_t$"), xlab="Time", cex.lab=1.5)
dev.off()
```

```{r fig-WN, echo=FALSE, out.width = "80%", fig.cap="White noise process with $\\sigma=20$."}
include_graphics(f_name)
```


To illustrate the behavior of the AR(1) process, @fig-AR1 plots two simulated AR(1) processes. 
Each is generated using the white noise process et displayed in @fig-WN.

The plot in @fig-AR1(a) sets $\rho=0.5$ and the plot in @fig-AR1(b) sets $\rho=0.95$.

Remarks

- @fig-AR1(b) is more smooth than @fig-AR1(a). 
- The smoothing increases with $\rho$.

```{r}
#| echo: false
#| include: false

# simulate AR(1)
# rho=0.5
epsilon_1 <- NULL
epsilon_1[1] <- 1
for (i in 2:n) {
  epsilon_1[i] <- 0.5*epsilon_1[i-1] + w[i]
}
# rho=0.95
epsilon_2 <- NULL
epsilon_2[1] <- 1
for (i in 2:n) {
  epsilon_2[i] <- 0.95*epsilon_2[i-1] + w[i]
}
```

```{r}
#| echo: false
#| include: false

f_name <- "images/autocorrelated_series.png"
my.locations <- rep("topleft", 2)
png(f_name, width=9.35*ppi, height=7.12*ppi, res=ppi)
par(mfrow=c(2,1), oma=c(0,0.5,0,2), mgp=c(2,0.8,0), xpd=T )
par(mar=c(2.5, 3.5, 2, 1))
plot(epsilon_1, type="l", main=TeX("$\\rho=0.5$"), 
     xlab=" ", ylab=TeX("$\\epsilon_t$"), cex.lab=1.5 )
put.fig.letter(label="(a)", location=my.locations[1], font=2, offset = c(0.1, -0.15))
par(mar=c(3.5, 3.5, 1, 1), xpd=T)
plot(epsilon_2, type="l", main=TeX("$\\rho=0.95$"), 
     xlab="Time", ylab=TeX("$\\epsilon_t$"), cex.lab=1.5 )
put.fig.letter(label="(b)", location=my.locations[2], font=2, offset = c(0.1, -0.1))
dev.off()
```



```{r fig-AR1, echo=FALSE, out.width = "80%", fig.cap="Simulated AR(1) processes with positive $\\rho$. (a) $\\rho=0.5$, (b) $\\rho=0.95$. Each is generated useing the white noise process $w_t$ displayed in @fig-WN."}
include_graphics(f_name)
```


-----------------------------------------------------------------------------

We have seen the cases when $\rho$ is positive, now let's consider when $\rho$ is **negative**.
@fig-AR1-neg(a) shows an AR(1) process with $\rho=-0.5$, and @fig-AR1-neg(a) shows an AR(1) process with $\rho=-0.95\,.$

We see that <span style='color:#337ab7'>the sample path is very choppy when $\rho$ is negative</span>. The different patterns for positive and negative $\rho$'s are due to their <span style='color:#337ab7'>*autocorrelation functions*</span> (ACFs).

```{r echo=FALSE}
# rho=-0.5
epsilon_3 <- NULL
epsilon_3[1] <- 1
for (i in 2:n) {
  epsilon_3[i] <- -0.5*epsilon_3[i-1] + w[i]
}

# rho=-0.95
epsilon_4 <- NULL
epsilon_4[1] <- 1
for (i in 2:n) {
  epsilon_4[i] <- -1*epsilon_4[i-1] + w[i]
}
```



```{r}
#| echo: false
#| include: false

f_name <- "images/autocorrelated_series_negative-rho.png"
my.locations <- rep("topleft", 2)
png(f_name, width=9.35*ppi, height=7.12*ppi, res=ppi)
par(mfrow=c(2,1), oma=c(0,0.5,0,2), mgp=c(2,0.8,0), xpd=T )
par(mar=c(2.5, 3.5, 2, 1))
plot(epsilon_3, type="l", main=TeX("$\\rho = -0.5$"), 
     xlab=" ", ylab=TeX("$\\epsilon_t$"), cex.lab=1.5 )
put.fig.letter(label="(a)", location=my.locations[1], font=2, offset = c(0.1, -0.15))
par(mar=c(3.5, 3.5, 1, 1), xpd=T)
plot(epsilon_4, type="l", main=TeX("$\\rho = -0.95$"), 
     xlab=" ", ylab=TeX("$\\epsilon_t$"), cex.lab=1.5 )
put.fig.letter(label="(b)", location=my.locations[2], font=2, offset = c(0.1, -0.1))
dev.off()
```


```{r echo=FALSE, out.width = "80%", fig.cap="Simulated AR(1) processes with negtive $\\rho$. (a) $\\rho=-0.5$, (b) $\\rho=-0.95$. Each is generated useing the white noise process $w_t$ displayed in @fig-WN."}
#| label: fig-AR1-neg
include_graphics(f_name)
```


--------------------------------------------------------------------------------

Possible causes of serial correlation: Incomplete or flawed model specification. Relevant factors omitted from the time series regression are correlated across periods.



--------------------------------------------------------------------------------

## Mathematical Representation

Let's formulate an AR(1) model as follows:

$$
\begin{align} 
\varepsilon_t = \rho \varepsilon_{t-1} + w_t 
\end{align}
$$ {#eq-AR1}

where <span style='color:#008B45'>$w_t$ is a white noise series with mean zero and variance $\sigma^2_w$</span>. We also assume $|\rho|<1$.

We can represent the AR(1) model as a linear combination of the innovations $w_t$.


By iterating backwards $k$ times, we get

$$
\begin{aligned}
\varepsilon_t &= \rho \,\varepsilon_{t-1} + w_t \\
&= \rho\, (\rho \, \varepsilon_{t-2} + w_{t-1}) + w_t \\
&= \rho^2 \varepsilon_{t-2} + \rho w_{t-1} + w_t \\
&\quad \vdots \\
&= \rho^k \varepsilon_{t-k} + \sum_{j=0}^{k-1} \rho^j \,w_{t-j} \,.
\end{aligned}
$$
This suggests that, by continuing to iterate backward, and provided that $|\rho|<1$ and $\sup_t \text{Var}(\varepsilon_t)<\infty$, we can represent $\varepsilon_t$ as a linear process given by

$$
\color{#EE0000FF}{\varepsilon_t = \sum_{j=0}^\infty \rho^j \,w_{t-j}} \,.
$$

___

### Expectation

$\varepsilon_t$ is stationary with mean zero.

$$
E(\varepsilon_t) = \sum_{j=0}^\infty \rho^j \, E(w_{t-j}) 
$$


___

### Autocovariance

The <span style='color:#337ab7'>autocovariance function</span> of the AR(1) process is
$$
\begin{aligned}
\gamma (h) &= \text{Cov}(\varepsilon_{t+h}, \varepsilon_t) \\
&= E(\varepsilon_{t+h}, \varepsilon_t) \\
&= E\left[\left(\sum_{j=0}^\infty \rho^j \,w_{t+h-j}\right)  \left(\sum_{k=0}^\infty \rho^k \,w_{t-k}\right) \right] \\
&= \sum_{l=0}^{\infty} \rho^{h+l} \rho^l \sigma_w^2 \\
&= \sigma_w^2 \cdot \rho^{h} \cdot \sum_{l=0}^{\infty} \rho^{2l}  \\
&= {\color{red} \frac{\sigma_w^2 \cdot \rho^{h} }{1-\rho^2} }, \quad h>0 \,.
\end{aligned}
$$
When $h=0$, 
$$
{\color{red} \gamma(0) = \var(\varepsilon_t) = \frac{\sigma_w^2}{1-\rho^2}}
$$ 
is the <span style='color:#008B45'>variance of the process</span> $\text{Var}(\varepsilon_t)$.


Note that 

- $\gamma(0) \ge |\gamma (h)|$ for all $h$. Maximum value at 0 lag.
- $\gamma (h)$ is symmetric, i.e., $\gamma (-h) = \gamma (h)$

The autocovariance matrix is

$$
\bOmega = \frac{\sigma_w^2}{1-\rho^2}
\begin{bmatrix}
1 & \rho & \cdots & \rho^{n-1} \\
\rho & 1 & \cdots & \rho^{n-2} \\
\vdots & \vdots & \ddots & \vdots \\
\rho^{n-1} & \rho^{n-2} & \cdots & 1 \\
\end{bmatrix} = \bOmega (\rho)
$$

___

### Autocorrelation

The <span style='color:#337ab7'>autocorrelation function</span> (ACF) is given by

$$
{\color{#337ab7} \rho(h) = \frac{\gamma (h)}{\gamma (0)} = \rho^h },
$$
which is simply the correlation between $\varepsilon_{t+h}$ and $\varepsilon_{t}\,.$

The autocorrelation of the stationary AR(1) -@eq-AR1 is a simple geometric decay ($\abs{\rho}<1$):

- If $\abs{\rho}$ is small, the autocorrelations decay rapidly to zero with $h$
- If $\abs{\rho}$ is large (close to unity), then the autocorrelations decay moderately
- The AR(1) parameter $\abs{\rho}$ describes the persistence in the time series


Note that $\rho(h)$ satisfies the recursion
$$
\rho(h) = \rho\cdot \rho(h-1) \,.
$$

- For $\rho >0$, $\rho(h)=\rho^h>0$ observations close together are positively correlated with each other. The larger the $\rho$, the larger the correlation.
- For $\rho <0$, the sign of the ACF $\rho(h)=\rho^h$ depends on the time interval.
    - When $h$ is even, $\rho(h)$ is positive; 
    - when $h$ is odd, $\rho(h)$ is negative. 
    
    This result means that observations at contiguous time points are negatively correlated, but observations two time points apart are positively correlated.
    -  For example, if an observation, $\varepsilon_t$, is positive, the next observation, $\varepsilon_{t+1}$, is typically negative, and the next observation, $\varepsilon_{t+2}$, is typically positive. Thus, in this case, the sample path is very choppy.

Another interpretation of $\rho(h)$ is the optimal weight for scaling $\varepsilon_t$ into $\varepsilon_{t+h}$, i.e., the weight, $a$, that minimizes $E[(\varepsilon_{t+h} - a\,\varepsilon_{t})^2]\,.$



--------------------------------------------------------------------------------

## References {.unlisted .unnumbered}

-  Chap 19 Models with Lagged Variables, *Econometric Analysis*, Greene 5th Edition, pp 558.



