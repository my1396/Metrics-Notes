[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metrics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability_theory.html",
    "href": "probability_theory.html",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "1.1 Notations\n\\(\\Omega\\): A sample space, a set of possible outcomes of a random experiment.\n\\(X\\): A random variable, a function from the sample space to the real numbers: \\(X: \\Omega \\to \\R\\).\nStochastic Process\nA stochastic process is a family of random variables, \\(\\{X(t): t\\in T\\},\\) where \\(t\\) usually denotes time. That is, at every time \\(t\\) in the set \\(T\\), a random number \\(X(t)\\) is observed.\nThe state space, \\(S\\), is the set of real values that \\(X(t)\\) can take.\nYou can think of “conditioning” as “changing the sample space.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "probability_theory.html#notations",
    "href": "probability_theory.html#notations",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "Discrete-time process: \\(T=\\{0,1,2,3\\}\\), the discrete process is \\(\\{X(0), X(1), X(2), \\dots\\}\\)\nContinuous-time process: \\(T=[0, \\infty]\\) or \\(T=[0, K]\\) for some \\(K\\).\n\n\n\n\nFrom unconditional to conditional\n\\[\n  \\P (B) = \\P(B\\mid \\Omega)\n  \\]\n\\(\\Omega\\) denotes the sample space, \\(\\P (B) = \\P(B\\mid \\Omega)\\) just means that we are looking for the probability of the event \\(B\\), out of all possible outcomes in the set \\(\\Omega.\\)\nPartition Theorem\n\\[\n  \\P(A) = \\sum_{i=1}^m \\P(A\\cap B_i) = \\sum_{i=1}^m \\P(A\\mid B_i) \\P(B_i)\n  \\]\nwhere \\(B_i, i=1,\\dots,m,\\) are a partition of \\(\\Omega.\\) The intuition behind the Partition Theorem is that the whole is the sum of its parts.\n\nA partition of \\(\\Omega\\) is a collection of mutually exclusive events whose union is \\(\\Omega.\\)\nThat is, sets \\(B_1, B_2, \\dots, B_m\\) form a partition of \\(\\Omega\\) if\n\\[\n  \\begin{split}\n  B_i \\cap B_j &= \\emptyset \\;\\text{ for all $i, j$ with $i\\ne j,$} \\\\\n  \\text{and }  \\bigcup_{i=1}^m B_i &= B_1 \\cup B_2 \\cup \\dots \\cup B_m = \\Omega.\n  \\end{split}\n  \\]\nBayes’ Theorem\nBayes’ Theorem allows us to invert a conditional statement, i.e., the express \\(\\P(B\\mid A)\\) in terms of \\(\\P(A\\mid B).\\)\nFor any events \\(A\\) and \\(B\\):\n\\[\n  \\P(B\\mid A) = \\frac{\\P(A\\cap B)}{\\P(A)} = \\frac{\\P(A\\mid B)\\P(B)}{\\P(A)}\n  \\]\nGeneralized Bayes’ Theorem\nFor any partition member \\(B_j\\),\n\\[\n  \\P(B_j\\mid A) = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\P(A)} = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\sum_{i=1}^m\\P(A\\mid B_i)\\P(B_i)}\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html",
    "href": "conditional_expectation.html",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "2.1 Generalized Adam’s Law\nIdentities for conditional expectations\n\\[\n  \\E\\left[ \\E[Y \\mid g(X)] \\mid f(g(X)) \\right] = \\E[Y\\mid f(g(X))]\n\\]\nShow that the following identify is a special case of the Generalized Adam’s Law:\n\\[\n\\E[\\E[Y\\mid X,Z] \\mid Z] = \\E[Y\\mid Z]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#generalized-adams-law",
    "href": "conditional_expectation.html#generalized-adams-law",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "Proof. If we take \\(f(g(x, z)) = z\\) and \\(g(x, z) = (x, z)\\) in the generalized Adam’s Law, we get the result.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#projection-interpretation",
    "href": "conditional_expectation.html#projection-interpretation",
    "title": "2  Conditional Expectation",
    "section": "2.2 Projection interpretation",
    "text": "2.2 Projection interpretation\nConditional expectation gives the best prediction\n\nTheorem 2.1 (Conditional expectation minimizes MSE) Suppose we have random element \\(X\\in \\Xcal\\) and random variable \\(Y\\in\\R.\\) Let \\(g(x)=\\E[Y\\mid X=x].\\) Then\n\\[\ng(x) = \\underset{f}{\\arg\\min}\\, \\E(Y-f(X))^2\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\E(Y-f(X))^2 &= \\E\\left[(Y-\\E[Y\\mid X]) + (\\E[Y\\mid X]-f(X)) \\right]^2 \\quad (\\text{plus and minus } \\E[Y\\mid X]) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2 \\\\\n& \\phantom{=}\\; + 2\\E[(Y-\\E[Y\\mid X])(\\underbrace{\\E[Y\\mid X]-f(X)}_{h(X)})] \\quad \\Bigl(\\E\\bigl[ \\bigl(Y-\\E[Y\\vert X]\\bigr) h(X) \\bigr] = 0\\Bigr) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2\n\\end{split}\n\\]\nThe first term is independent of \\(f\\), and the second term is minimized by taking \\(f(x)=\\E[Y\\mid X].\\)\n\n□\n\n\nIf we think of \\(\\E[Y\\mid X]\\) as a prediction/projection for \\(Y\\) given \\(X\\), then \\((Y-\\E[Y\\mid X])\\) is the residual of that prediction.\nIt’s helpful to think of decomposing \\(Y\\) as\n\\[\nY = \\underbrace{\\E[Y\\mid X]}_\\text{best prediction for $Y$ given $X$} + \\underbrace{(Y-E[Y\\mid X])}_\\text{residual}\n\\]\nNote that the two terms on the RHS are uncorrelated, by the projection interpretation.\nSince variance is additive for uncorrelated random variables (i.e., if \\(X\\) and \\(Y\\) are uncorrelated, then \\(\\var(X+Y)=\\var(X)+\\var(Y)\\)), we get the following theorem\n\nTheorem 2.2 (Variance decomposition with projection) For any random variable \\(X\\in \\Xcal\\) and random variable \\(Y\\in \\R,\\) we have\n\\[\n\\var(Y) = \\var(\\E[Y\\mid X]) + \\var(Y-\\E[Y\\mid X])\n\\]\n\nTheorem 2.1 tells us that \\(\\E[Y\\mid X]\\) is the best approximation of \\(Y\\) we can get from \\(X.\\) We can also think of \\(\\E[Y\\mid X]\\) as a “less random” version of \\(Y,\\) since \\(\\var(\\E[Y\\vert X]) \\le \\var(Y).\\)\nWe can say that \\(\\E[Y\\mid X]\\) only keeps the randomness in \\(Y\\) that is predictable from \\(X.\\) \\(\\E[Y\\mid X]\\) is a deterministic function of \\(X,\\) so there’s no other source of randomness in \\(\\E[Y\\mid X].\\)\n\nTheorem 2.3 (Projection interpretation) For any \\(h:\\Xcal \\to \\R,\\)\n\\[\n\\E[(Y-\\E[Y\\mid X])h(X)]=0\n\\]\n\nTheorem 2.3 says that the residual of \\(\\E[Y\\mid X]\\) is “orthogonal” to every random variable of the form \\(h(X).\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#keeping-just-what-is-needed",
    "href": "conditional_expectation.html#keeping-just-what-is-needed",
    "title": "2  Conditional Expectation",
    "section": "2.3 Keeping just what is needed",
    "text": "2.3 Keeping just what is needed\n\nTheorem 2.4 For any random variables \\(X, Y\\in \\R,\\)\n\\[\n\\E[XY] = \\E[X\\E[Y\\mid X]]\n\\]\n\nOne way to think about this is that for the purposes of computing \\(\\E [XY],\\) we only care about the randomness in \\(Y\\) that is predictable from \\(X\\).\n\nProof. \n\\[\n\\begin{split}\n\\E[XY] &= \\E[\\E[XY\\mid X]] \\quad (\\text{LIE}) \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Taking out what is known})\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof1). We can show this using the projection interpretation:\n\\[\n\\begin{split}\n\\E[XY] &= \\E\\left[ X \\left(\\E[Y\\mid X] + \\underbrace{Y-\\E[Y\\mid X]}_\\text{residuals uncorrelated with $X$} \\right)\\right] \\\\[1em]\n&= \\E[X\\E[Y\\mid X]] + \\E[X(Y-\\E[Y\\mid X])] \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Projection interpretation, } \\E[X(Y-\\E[Y\\mid X])]=0)\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof2). \n\\[\n\\begin{split}\n\\E[X\\E[Y\\mid X]] &= \\sum_x x\\E[Y\\mid X=x] \\P(X=x) \\\\\n&= \\sum_x\\sum_y xy\\P(Y=y\\mid X=x)\\P(X=x) \\\\\n&= \\sum_x\\sum_y xy \\P(Y=y, X=x)\n\\end{split}\n\\]\n\n□\n\n\nA more general case of \\(\\E[XY] = \\E[X\\E[Y\\mid X]]\\) is",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#references",
    "href": "conditional_expectation.html#references",
    "title": "2  Conditional Expectation",
    "section": "References",
    "text": "References\n\nDavid S. Rosenberg. Conditional Expectations: Review and Lots of Examples, https://davidrosenberg.github.io/ttml2021fall/background/conditional-expectation-notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "measure_theory.html",
    "href": "measure_theory.html",
    "title": "3  Measure Theory",
    "section": "",
    "text": "3.1 Definitions\nWe denote the collection of subsets, or power set, of a set \\(X\\) by \\(\\Pcal(X).\\)\nThe Cartesian product, or product, of sets \\(X, Y\\) is the collection of all ordered pairs\n\\[\nX\\times Y = \\{(x,y): x\\in X, y\\in Y\\}.\n\\]\nA topological space is a set equipped with a collection of open subsets that satisfies appropriate conditions.\nThe complement of an open set in \\(X\\) is called a closed set, and \\(\\Tcal\\) is called a topology on \\(X.\\)\nA \\(\\sigma\\)-algebra on a set \\(X\\) is a collection of subsets of a set \\(X\\) that contains \\(\\emptyset\\) and \\(X\\), and is closed under complements, finite unions, countable unions, and countable intersections.\nA measurable space \\((X, \\Acal)\\) is an non-empty set \\(X\\) equipped with a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X.\\)\nDifference between a measurable space and \\(\\sigma\\)-algebra:\nA measure \\(\\mu\\) is a countably additive, non-negative, extended real-valued function defined on a \\(\\sigma\\)-algebra.\nA measure space \\((X, \\Acal, \\mu)\\) consist of a set \\(X\\), a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X\\), and a measure \\(\\mu\\) defined on \\(\\Acal.\\) When \\(\\Acal\\) and \\(\\mu\\) are clear from the context, we will refer to the measure space \\(X\\).\nAn abstract probability space \\((\\Omega, \\Fcal, \\P)\\)\nA random variable is any function \\(X: \\Omega \\to \\Xcal.\\) We say that \\(X\\) has distribution \\(P,\\) and write \\(X\\sim P\\), if\n\\[\n\\P(X\\in B) = \\P(\\{\\omega: X(\\omega)\\in B\\}) = \\P(B)\n\\]\nWe say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue measure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any function \\(f\\).\nThe expectation of a random variable is defined as an integral with respect to \\(\\P\\):\n\\[\n\\E[X] = \\int X(\\omega)\\, \\mathrm d \\P(\\omega),\n\\]\nand\n\\[\n\\E[f(X,Y)] = \\int f(X(\\omega), Y(\\omega))\\, \\mathrm d \\P(\\omega).\n\\]\nA measure \\(\\mu\\) on a measurable space \\((X,\\Acal)\\) is a function\n\\[\n\\mu: \\Acal \\to [0, \\infty]\n\\]\nsuch that\n\\[\n\\mu \\left( \\bigcup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nA measure \\(\\mu\\) on a set \\(X\\) is\nReferences:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "measure_theory.html#definitions",
    "href": "measure_theory.html#definitions",
    "title": "3  Measure Theory",
    "section": "",
    "text": "Definition 3.1 (Topological Space) A topological space \\((X, \\Tcal)\\) is a set \\(X\\) and a collection \\(\\Tcal \\subset \\Pcal(X)\\) of subsets of \\(X,\\) called open sets, such that\n\n\\(\\emptyset, X \\in \\Tcal;\\)\nIf \\(\\{U_\\alpha \\in \\Tcal: \\alpha \\in I \\}\\) is an arbitrary collection of open sets, then their union\n\\[\n\\bigcup_{\\alpha\\in I} U_\\alpha \\in \\Tcal\n\\]\nis open;\nIf \\(\\{U_i \\in \\Tcal: i=1,2,\\dots,N \\}\\) is a finite collection of open sets,Then their intersection\n\\[\n\\bigcap_{i=1}^N U_i \\in \\Tcal\n\\]\nis open.\n\n\n\n\n\nDefinition 3.2 A \\(\\sigma\\)-algebra on a set \\(X\\) is a collection \\(\\Acal\\) of subsets of a set \\(X\\) such that:\n\n\\(\\emptyset, X \\in \\Acal;\\)\nIf \\(A\\in\\Acal\\) then \\(A^c\\in\\Acal;\\)\nIf \\(A_i\\in\\Acal\\) then \\[\n\\bigcup_{i=1}^\\infty A_i \\in \\Acal, \\quad \\bigcap_{i=1}^\\infty A_i \\in \\Acal.\n\\]\n\n\n\nExample 3.1 If \\(X\\) is a set, then \\(\\{\\emptyset,X\\}\\) and \\(\\Pcal(X)\\) are \\(\\sigma\\)-algebras on \\(X\\); they are the smallest and largest \\(\\sigma\\)-algebras on \\(X\\), respectively.\n\n\n\n\nThe complement of a measurable set is measurable, but the complement of an open set is not, in general, open, excluding special cases such as the discrete topology \\(\\Tcal = \\Pcal (X)\\)\nCountable intersections and unions of measurable sets are measurable, but only finite intersections of open sets are open while arbitrary (even uncountable) unions of open sets are open.\n\n\n\n\n\n\\(\\omega\\in \\Omega\\) is called an outcome;\n\\(A\\in \\Fcal\\) is called an event;\n\\(\\P(A)\\) is called the probability of \\(A.\\)\n\\(\\P(\\Omega)=1\\) the sum of probability of all possible outcomes is 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu(\\emptyset)=0;\\)\nIf \\(\\{A_i\\in \\Acal: i\\in \\N\\}\\) is a countable disjoint collection of sets in \\(\\Acal,\\) then\n\n\n\n\nfinite if \\(\\mu(X)&lt;\\infty,\\) and\n\\(\\sigma\\)-finite if \\(X=\\bigcup_{n=1}^\\infty A_n\\) is a countable union of measurable sets \\(A_n\\) with finite measure, \\(\\mu(A_n)&lt;\\infty.\\)\n\n\n\nJ. K. Hunter (2011). Measure Theory. Department of Mathematics, University of California at Davis. https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html",
    "href": "TS01_AR1.html",
    "title": "4  AR(1)",
    "section": "",
    "text": "4.1 AR(1) Visualization\nTime series data often display autocorrelation, or serial correlation of the disturbances across periods.\nIf you plot the residuals and observe that the effect of a given disturbance is carried, at least in part, across periods, then it is a strong signal of serial correlation. It’s like the disturbances exhibiting a sort of “memory” over time.\nThe first-order autoregressive process, denoted AR(1), is \\[\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\] where \\(w_t\\) is a strictly stationary and ergodic white noise process with 0 mean and variance \\(\\sigma^2_w\\).\nFigure 4.1: White noise process with \\(\\sigma=20\\).\nTo illustrate the behavior of the AR(1) process, Figure 4.2 plots two simulated AR(1) processes. Each is generated using the white noise process et displayed in Figure 4.1.\nThe plot in Figure 4.2(a) sets \\(\\rho=0.5\\) and the plot in Figure 4.2(b) sets \\(\\rho=0.95\\).\nRemarks\nFigure 4.2: Simulated AR(1) processes with positive \\(\\rho\\). (a) \\(\\rho=0.5\\), (b) \\(\\rho=0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nWe have seen the cases when \\(\\rho\\) is positive, now let’s consider when \\(\\rho\\) is negative. Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.5\\), and Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.95\\,.\\)\nWe see that the sample path is very choppy when \\(\\rho\\) is negative. The different patterns for positive and negative \\(\\rho\\)’s are due to their autocorrelation functions (ACFs).\nFigure 4.3: Simulated AR(1) processes with negtive \\(\\rho\\). (a) \\(\\rho=-0.5\\), (b) \\(\\rho=-0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nPossible causes of serial correlation: Incomplete or flawed model specification. Relevant factors omitted from the time series regression are correlated across periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#ar1-visualization",
    "href": "TS01_AR1.html#ar1-visualization",
    "title": "4  AR(1)",
    "section": "",
    "text": "Figure 4.2(b) is more smooth than Figure 4.2(a).\nThe smoothing increases with \\(\\rho\\).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#mathematical-representation",
    "href": "TS01_AR1.html#mathematical-representation",
    "title": "4  AR(1)",
    "section": "4.2 Mathematical Representation",
    "text": "4.2 Mathematical Representation\nLet’s formulate an AR(1) model as follows:\n\\[\n\\begin{align}\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\end{align}\n\\tag{4.1}\\]\nwhere \\(w_t\\) is a white noise series with mean zero and variance \\(\\sigma^2_w\\). We also assume \\(|\\rho|&lt;1\\).\nWe can represent the AR(1) model as a linear combination of the innovations \\(w_t\\).\nBy iterating backwards \\(k\\) times, we get\n\\[\n\\begin{aligned}\n\\varepsilon_t &= \\rho \\,\\varepsilon_{t-1} + w_t \\\\\n&= \\rho\\, (\\rho \\, \\varepsilon_{t-2} + w_{t-1}) + w_t \\\\\n&= \\rho^2 \\varepsilon_{t-2} + \\rho w_{t-1} + w_t \\\\\n&\\quad \\vdots \\\\\n&= \\rho^k \\varepsilon_{t-k} + \\sum_{j=0}^{k-1} \\rho^j \\,w_{t-j} \\,.\n\\end{aligned}\n\\] This suggests that, by continuing to iterate backward, and provided that \\(|\\rho|&lt;1\\) and \\(\\sup_t \\text{Var}(\\varepsilon_t)&lt;\\infty\\), we can represent \\(\\varepsilon_t\\) as a linear process given by\n\\[\n\\color{#EE0000FF}{\\varepsilon_t = \\sum_{j=0}^\\infty \\rho^j \\,w_{t-j}} \\,.\n\\]\n\n\n4.2.1 Expectation\n\\(\\varepsilon_t\\) is stationary with mean zero.\n\\[\nE(\\varepsilon_t) = \\sum_{j=0}^\\infty \\rho^j \\, E(w_{t-j})\n\\]\n\n\n\n4.2.2 Autocovariance\nThe autocovariance function of the AR(1) process is \\[\n\\begin{aligned}\n\\gamma (h) &= \\text{Cov}(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E\\left[\\left(\\sum_{j=0}^\\infty \\rho^j \\,w_{t+h-j}\\right)  \\left(\\sum_{k=0}^\\infty \\rho^k \\,w_{t-k}\\right) \\right] \\\\\n&= \\sum_{l=0}^{\\infty} \\rho^{h+l} \\rho^l \\sigma_w^2 \\\\\n&= \\sigma_w^2 \\cdot \\rho^{h} \\cdot \\sum_{l=0}^{\\infty} \\rho^{2l}  \\\\\n&= \\frac{\\sigma_w^2 \\cdot \\rho^{h} }{1-\\rho^2}, \\quad h&gt;0 \\,.\n\\end{aligned}\n\\] When \\(h=0\\), \\[\n\\gamma(0) = \\frac{\\sigma_w^2}{1-\\rho^2}\n\\] is the variance of the process \\(\\text{Var}(\\varepsilon_t)\\).\nNote that\n\n\\(\\gamma(0) \\ge |\\gamma (h)|\\) for all \\(h\\). Maximum value at 0 lag.\n\\(\\gamma (h)\\) is symmetric, i.e., \\(\\gamma (-h) = \\gamma (h)\\)\n\n\n\n\n4.2.3 Autocorrelation\nThe autocorrelation function (ACF) is given by\n\\[\n\\rho(h) = \\frac{\\gamma (h)}{\\gamma (0)} = \\rho^h,\n\\] which is simply the correlation between \\(\\varepsilon_{t+h}\\) and \\(\\varepsilon_{t}\\,.\\)\nNote that \\(\\rho(h)\\) satisfies the recursion \\[\n\\rho(h) = \\rho\\cdot \\rho(h-1) \\,.\n\\]\n\nFor \\(\\rho &gt;0\\), \\(\\rho(h)=\\rho^h&gt;0\\) observations close together are positively correlated with each other. The larger the \\(\\rho\\), the larger the correlation.\nFor \\(\\rho &lt;0\\), the sign of the ACF \\(\\rho(h)=\\rho^h\\) depends on the time interval.\n\nWhen \\(h\\) is even, \\(\\rho(h)\\) is positive;\nwhen \\(h\\) is odd, \\(\\rho(h)\\) is negative.\n\nThis result means that observations at contiguous time points are negatively correlated, but observations two time points apart are positively correlated.\n\nFor example, if an observation, \\(\\varepsilon_t\\), is positive, the next observation, \\(\\varepsilon_{t+1}\\), is typically negative, and the next observation, \\(\\varepsilon_{t+2}\\), is typically positive. Thus, in this case, the sample path is very choppy.\n\n\nAnother interpretation of \\(\\rho(h)\\) is the optimal weight for scaling \\(\\varepsilon_t\\) into \\(\\varepsilon_{t+h}\\), i.e., the weight, \\(a\\), that minimizes \\(E[(\\varepsilon_{t+h} - a\\,\\varepsilon_{t})^2]\\,.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html",
    "href": "TS01_AR_example.html",
    "title": "5  AR – Example",
    "section": "",
    "text": "5.1 Dataset Description\nThis script provides an example of autocorrelated residuals using expectations augmented Phillips Curve.\nUS Macroeconomics Data Set, Quarterly, 1950I to 2000IV, 204 Quarterly Observations\nSource: Department of Commerce, BEA website and www.economagic.com\n# data preview\ndata &lt;- read.table(\"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/TableF5-2.txt\", header = TRUE)\ndata &lt;- data %&gt;% \n    mutate(delta_infl = infl-lag(infl))\ndata %&gt;% \n    head() %&gt;% \n    knitr::kable(digits = 5) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, latex_options=\"scale_down\") %&gt;% \n    scroll_box(width = \"100%\")\n\n\n\n\n\nYear\nqtr\nrealgdp\nrealcons\nrealinvs\nrealgovt\nrealdpi\ncpi_u\nM1\ntbilrate\nunemp\npop\ninfl\nrealint\ndelta_infl\n\n\n\n\n1950\n1\n1610.5\n1058.9\n198.1\n361.0\n1186.1\n70.6\n110.20\n1.12\n6.4\n149.461\n0.0000\n0.0000\n\n\n\n1950\n2\n1658.8\n1075.9\n220.4\n366.4\n1178.1\n71.4\n111.75\n1.17\n5.6\n150.260\n4.5071\n-3.3404\n4.5071\n\n\n1950\n3\n1723.0\n1131.0\n239.7\n359.6\n1196.5\n73.2\n112.95\n1.23\n4.6\n151.064\n9.9590\n-8.7290\n5.4519\n\n\n1950\n4\n1753.9\n1097.6\n271.8\n382.5\n1210.0\n74.9\n113.93\n1.35\n4.2\n151.871\n9.1834\n-7.8301\n-0.7756\n\n\n1951\n1\n1773.5\n1122.8\n242.9\n421.9\n1207.9\n77.3\n115.08\n1.40\n3.5\n152.393\n12.6160\n-11.2160\n3.4326\n\n\n1951\n2\n1803.7\n1091.4\n249.2\n480.1\n1225.8\n77.6\n116.19\n1.53\n3.1\n152.917\n1.5494\n-0.0161\n-11.0666",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#dataset-description",
    "href": "TS01_AR_example.html#dataset-description",
    "title": "5  AR – Example",
    "section": "",
    "text": "Feild Name\nDefinition\n\n\n\n\nyear\nYear\n\n\nqtr\nQuarter\n\n\nrealgdp\nReal GDP ($bil)\n\n\nrealcons\nReal consumption expenditures\n\n\nrealinvs\nReal investment by private sector\n\n\nrealgovt\nReal government expenditures\n\n\nrealdpi\nReal disposable personal income\n\n\ncpi_u\nConsumer price index\n\n\nM1\nNominal money stock\n\n\ntbilrate\nQuarterly average of month end 90 day t bill rate\n\n\nunemp\nUnemployment rate\n\n\npop\nPopulation, mil. interpolate of year end figures using constant growth rate per quarter\n\n\ninfl\nRate of inflation (First observation is missing)\n\n\nrealint\nEx post real interest rate = Tbilrate - Infl. (First observation is missing)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#empirical-model",
    "href": "TS01_AR_example.html#empirical-model",
    "title": "5  AR – Example",
    "section": "5.2 Empirical Model",
    "text": "5.2 Empirical Model\n\\[\n\\Delta I_t =  \\beta_1 + \\beta_2 u_t + \\varepsilon_t\n\\] where\n\n\\(I_t\\) is the inflation rate; \\(\\Delta I_t = I_t - I_{t-1}\\) is the first difference of the inflation rate;\n\\(u_t\\) is the unemployment rate;\n\\(\\varepsilon_t\\) is the error term.\n\nWe remove the first two quarters due to missing value in the first observation and the change in the rate of inflation.\nRegression result for OLS.\n\nlm_phillips &lt;- lm(delta_infl ~ unemp, data = data %&gt;% tail(-2))\nstargazer(lm_phillips, \n          type = \"html\", \n          title = \"Phillips Curve Regression\",\n          notes = \"&lt;span&gt;&#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&#42;&#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&#42;&#42;&#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.\",\n          notes.append = F)\n\n\nPhillips Curve Regression\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndelta_infl\n\n\n\n\n\n\n\n\nunemp\n\n\n-0.090\n\n\n\n\n\n\n(0.126)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.492\n\n\n\n\n\n\n(0.740)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n202\n\n\n\n\nR2\n\n\n0.003\n\n\n\n\nAdjusted R2\n\n\n-0.002\n\n\n\n\nResidual Std. Error\n\n\n2.822 (df = 200)\n\n\n\n\nF Statistic\n\n\n0.513 (df = 1; 200)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.\n\n\n\n\n\n# variance-covariance matrix\nvcov(lm_phillips)\n\n            (Intercept)       unemp\n(Intercept)  0.54830829 -0.08973175\nunemp       -0.08973175  0.01582211\n\n\nAutocorrelated residuals\nPlot the residuals.\n\nplot(lm_phillips$residuals, type=\"l\")\n\n\n\n\n\n\n\nFigure 5.1: Phillips Curve Deviations from Expected Inflation\n\n\n\n\n\nFigure 5.1 shows striking negative autocorrelation.\nNow we test the serial correlation of the residuals by regressing \\(\\varepsilon_t\\) on \\(\\varepsilon_{t-1}\\). \\[\n\\varepsilon_t = \\phi\\varepsilon_{t-1} + e_t\n\\]\n\nres &lt;- tibble(\n    res_t = lm_phillips$residuals,\n    res_t1 = lag(lm_phillips$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8694 -1.4800  0.0718  1.4990  8.3258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.02155    0.17854  -0.121    0.904    \nres_t1      -0.42630    0.06355  -6.708    2e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1844,    Adjusted R-squared:  0.1803 \nF-statistic: 44.99 on 1 and 199 DF,  p-value: 2.002e-10\n\n\nThe regression of the least squares residuals on their past values gives a slope of -0.4263 with a highly significant \\(t\\) ratio of -6.7078. We thus conclude that the residuals in this models are highly negatively autocorrelated.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#test-for-serial-correlation",
    "href": "TS01_AR_example.html#test-for-serial-correlation",
    "title": "5  AR – Example",
    "section": "5.3 Test for Serial Correlation",
    "text": "5.3 Test for Serial Correlation\n\nDurbin-Watson (DW) test for AR(1)\nBreusch-Godfrey test for AR(q)\n\n\nlibrary(lmtest)\ndwtest(lm_phillips, alternative = \"two.sided\") # Durbin Watson test \n\n\n    Durbin-Watson test\n\ndata:  lm_phillips\nDW = 2.8276, p-value = 5.212e-09\nalternative hypothesis: true autocorrelation is not 0\n\nbgtest(lm_phillips, order=1) # Breusch-Godfrey test \n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  lm_phillips\nLM test = 36.601, df = 1, p-value = 1.45e-09\n\n\nBoth tests show strong evidence of AR(1) serial correlation in the errors.\nOne consequence of the serial correlated errors is that the standard error and \\(t\\) statistics are not valid anymore. In the case if serial correlation, you can either\n\nTransform the model to remove the serial correlation, or alternatively,\nFGLS (Feasible Genralized Least Squares), transform the original equation using, e.g., Cochrane-Orcutt or Prais-Winsten transformation.\nUse serial correlation-robust standard errors\nHAC (Heteroskedasticity and autocorrelation consistent) standard errors or Newey-West standard errors.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#references",
    "href": "TS01_AR_example.html#references",
    "title": "5  AR – Example",
    "section": "References",
    "text": "References\n\nEx. 12.3, Chap 12 Serial Correlation, Econometric Analysis, Greene 5th Edition, pp 251.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html",
    "href": "TS01_phillips-curve.html",
    "title": "6  Phillips Curve",
    "section": "",
    "text": "6.1 Static Phillips Curve\nA static Phillips curve is given by:\n\\[\ninf_t = \\beta_0 + \\beta_1\\, unem_t + u_t,\n\\]\nwhere \\(inf_t\\) is the annual inflation rate and \\(unem_t\\) is the unemployment rate.\nThis form of the Phillips curve assumes a constant natural rate of unemployment and constant inflationary expectations, and it can be used to study the contemporaneous tradeoff between inflation and unemployment.\n# data preview\ndata &lt;- read.table(\"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/TableF5-2.txt\", header = TRUE)\ndata &lt;- data %&gt;% \n    mutate(delta_infl = infl-lag(infl))\ndata %&gt;% \n    head() %&gt;% \n    knitr::kable(digits = 5) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, latex_options=\"scale_down\") %&gt;% \n    scroll_box(width = \"100%\")\n\n\n\n\n\nYear\nqtr\nrealgdp\nrealcons\nrealinvs\nrealgovt\nrealdpi\ncpi_u\nM1\ntbilrate\nunemp\npop\ninfl\nrealint\ndelta_infl\n\n\n\n\n1950\n1\n1610.5\n1058.9\n198.1\n361.0\n1186.1\n70.6\n110.20\n1.12\n6.4\n149.461\n0.0000\n0.0000\n\n\n\n1950\n2\n1658.8\n1075.9\n220.4\n366.4\n1178.1\n71.4\n111.75\n1.17\n5.6\n150.260\n4.5071\n-3.3404\n4.5071\n\n\n1950\n3\n1723.0\n1131.0\n239.7\n359.6\n1196.5\n73.2\n112.95\n1.23\n4.6\n151.064\n9.9590\n-8.7290\n5.4519\n\n\n1950\n4\n1753.9\n1097.6\n271.8\n382.5\n1210.0\n74.9\n113.93\n1.35\n4.2\n151.871\n9.1834\n-7.8301\n-0.7756\n\n\n1951\n1\n1773.5\n1122.8\n242.9\n421.9\n1207.9\n77.3\n115.08\n1.40\n3.5\n152.393\n12.6160\n-11.2160\n3.4326\n\n\n1951\n2\n1803.7\n1091.4\n249.2\n480.1\n1225.8\n77.6\n116.19\n1.53\n3.1\n152.917\n1.5494\n-0.0161\n-11.0666\nlm_phillips_stat &lt;- lm(infl ~ unemp, data = data %&gt;% tail(-2))\nsummary(lm_phillips_stat)\n\n\nCall:\nlm(formula = infl ~ unemp, data = data %&gt;% tail(-2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7020 -2.1922 -0.6098  1.4644 12.7362 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.2028     0.8873   2.483   0.0139 *\nunemp         0.3056     0.1507   2.028   0.0439 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.381 on 200 degrees of freedom\nMultiple R-squared:  0.02014,   Adjusted R-squared:  0.01524 \nF-statistic: 4.111 on 1 and 200 DF,  p-value: 0.04394\nThe simple regression estimates are\n\\[\n\\begin{aligned}\n\\widehat{inf}_t &= 2.22 + 0.30\\, unem_t \\\\\n&\\phantom{={ }} (0.89)\\;\\; (.15)\n\\end{aligned}\n\\tag{6.1}\\]\nThe regression indicates a positive relationship (\\(\\hat{\\beta}_1&gt;0\\)) between inflation and unemployment at 5% significance level.\nThere are some problems with this analysis that we cannot address in detail now. The Classical Linear Model assumptions do not hold. In addition, the static Phillips curve is probably not the best model for determining whether there is a short run tradeoff between inflation and unemployment. Macroeconomists generally prefer the expectations augmented Phillips curve, while we will see shortly.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "href": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "title": "6  Phillips Curve",
    "section": "6.2 Expectations Augmented Phillips Curve",
    "text": "6.2 Expectations Augmented Phillips Curve\nExample 12.3 in Greene (2003), 5ed, Econometric Analysis.\nA linear version of the expectations augmented Phillips curve can be written as\n\\[\ninf_t - inf^e_t = \\beta_1 (unem_t - \\mu_0) + e_t,\n\\]\nwhere \\(\\mu_0\\) is the natural rate of unemployment and \\(inf^e_t\\) is the expected rate of inflation formed in year \\(t-1.\\)\nThe difference between actual unemployment and the natural rate is called cyclical unemployment, while the difference between actual and expected inflation is called unanticipated inflation.\nIf there is a tradeoff between unanticipated inflation and cyclical unemployment, then \\(\\beta_1&lt;0.\\)\nThe error term, \\(e_t\\), is called a supply shock by macroeconomists.\nTo complete this model, we need to make an assumption about inflationary expectations. Under adaptive expectations, the expected value of current inflation depends on recently observed inflation. A particularly simple formulation is that expected inflation this year is last year’s inflation:\n\\[\ninf^e_t = inf_{t-1}\n\\]\nUnder this assumption, we can write the following empirical model:\n\\[\ninf_t - inf_{t-1} = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] or\n\\[\n\\Delta  inf_t = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] where \\(\\Delta  inf_t = inf_t - inf_{t-1}\\) and \\(\\beta_0=-\\beta_1\\mu_0.\\)\nHence, the natural unemployment rate can be obtained by:\n\\[\n\\mu_0 = -\\frac{\\beta_0}{\\beta_1}.\n\\]\n\nlm_phillips_aug &lt;- lm(delta_infl ~ unemp, data = data %&gt;% tail(-2))\nsummary(lm_phillips_aug)\n\n\nCall:\nlm(formula = delta_infl ~ unemp, data = data %&gt;% tail(-2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2791  -1.6635  -0.0117   1.7813   8.5472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.49189    0.74048   0.664    0.507\nunemp       -0.09013    0.12579  -0.717    0.474\n\nResidual standard error: 2.822 on 200 degrees of freedom\nMultiple R-squared:  0.002561,  Adjusted R-squared:  -0.002427 \nF-statistic: 0.5134 on 1 and 200 DF,  p-value: 0.4745\n\n\nThe OLS estimates are\n\\[\n\\begin{aligned}\n\\Delta\\widehat{inf}_t &= 0.49 - 0.09 \\,unem_t \\\\\n&\\phantom{={}} (0.74) \\;\\; (0.13)\n\\end{aligned}\n\\tag{6.2}\\]\n\\(\\hat{\\beta}_1&lt;0\\) indicates a tradeoff between cyclical unemployment and unanticipated inflation. But the effect is statistically insignificant.\nUnder expectations augmented Phillips Curve, one-point increase in \\(unem\\) lowers unanticipated inflation by about 0.1 of a point. We can contrast this with the static Phillips curve in Equation 6.1, where we found a positive relationship between inflation and unemployment.\nThe natural employment rate is 5.78 percent.\n\\[\n\\mu_0 = -\\frac{\\beta_0}{\\beta_1} = \\frac{0.49}{0.09} \\approx 5.44\n\\]\nVariance can be obtained by the Delta Method. In R, you can use car::deltaMethod to get the confidence interval for your parameter of interest.\n\nlibrary(car)\ndeltaMethod(lm_phillips_aug, \"-b0/b1\", \n            parameterNames = paste(\"b\", 0:1, sep=\"\"))\n\n       Estimate     SE  2.5 % 97.5 %\n-b0/b1   5.4575 2.2228 1.1009 9.8141\n\n\nSerial correlation in errors\n\nres &lt;- tibble(\n    res_t = lm_phillips_aug$residuals,\n    res_t1 = lag(lm_phillips_aug$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8694 -1.4800  0.0718  1.4990  8.3258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.02155    0.17854  -0.121    0.904    \nres_t1      -0.42630    0.06355  -6.708    2e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1844,    Adjusted R-squared:  0.1803 \nF-statistic: 44.99 on 1 and 199 DF,  p-value: 2.002e-10\n\n\nThere is strong evidence of serial correlation in the residuals.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#remedies-for-ar-errors",
    "href": "TS01_phillips-curve.html#remedies-for-ar-errors",
    "title": "6  Phillips Curve",
    "section": "6.3 Remedies for AR errors",
    "text": "6.3 Remedies for AR errors\nExample 12.5 in Wooldridge (2013), 5ed, Introductory Econometrics: A Modern Approach.\nExample 19.2 in Greene (2003), 5ed, Econometric Analysis.\n\n6.3.1 Prais-Winsten Estimation (Transformation)\nStatic model\nStatic model indicates positive autocorrelation.\n\nres &lt;- tibble(\n    res_t = lm_phillips_stat$residuals,\n    res_t1 = lag(lm_phillips_stat$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5887 -1.6354 -0.1089  1.2856  7.9793 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.04062    0.18009  -0.226    0.822    \nres_t1       0.64520    0.05349  12.062   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4223,    Adjusted R-squared:  0.4194 \nF-statistic: 145.5 on 1 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n\nlibrary(prais) # install.packages(\"prais\")\ndata &lt;- data %&gt;% \n    mutate(yrQ = as.yearqtr(paste(Year, qtr, sep=\"-\")))\npw_est_stat &lt;- prais_winsten(lm_phillips_stat, data = data %&gt;% tail(-2), index = \"yrQ\" )\n\nIteration 0: rho = 0\nIteration 1: rho = 0.6452\nIteration 2: rho = 0.655\nIteration 3: rho = 0.6558\nIteration 4: rho = 0.6559\nIteration 5: rho = 0.6559\nIteration 6: rho = 0.6559\n\nsummary(pw_est_stat)\n\n\nCall:\nprais_winsten(formula = lm_phillips_stat, data = data %&gt;% tail(-2), \n    index = \"yrQ\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4276 -2.2063 -0.8301  1.6560 12.8870 \n\nAR(1) coefficient rho after 6 iterations: 0.6559\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.82017    1.69871   2.249   0.0256 *\nunemp        0.02493    0.28647   0.087   0.9307  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.564 on 200 degrees of freedom\nMultiple R-squared:  0.01303,   Adjusted R-squared:  0.008099 \nF-statistic: 2.641 on 1 and 200 DF,  p-value: 0.1057\n\nDurbin-Watson statistic (original): 0.6931 \nDurbin-Watson statistic (transformed): 2.378\n\n\n\nExpectations augmented model\nExpectations augmented model indicates negative autocorrelation.\n\npw_est_aug &lt;- prais_winsten(lm_phillips_aug, data = data %&gt;% tail(-2), index = \"yrQ\" )\n\nIteration 0: rho = 0\nIteration 1: rho = -0.4263\nIteration 2: rho = -0.4263\n\nsummary(pw_est_aug)\n\n\nCall:\nprais_winsten(formula = lm_phillips_aug, data = data %&gt;% tail(-2), \n    index = \"yrQ\")\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2668  -1.6601  -0.0065   1.7870   8.5401 \n\nAR(1) coefficient rho after 2 iterations: -0.4263\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.47007    0.47247   0.995    0.321\nunemp       -0.08705    0.08024  -1.085    0.279\n\nResidual standard error: 2.548 on 200 degrees of freedom\nMultiple R-squared:  0.005932,  Adjusted R-squared:  0.000962 \nF-statistic: 1.194 on 1 and 200 DF,  p-value: 0.2759\n\nDurbin-Watson statistic (original): 2.828 \nDurbin-Watson statistic (transformed): 2.285",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#references",
    "href": "TS01_phillips-curve.html#references",
    "title": "6  Phillips Curve",
    "section": "References",
    "text": "References\n\nTomas Formanek, Materials for the Advanced Econometric 1 – courses 4EK608 and 4EK416 https://github.com/formanektomas/4EK608_4EK416/tree/master?tab=readme-ov-file",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html",
    "href": "TS01_examples.html",
    "title": "7  TS Examples",
    "section": "",
    "text": "7.1 Example 11.6: Fertility and Personal Exemption\nTextbook: Chapter 11, Introductory Econometrics: A Modern Approach, 7e by Jeffrey M. Wooldridge\nSummary notes by Marius v. Oordt: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3401712\ngfr: general fertility rate\npe: personal exemption\ndata('fertil3')\nfertility_diff &lt;- lm(diff(gfr) ~ diff(pe), data = fertil3)\nfertility_lag &lt;- lm(diff(gfr) ~ diff(pe) + diff(pe_1) + diff(pe_2), data = fertil3)\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndiff(gfr)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndiff(pe)\n\n\n-0.04268 (0.02837)\n\n\n-0.03620 (0.02677)\n\n\n\n\ndiff(pe_1)\n\n\n\n\n-0.01397 (0.02755)\n\n\n\n\ndiff(pe_2)\n\n\n\n\n0.10999*** (0.02688)\n\n\n\n\nConstant\n\n\n-0.78478 (0.50204)\n\n\n-0.96368** (0.46776)\n\n\n\n\n\n\n\n\nObservations\n\n\n71\n\n\n69\n\n\n\n\nR2\n\n\n0.03176\n\n\n0.23248\n\n\n\n\nAdjusted R2\n\n\n0.01773\n\n\n0.19705\n\n\n\n\nResidual Std. Error\n\n\n4.22082 (df = 69)\n\n\n3.85945 (df = 65)\n\n\n\n\nF Statistic\n\n\n2.26343 (df = 1; 69)\n\n\n6.56266*** (df = 3; 65)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.\nThe first regression uses first differences:\n\\[\n\\begin{aligned}\n\\Delta\\widehat{gfr} &= -.785 - .043\\, \\Delta pe \\\\\n&\\phantom{=}\\;\\; (.502)\\;\\; (.028) \\\\\nn &= 71, R^2=.032, \\bar{R^2} = .018.\n\\end{aligned}\n\\tag{7.1}\\]\nThe estimates indicate an increase in \\(pe\\) lowers \\(gfr\\) contemporaneously, although the estimate is not statistically different from zero at the 5% level.\nIf we add two lags of \\(\\Delta pe,\\) things improve:\n\\[\n\\begin{aligned}\n\\Delta\\widehat{gfr} &= -.964 - .036\\, \\Delta pe - .014\\, \\Delta pe_{-1} + .110\\, \\Delta pe_{-2} \\\\\n&\\phantom{=}\\;\\; (.468)\\quad (.027) \\qquad\\; (.028) \\qquad\\quad\\; (.027)\\\\\nn &= 69, R^2=.232, \\bar{R^2} = .197.\n\\end{aligned}\n\\tag{7.2}\\]\nEven though \\(\\Delta pe\\) and \\(\\Delta pe_{-1}\\) have negative coefficients, their coefficients are small and jointly insignificant (\\(p\\text{-value}=.28,\\) see Anova test below).\n# Compare the restricted with the full model\nfertility_lag2 &lt;- lm(diff(gfr) ~ diff(pe_2), data = fertil3)\nanova(fertility_lag2, fertility_lag)\n\nAnalysis of Variance Table\n\nModel 1: diff(gfr) ~ diff(pe_2)\nModel 2: diff(gfr) ~ diff(pe) + diff(pe_1) + diff(pe_2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     67 1006.6                           \n2     65  968.2  2    38.413 1.2894 0.2824\nThe second lag (\\(\\Delta pe_{-2}\\)) is very significant and indicates a positive relationship between changes in \\(pe\\) and subsequent changes in \\(gfr\\) two years hence. This makes more sense than having a contemporaneous effect.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html#example-11.6-fertility-and-personal-exemption",
    "href": "TS01_examples.html#example-11.6-fertility-and-personal-exemption",
    "title": "7  TS Examples",
    "section": "",
    "text": "7.1.1 Example 11.8\nIn this example, we want to test whether the Finite Distributed Lag model (7.2) for \\(\\Delta\\widehat{gfr}\\) and \\(\\Delta pe\\) is dynamically complete.\nBeing dynamically complete indicates that neither lags of \\(\\Delta\\widehat{gfr}\\) nor further lags of \\(\\Delta pe\\) should appear in the equation. Mathematically, given the following finite distributed lag model:\n\\[\n\\Delta gfr_t = \\beta_0 + \\beta_1\\Delta pe_t + \\beta_2\\Delta pe_{t-1} + \\beta_3 \\Delta pe_{t-2} + u_t .\n\\] Rewrite it as\n\\[\n\\begin{aligned}\n\\Delta gfr_t &= \\beta_0 + \\beta_1x_{t1} + \\beta_2x_{t2} + \\beta_3 x_{t3} + u_t \\\\\ny_t &= \\bx_t'\\bbeta + u_t\n\\end{aligned}\n\\] where the explanatory variables \\(\\bx_t=(x_{t1}, x_{t2}, x_{t3})' = (\\Delta pe_t, \\Delta pe_{t-2}, \\Delta pe_{t-3})'\\) and the dependent variable \\(y_t=\\Delta gfr_t.\\)\nA dynamically complete model requires the following condition:\n\\[\n\\E(u_t\\mid \\bx_t, y_{t-1}, \\bx_{t-1}, \\ldots) = 0.\n\\tag{7.3}\\] Written in terms of \\(y_t,\\)\n\\[\n\\E(y_t\\mid \\bx_t, y_{t-1}, \\bx_{t-1}, \\ldots) = \\E(y_t\\mid \\bx_t).\n\\tag{7.4}\\]\nWe can test for dynamic completeness by adding \\(\\Delta gfr_{t-1}.\\)\n\nfertility_lag_dep &lt;- lm(diff(gfr) ~ lag(diff(gfr)) + diff(pe) + diff(pe_1) + diff(pe_2), data = fertil3)\ntidy(fertility_lag_dep) %&gt;% \n    knitr::kable(digits = 3) \n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.702\n0.454\n-1.547\n0.127\n\n\nlag(diff(gfr))\n0.300\n0.106\n2.835\n0.006\n\n\ndiff(pe)\n-0.045\n0.026\n-1.773\n0.081\n\n\ndiff(pe_1)\n0.002\n0.027\n0.077\n0.939\n\n\ndiff(pe_2)\n0.105\n0.026\n4.108\n0.000\n\n\n\n\n\nThe coefficient estimate is .300 and its \\(t\\) statistic is 2.84. Thus, the model is NOT dynamically complete in the sense of (7.4).\nThe fact that (7.2) is not dynamically complete suggests that there may be serial correlation in the errors. We will need to test and correct for this.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html#example-11.7-wages-and-productivity",
    "href": "TS01_examples.html#example-11.7-wages-and-productivity",
    "title": "7  TS Examples",
    "section": "7.2 Example 11.7: Wages and Productivity",
    "text": "7.2 Example 11.7: Wages and Productivity\n\\[\\log(hrwage_t) = \\beta_0 + \\beta_1\\log(outphr_t) + \\beta_2t + u_t\\] Data from the Economic Report of the President, 1989, Table B-47. The data are for the non-farm business sector.\n\ndata(\"earns\")\nwage_time &lt;- lm(lhrwage ~ loutphr + t, data = earns)\nwage_diff &lt;- lm(diff(lhrwage) ~ diff(loutphr), data = earns)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlhrwage\n\n\ndiff(lhrwage)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nloutphr\n\n\n1.63964*** (0.09335)\n\n\n\n\n\n\nt\n\n\n-0.01823*** (0.00175)\n\n\n\n\n\n\ndiff(loutphr)\n\n\n\n\n0.80932*** (0.17345)\n\n\n\n\nConstant\n\n\n-5.32845*** (0.37445)\n\n\n-0.00366 (0.00422)\n\n\n\n\n\n\n\n\nObservations\n\n\n41\n\n\n40\n\n\n\n\nR2\n\n\n0.97122\n\n\n0.36424\n\n\n\n\nAdjusted R2\n\n\n0.96971\n\n\n0.34750\n\n\n\n\nResidual Std. Error (df = 38)\n\n\n0.02854\n\n\n0.01695\n\n\n\n\nF Statistic\n\n\n641.22430*** (df = 2; 38)\n\n\n21.77054*** (df = 1; 38)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html",
    "href": "TS02_lag_poly.html",
    "title": "8  Lag polynomials",
    "section": "",
    "text": "8.1 Product of Filters\nA \\(p\\)-th degree lag polynomial is given by:\n\\[\n\\alpha(L) = \\alpha_0 + \\alpha_1L + \\cdots + \\alpha_pL^p,\n\\] where \\(L\\) is the lag operator, defined by the relation \\(L^jx_t=x_{t-j}.\\)\nWe define a filter given by \\(\\alpha(L)\\) to an input process \\(\\{x_t\\}\\), we get a weighted average of the current and \\(p\\) most recent values of the process:\n\\[\n\\begin{aligned}\n\\alpha(L)x_t &= \\alpha_0x_t + \\alpha_1Lx_t + \\alpha_2L^2x_t + \\cdots + \\alpha_pL^px_t \\\\\n&= \\alpha_0x_t + \\alpha_1x_{t-1}+ \\alpha_2x_{t-2} + \\cdots + \\alpha_px_{t-p} \\\\\n&= \\sum_{j=0}^p \\alpha_jx_{t-j}\n\\end{aligned}\n\\]\nLet \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}\\) be two arbitrary sequences of real numbers and define the sequence \\(\\{\\delta_j\\}\\) by the relation\n\\[\n\\begin{gathered}\n\\delta_0 = \\alpha_0\\beta_0, \\\\\n\\delta_1 = \\alpha_0\\beta_1 + \\alpha_1\\beta_0, \\\\\n\\delta_2 = \\alpha_0\\beta_2 + \\alpha_1\\beta_1 + \\alpha_2\\beta_0, \\\\\n\\vdots \\\\\n\\delta_j = \\alpha_0\\beta_j + \\alpha_1\\beta_{j-1} + \\alpha_2\\beta_{j-2} + \\cdots + \\alpha_{j-1}\\beta_{1} + \\alpha_{j}\\beta_{0}, \\\\\n\\vdots \\\\\n\\end{gathered}\n\\tag{8.1}\\]\nThe sequence \\(\\{\\delta_j\\}\\) created from this convoluted formula is called the convolution of \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}.\\)\nFor example, for \\(\\alpha(L)=1+\\alpha_1L\\) and \\(\\beta(L)=1+\\beta_1L\\), we have\n\\[\n\\delta(L)=(1+\\alpha_1L)(1+\\beta_1L) = 1+ (\\alpha_1+\\beta_1)L + \\alpha_1\\beta_1L^2.\n\\]\nFilters are commutative:\n\\[\n\\alpha(L)\\beta(L) = \\beta(L)\\alpha(L)\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#inverses",
    "href": "TS02_lag_poly.html#inverses",
    "title": "8  Lag polynomials",
    "section": "8.2 Inverses",
    "text": "8.2 Inverses\nThe inverse of \\(\\alpha(L)\\) is denoted as \\(\\alpha(L)^{-1}\\) or \\(1/\\alpha(L)\\):\n\\[\n\\alpha(L)\\alpha(L)^{-1}=1\n\\] Define a \\(p\\)-th degree lag polynomial \\(\\phi(L)\\)\n\\[\n\\phi(L) = 1-\\phi_1L-\\phi_2L^2-\\cdots-\\phi_pL^p.\n\\tag{8.2}\\]\nEquation 8.2 is often used to construct AR processes.\nNow let’s calculate its inverse, \\(\\psi(L) = \\phi(L)^{-1}.\\)\n\\[\n\\psi(L) = \\psi_0 + \\psi_1L + \\psi_2L^2 + \\cdots\n\\]\nBy the convolution formula (8.1), we have\n\\[\n\\begin{aligned}\n\\text{constant}:&\\quad  \\psi_0 =1 \\\\\nL: &\\quad  \\psi_1-\\psi_0\\phi_1 = 0 \\Longrightarrow \\psi_1 = \\phi_1 \\\\\nL^2: &\\quad  \\psi_2-\\psi_1\\phi_1-\\psi_0\\phi_2 = 0 \\Longrightarrow \\psi_2 = \\phi_1^2 + \\phi_2 \\\\\n\\vdots \\\\\nL^p: &\\quad  \\psi_p - \\psi_{p-1}\\phi_1 - \\psi_{p-2}\\phi_2 - \\cdots - \\psi_{1}\\phi_{p-1} - \\psi_{0}\\phi_p = 0 \\\\\nL^{p+1}: &\\quad  \\psi_{p+1} - \\psi_{p}\\phi_1 - \\psi_{p-1}\\phi_2 - \\cdots - \\psi_{2}\\phi_{p-1} - \\psi_{1}\\phi_p = 0 \\\\\n\\vdots\n\\end{aligned}\n\\]\n\nExample 8.1 Consider a 1st degree lag polynomial \\(\\phi(L)=1-\\phi L\\), its inverse \\(\\psi(L)\\) can be calculated as\n\\[\n\\begin{aligned}\n\\text{constant}: &\\quad \\psi_0 =1  \\\\\nL: &\\quad \\psi_1 - \\psi_0\\phi = 0 \\Longrightarrow \\psi_1 = \\phi \\\\\nL^2: &\\quad \\psi_2 - \\psi_1\\phi = 0 \\Longrightarrow \\psi_2 = \\phi^2 \\\\\nL^3: &\\quad \\psi_3 - \\psi_2\\phi = 0 \\Longrightarrow \\psi_3 = \\phi^3 \\\\\n\\vdots\n\\end{aligned}\n\\] Hence\n\\[\n\\begin{aligned}\n\\psi(L) &= (1-\\phi L)^{-1} \\\\\n&=1 + \\phi L + \\phi^2 L^2 + \\phi^3 L^3 + \\cdots \\\\\n&= \\sum_{j=0}^\\infty \\phi^jL^j.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#stability-condition",
    "href": "TS02_lag_poly.html#stability-condition",
    "title": "8  Lag polynomials",
    "section": "8.3 Stability Condition",
    "text": "8.3 Stability Condition\nThe solution sequence \\(\\{\\psi_j\\}\\) eventually starts declining at a geometric rate if the stability condition holds. The condition states:\nAll the roots of the \\(p\\)-th degree polynomial equation in \\(z\\)\n\\[\n\\phi(z) = 0 \\text{ where } \\phi(z) \\equiv 1-\\phi_1z-\\phi_2z^2-\\cdots-\\phi_pz^p\n\\] are greater than 1 in absolute value (lie outside the unit circle).\nEquivalently, we can consider the roots of the reciprocal polynomial defined as (basically this means inverting the order of the coefficients)\n\\[\n\\phi^*(z) = z^p\\phi(z^{-1}) = z^p - \\phi_1z^{p-1} - \\dots - \\phi_p.\n\\] The stability condition can be stated as:\nAll the roots of\n\\[\n\\phi^*(z) \\equiv z^p - \\phi_1z^{p-1} - \\dots - \\phi_p =0\n\\] are less than 1 in the absolute value (i.e., lie inside the unit circle).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html",
    "href": "TS02_AR-MA-representation.html",
    "title": "9  AR(1) and Its MA representation",
    "section": "",
    "text": "9.1 Case 1: \\(\\abs{\\phi}&lt;1\\)\nA first-order autoregressive process (AR(1)) satisfies the following stochastic difference equation:\n\\[\n\\begin{aligned}\ny_t &= c + \\phi y_{t-1} + \\varepsilon_t,  \\quad \\text{or} \\\\\ny_t - \\phi y_{t-1} &= c + \\varepsilon_t,  \\quad \\text{or} \\\\\n(1-\\phi L) y_t &= c + \\varepsilon_t,\n\\end{aligned}\n\\tag{9.1}\\] where \\(\\{\\varepsilon_t\\}\\) is white noise.\nIf \\(\\phi\\ne 1,\\) let \\(\\mu\\equiv c/(1-\\phi)\\) and rewrite the equation as\n\\[\n\\begin{aligned}\n(y_t-\\mu) - \\phi(y_{t-1}-\\mu) &= \\varepsilon_t \\quad \\text{or} \\\\\n(1-\\phi L) (y_t-\\mu) &= \\varepsilon_t.\n\\end{aligned}\n\\tag{9.2}\\]\n\\(\\mu\\) is the mean of \\(y_t\\) if \\(y_t\\) is covariance-stationary. For this reason, we call (9.2) a deviation-from-the-mean form. Note that the moving average is on the successive values of \\(\\{y_t\\},\\) not on \\(\\{\\varepsilon_t\\}.\\) The difference equation is called stochastic because of the presence of the random variable \\(\\varepsilon_t.\\)\nWe seek a covariance-stationary solution \\(\\{y_t\\}\\) to this stochastic difference equation. The solution depends on whether \\(\\abs{\\phi}\\) is less than, equal to, or greater than 1.\nSumming up:\nCase 1: \\(\\abs{\\phi}&lt;1\\)\nThe AR(1) process is stationary and causal, i.e., allows us to write an AR(1) as an MA(\\(\\infty\\)) using past values of \\(\\varepsilon_t.\\)\nCase 2: \\(\\abs{\\phi}&gt;1\\)\nThe AR(1) process is stationary but not causal. \\(y_t\\) is correlated with future values of \\(\\varepsilon_t.\\) This is a feasible representation but it is unnatural.\nCase 3: \\(\\abs{\\phi}=1\\)\nWe have a non-stationary process (called random walk when \\(\\phi = 1\\)) and we say that this process has a unit root.\nThe solution can be obtained easily by the use of the inverse \\((1 - \\phi L)^{-1}\\). Since this filter is absolutely summable when \\(|\\phi| &lt; 1\\), we can apply it to both sides of the AR(1) (9.2) to obtain\n\\[\n(1 - \\phi L)^{-1}(1 - \\phi L)(y_t - \\mu) = (1 - \\phi L)^{-1} \\varepsilon_t.\n\\]\nSo\n\\[\ny_t - \\mu = (1 - \\phi L)^{-1} \\varepsilon_t = (1 + \\phi L + \\phi^2 L^2 + \\cdots)\\varepsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\]\n\\[\n\\text{or} \\quad y_t = \\mu + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{9.3}\\]\nWhat we have shown is that, if \\(\\{y_t\\}\\) is a covariance-stationary solution to the stochastic difference equation (9.1) or (9.2), then \\(y_t\\) has the moving-average representation as in (9.3). Conversely, if \\(y_t\\) has the representation (9.3), then it satisfies the difference equation.\nThe condition \\(\\abs{\\phi}&lt;1,\\) which is the stability condition associated with the first-degree polynomial equation \\(1-\\phi z=0,\\) is called the stationary condition in the context of autoregressive processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-2-absphi1",
    "href": "TS02_AR-MA-representation.html#case-2-absphi1",
    "title": "9  AR(1) and Its MA representation",
    "section": "9.2 Case 2: \\(\\abs{\\phi}>1\\)",
    "text": "9.2 Case 2: \\(\\abs{\\phi}&gt;1\\)\nBy shifting time forward by one period (i.e., by replacing \\(t\\) by \\(t+1\\)), multiplying both sides by \\(\\phi^{-1}\\), and rearranging, the stochastic difference equation (9.2) can be written as\n\\[\ny_t - \\mu = \\phi^{-1}(y_{t+1} - \\mu) - \\phi^{-1} \\varepsilon_{t+1}.\n\\] Keep this substitution,\n\\[\ny_{t+1} - \\mu = \\phi^{-1}(y_{t+2} - \\mu) - \\phi^{-1} \\varepsilon_{t+2}\n\\] then\n\\[\ny_t - \\mu = \\phi^{-2}(y_{t+2} - \\mu) - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.\n\\]\nSubstituting \\((y_{t+2} - \\mu)\\) for the corresponding next period equation:\n\\[\n\\begin{aligned}\ny_{t+2} - \\mu &= \\phi^{-1}(y_{t+3} - \\mu) - \\phi^{-1} \\varepsilon_{t+3}, \\\\\ny_t - \\mu &= \\phi^{-3}(y_{t+3} - \\mu) - \\phi^{-3}\\varepsilon_{t+3} - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.  \\\\\n\\end{aligned}\n\\]\nThen likewise for \\((y_{t+3} - \\mu)\\) and so on. Iterating \\(k\\) times, we get the following representation:\n\\[\n\\begin{aligned}\ny_t - \\mu &= \\phi^{-k}(y_{t+k} - \\mu) - \\phi^{-k}\\varepsilon_{t+k}  - \\phi^{-k+1}\\varepsilon_{t+k-1} - \\cdots - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1} \\\\\n&= \\phi^{-k}(y_{t+k} - \\mu) - \\sum_{j=1}^k \\phi^{-j}\\varepsilon_{t+j}.\n\\end{aligned}\n\\] As \\(k\\to\\infty\\), \\(\\phi^{-k}\\to 0\\), we have\n\\[\ny_t = \\mu - \\sum_{j=1}^{\\infty} \\phi^{-j} \\varepsilon_{t+j}.\n\\]\nThat is, the current value of \\(y\\) is a moving average of future values of \\(\\varepsilon\\). The infinite sum is well defined because the sequence \\(\\{\\phi^{-j}\\}\\) is absolutely summable if \\(|\\phi| &gt; 1\\). This is a feasible representation but unnatural. Moreover, it is unstable as the initial condition should now be set into the future as for example \\(y_T=0\\) as \\(T\\to\\infty\\) which is not likely to be the case.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-3-absphi1",
    "href": "TS02_AR-MA-representation.html#case-3-absphi1",
    "title": "9  AR(1) and Its MA representation",
    "section": "9.3 Case 3: \\(\\abs{\\phi}=1\\)",
    "text": "9.3 Case 3: \\(\\abs{\\phi}=1\\)\nThe stochastic difference equation has no covariance-stationary solution. For example, if \\(\\phi = 1\\), the stochastic difference equation becomes\n\\[\n\\begin{aligned}\ny_t &= c + y_{t-1} + \\varepsilon_t \\\\\n    &= c + (c + y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\quad \\text{(since } y_{t-1} = c + y_{t-2} + \\varepsilon_{t-1}) \\\\\n    &= c + (c + (c + y_{t-3} + \\varepsilon_{t-2}) + \\varepsilon_{t-1}) + \\varepsilon_t, \\quad \\text{etc.}\n\\end{aligned}\n\\]\nRepeating this type of successive substitution \\(j\\) times, we obtain\n\\[\ny_t - y_{t-j} = c \\cdot j + (\\varepsilon_t + \\varepsilon_{t-1}+ \\varepsilon_{t-2} + \\cdots + + \\varepsilon_{t-j+1})\n\\] If \\(\\{\\varepsilon_t\\}\\) is independent white noise, \\(\\{y_t - y_{t-j}\\}\\) is a random walk with drift \\(c.\\)\nUnit root processes are non-stationary not because of the presence of a linear deterministic trend but because they are driven by a stochastic trend which makes their variance time dependent and are called Difference Stationary processes as opposed to Trend Stationary processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#references",
    "href": "TS02_AR-MA-representation.html#references",
    "title": "9  AR(1) and Its MA representation",
    "section": "References",
    "text": "References\n\n§6.2, F. Hayashi (2021), Econometrics, Princeton University Press, IBSN: 9780691010182.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html",
    "href": "TS03_finite-property.html",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "",
    "text": "10.1 Finite Sample Classical Assumptions of OLS\nIn this section, we give a complete listing of the finite sample, or small sample, properties of OLS under standard assumptions.\nIn the notation \\(x_{tj}\\), \\(t\\) denotes the time period, and \\(j\\) is a label to indicate one of the \\(K\\) explanatory variables.\nFor example, a Finite Distributed Lag (FDL) model of order two\n\\[\ny_t = \\alpha_0 + \\delta_0 z_t + \\delta_1 z_{t-1} + \\delta_2 z_{t-2} + u_t,\n\\] is obtained by setting \\(x_{t1}=z_t,\\) \\(x_{t2}=z_{t-1},\\) and \\(x_{t3}=z_{t-2}.\\)\nFor the remaining assumptions, let \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) denote the set of all independent variables at time \\(t.\\) Further, \\(\\bX\\) denote the collection of all independent variables for all time periods. It is useful to think of \\(\\bX\\) as being an array, with \\(n\\) rows and \\(k\\) columns.\n\\[\n\\bX = \\begin{bmatrix}\n\\bx_1 \\\\\n\\bx_2 \\\\\n\\vdots \\\\\n\\bx_n\n\\end{bmatrix}\n= \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1K} \\\\\nx_{21} & x_{22} & \\cdots & x_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{nK} \\\\\n\\end{bmatrix}\n\\]\nAssumption 10.2 allows the explanatory variables to be correlated, but it rules out perfect correlation in the sample.\nAssumption 10.3 implies that the error at time \\(t\\), \\(u_t,\\) is uncorrelated with each explanatory in every time period. This is called the strict exogeneity.\nA relaxed version is contemporaneously exogenous, which only requires \\(u_t\\) to be uncorrelated with the explanatory variables dated at time \\(t\\): in conditional mean terms,\n\\[\n\\E(u_t\\mid x_{t1}, x_{t2}, \\ldots, x_{tK} ) = \\E(u_t\\mid \\bx_t) = 0.\n\\] Contemporaneously exogeneity is much weaker than strict exogeneity because it puts no restrictions on how \\(u_t\\) is related to the explanatory variables in other time periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#finite-sample-classical-assumptions-of-ols",
    "href": "TS03_finite-property.html#finite-sample-classical-assumptions-of-ols",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "",
    "text": "Assumption 10.1 (Linear in Parameters) The stochastic process \\(\\{(x_{t1}, x_{t2}, \\ldots, x_{tK}, y_t): t=1,2,\\ldots, n\\}\\) follows the linear model\n\\[\ny_t = \\beta_0 + \\beta_1x_{t1} + \\beta_2x_{t2} + \\cdots + + \\beta_Kx_{tK} + u_t,\n\\]\nwhere \\(\\{u_t: t=1,2,\\ldots, n\\}\\) is the sequence of errors or disturbances. Here \\(n\\) is the number of observations (time periods).\n\n\n\n\n\n\n\nAssumption 10.2 (No Perfect Collinearity) In the sample (and therefore in the underlying time series process), no independent variable is constant nor a perfect linear combination of the others.\n\n\n\nAssumption 10.3 (Zero Conditional Mean) For each \\(t\\), the expected value of the error \\(u_t,\\) given the explanatory variables for all time periods, is zero. Mathematically,\n\\[\n\\E(u_t\\mid \\bX) = 0,\\, t=1,2,\\ldots,n.\n\\]\n\n\n\n\n\nAssumption 10.4 (Homoskedasticity) Conditional on \\(\\bX,\\) the variance of \\(u_t\\) is the same for all \\(t:\\) \\(\\var(u_t\\mid \\bX)=\\var(u_t)=\\sigma^2,\\) \\(t=1,2,\\ldots,n.\\)\n\n\nAssumption 10.5 (No Serial Correlation) Conditional on \\(\\bX,\\) the errors in two different time periods are uncorrelated: \\(\\cor(u_t, u_s\\mid \\bX)=0,\\) for all \\(t\\ne s.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#finite-sample-properties-of-ols",
    "href": "TS03_finite-property.html#finite-sample-properties-of-ols",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.2 Finite Sample Properties of OLS",
    "text": "10.2 Finite Sample Properties of OLS\nUnder Assumptions 10.1 through 10.3, the OLS estimators are unbiased:\n\\[\n\\E(\\hat{\\beta}_j\\mid \\bX) = \\E(\\hat{\\beta}_j) = \\beta_j, \\, j=0,1,\\ldots,K.\n\\]\n\nUnder Assumptions 10.1 through 10.5, the OLS estimators are the best linear unbiased estimators conditional on \\(\\bX.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "href": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.3 Violation of Strict Exogeneity",
    "text": "10.3 Violation of Strict Exogeneity\nStrict exogeneity (Assumption 10.3) requires \\(u_t\\) to be uncorrelated with \\(\\bx_s\\), for \\(s=1,2,\\ldots,n.\\)\nIf the unobservables at time \\(t\\)are correlated with any of the explanatory variables in any time period, then Zero Conditional Mean fails. Two leading candidates for failure are\n\nomitted variables and\nmeasurement error in some of the regressors.\n\nOther more subtle causes:\n\nLagged effects\nEither the dependent variable of one of the independent variables is based on expectation.\n\n\nExample 10.1 For example, consider a static model to explain a city’s murder rate (\\(mrdrte_t\\)) in terms of police officers per capita (\\(polpc_t\\)):\n\\[\nmrdrte_t = \\beta_0 + \\beta_1\\, polpc_t + u_t.\n\\]\nSuppose that the city adjusts the size of its police force based on past values of the murder rate, such as\n\\[\npolpc_{t+1} = \\delta_0 + \\delta_1 mrdrte_t + v_t.\n\\]\nThis means that, say, \\(polpc_{t+1}\\) might be correlated with \\(u_t\\) (since a higher \\(u_t\\) leads to a higher \\(mrdrte_t\\)). If this is the case, strict exogeneity is violated.\nMathematically,\n\\[\n\\begin{aligned}\n\\E [u_t polpc_{t+1}] &= \\E [u_t (\\delta_0 + \\delta_1 mrdrte_t + v_t)] \\\\\n&= \\delta_1 \\E[u_t\\, mrdrte_t] \\\\\n&= \\delta_1 \\E[u_t (\\beta_0 + \\beta_1\\, polpc_t + u_t)] \\\\\n&= \\delta_1 \\E[u_t^2] \\\\\n&= \\delta_1 \\sigma^2_u.\n\\end{aligned}\n\\]\n\n\nExample 10.2 A general representation with two explanatory variables:\n\\[\ny_t = \\beta_0 + \\beta_1z_{t1} + \\beta_2z_{t2} + u_t.\n\\] Under weak dependence, the condition sufficient for consistency of OLS is\n\\[\n\\E(u_t\\mid z_{t1}, z_{t2}) = 0\n\\] Importantly, the condition does NOT rule out correlation between, say, \\(u_{t-1}\\) and \\(z_{t1}\\). This type of correlation could arise if \\(z_{t1}\\) is related to past \\(y_{t-1}\\), such as\n\\[\nz_{t1} = \\delta_0 + \\delta_1y_{t-1} + v_t.\n\\] For example, \\(z_{t1}\\) might be a policy variable, such as monthly percentage change in the money supply, and this change might depend on last month’s rate of inflation (\\(y_{t-1}\\)). Such a mechanism generally causes \\(z_{t1}\\) and \\(u_{t-1}\\) to be correlated (as can be seen by plugging in for \\(y_{t-1}\\)). This kind of feedback is allowed under contemporaneous exogeneity.\n\nIn the social sciences, many explanatory variables may very well violate the strict exogeneity assumption. For example, the amount of labor input might not be strictly exogenous, as it is chosen by the farmer, and the farmer may adjust the amount of labor based on last year’s yield. Policy variables, such as growth in the money supply, expenditures on welfare, and highway speed limits, are often influenced by what has happened to the outcome variable in the past.\nThere are similar considerations in distributed lag models. Usually, we do not worry that \\(u_t\\) might be correlated with past \\(z\\) because we are controlling for past \\(z\\) in the model. But feedback from \\(u\\) to future \\(z\\) is always an issue.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "href": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.4 Violation of No Serial Correlation",
    "text": "10.4 Violation of No Serial Correlation\nWhen Assumption 10.5 is false, we say that the error suffers from serial correlation, or autocorrelation, because they are correlated across time.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html",
    "href": "TS03_asymptotic-property.html",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "",
    "text": "11.1 Large sample assumptions of OLS\nIt is equivalent to say that \\(u_t\\) has zero unconditional mean and is uncorrelated with each \\(x_{tj}, j=1,\\ldots,K\\):\n\\[\n\\E(u_t) = 0, \\cov(x_{tj}, u_t)=0, j=1,\\ldots,K.\n\\]\nNote that the condition is only on the explanatory variables at time \\(t.\\)\nWe condition only on the explanatory variables in the time periods coinciding with \\(u_t\\) and \\(u_s.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html#large-sample-assumptions-of-ols",
    "href": "TS03_asymptotic-property.html#large-sample-assumptions-of-ols",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "",
    "text": "Assumption 11.1 (Linear and Weak Dependence) We assume the model is exactly as in Assumption 10.1, but now we add the assumption that \\(\\{(\\bx_t, y_t): t=1, 2, \\ldots\\}\\) is stationary and weakly dependent. In particular, the law of large numbers and the central limit theorem can be applied to sample averages.\n\n\nAssumption 11.2 (No Perfect Collinearity) Same as Assumption 10.2.\n\n\nAssumption 11.3 (Zero Conditional Mean) The explanatory variables \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) are contemporaneously exogenous:\n\\[\n\\E(u_t\\mid \\bx_t) = 0.\n\\]\n\n\n\n\nAssumption 11.4 (Homoskedasticity) The errors are contemporaneously homoskedastic, that is, conditional on \\(\\bx_t,\\) the variance of \\(u_t\\) is the same for all \\(t:\\)\n\\[\n\\var(u_t\\mid \\bx_t)=\\var(u_t)=\\sigma^2,\\, t=1,2,\\ldots,n.\n\\]\n\n\n\nAssumption 11.5 (No Serial Correlation) Conditional on \\(\\bx_t\\) and \\(\\bx_s\\) the errors in two different time periods are uncorrelated:\n\\[\n\\cor(u_t, u_s\\mid \\bx_t, \\bx_s)=0, \\text{ for all } t\\ne s.\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html#large-sample-properties-of-ols",
    "href": "TS03_asymptotic-property.html#large-sample-properties-of-ols",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "11.2 Large sample properties of OLS",
    "text": "11.2 Large sample properties of OLS\nUnder Assumptions 11.1 through 11.3, the OLS estimators are consistent: \\(\\mathrm{plim}\\, \\hat{\\beta}_j = \\beta_j,\\) \\(j=1,\\ldots,K.\\)\n\nOLS estimators are consistent, but not necessarily unbiased.\nWe have weakened the sense in which the explanatory variables must be exogenous, but weak dependence is required in the underlying time series.\n\n\nUnder Assumptions 11.1 through 11.5, the OLS estimators are asymptotically normally distributed. Further, the usual OLS standard errors, \\(t\\) statistics, \\(F\\) statistics, and \\(LM\\) statistics are asymptotically valid.\nModels with trending explanatory variables can effectively satisfy Assumptions 11.1 through 11.5, provided they are trend stationary. As long as time trends are included in the equations when needed, the usual inference procedures are asymptotically valid.\n\n\nExample 11.1 The AR(1) model cannot satisfy the strict exogeneity assumption (11.3).\nConsider the AR(1) model,\n\\[\ny_t = \\beta_0 + \\beta_1y_{t-1} + u_t,\n\\] where the error \\(u_t\\) has a zero expected value, given all past values of \\(y:\\)\n\\[\n\\E(u_t\\mid y_{t-1},y_{t-2},\\ldots)=0.\n\\tag{11.1}\\]\nCombined, these two equations imply that\n\\[\n\\E(y_t\\mid y_{t-1},y_{t-2},\\ldots) = \\E(y_t\\mid y_{t-1}) =  \\beta_0 + \\beta_1y_{t-1}.\n\\] It means that, once \\(y\\) lagged one period has been controlled for, no further lags of \\(y\\) affect the expected value of \\(y_t.\\)\nBecause \\(\\bx_t\\) contains only \\(y_{t-1},\\) Equation 11.1 implies that contemporaneous exogeneity holds.\nBy contrast, the strict exogeneity assumption is needed for unbiasedness. Since \\(u_t\\) is always correlated with \\(y_t\\) (\\(\\cov(u_t, y_t)=\\var(u_t)&gt;0\\)), strict exogeneity assumption cannot be true.\nTherefore, a model with a lagged dependent variable cannot satisfy the strict exogeneity assumption.\nFor the weak dependence of \\(y_t\\) to hold, we must assume that \\(\\abs{\\beta_1}&lt;1.\\) If this condition holds, then Assumptions 11.1 through 11.3 implies that the OLS estimator from the regression of \\(y_t\\) on \\(y_{t-1}\\) produces consistent estimators of \\(\\beta_0\\) and \\(\\beta_1.\\)\nUnfortunately, \\(\\hat{\\beta}_1\\) s biased, and this bias can be large if the sample size is small or if \\(\\beta_1\\) is near 1. For \\(\\beta_1\\) near 1, \\(\\hat{\\beta}_1\\) can have a severe downward bias. In moderate to large samples, \\(\\hat{\\beta}_1\\) should be a good estimator of \\(\\beta_1.\\)\n\n\nEquation 11.1 also indicates no serial correlation in \\(u_t.\\)\n\nProof. To show that the errors \\(\\{u_t\\}\\) are serially uncorrelated, we must show that \\(\\E(u_tu_s\\mid \\bx_t, \\bx_s)=0\\) for \\(t\\ne s.\\) The explanatory variable at \\(t\\) is \\(y_{t-1}\\), hence we need to prove\n\\[\n\\E(u_tu_s\\mid y_{t-1}, y_{s-1})=0\n\\] Assume \\(s&lt;t,\\) rewrite \\(u_s = y_s-beta_0-beta_1y_{s-1},\\) that is, \\(u_s\\) is a function of \\(y\\) dated before time \\(t.\\)\nBy Equation 11.1, we have\n\\[\n\\E(u_t\\mid u_s, y_{t-1}, y_{s-1}) = 0.\n\\] Then \\[\n\\begin{aligned}\n&\\phantom{=}\\E(u_tu_s \\mid u_s, y_{t-1}, y_{s-1} ) \\\\\n&= u_s\\E(u_t \\mid u_s, y_{t-1} , y_{s-1} )  \\quad (\\text{taking out what is known})\\\\\n&= 0 .\n\\end{aligned}\n\\] By ILE (Law of Iterated Expectations),\n\\[\n\\begin{aligned}\n\\E(u_tu_s \\mid y_{t-1}, y_{s-1} )\n&= \\E \\left[\\E(u_tu_s \\mid u_s, y_{t-1}, y_{s-1} ) \\mid y_{t-1}, y_{s-1} \\right] = 0.\n\\end{aligned}\n\\] Conclusion: as long as only one lag of \\(y\\) appears in \\(\\E(y_t\\mid y_{t-1}, y_{t-2}, \\ldots)\\), the errors \\(\\{u_t\\}\\) must be serially uncorrelated.\n\n\n□\n\n\nExample 11.2 In a static model \\(y_t = \\beta_0+\\beta_1z_t+u_t,\\) the homoskedasticity assumption requires that\n\\[\n\\var(u_t\\mid z_t) = \\var(y_t\\mid z_t) = \\sigma^2.\n\\]\nIn the AR(1) model, \\(y_t = \\beta_0+\\beta_1y_{t-1}+u_t,\\) the homoskedasticity assumption is\n\\[\n\\var(u_t\\mid y_{t-1}) = \\var(y_t\\mid y_{t-1}) = \\sigma^2.\n\\] If we have the model\n\\[\ny_t = \\beta_0 + \\beta_1z_t + \\beta_2y_{t-1} + \\beta_3z_{t-1}+u_t,\n\\] the homoskedasticity assumption is\n\\[\n\\var(u_t\\mid z_t, y_{t-1}, z_{t-1}) = \\var(y_t\\mid z_t, y_{t-1}, z_{t-1}) = \\sigma^2,\n\\] so that the variance of \\(u_t\\) cannot depend on \\(z_t, y_{t-1},\\) or \\(z_{t-1}\\) (or some other function of time).\nGenerally, whatever explanatory variables appear in the model, we must assume that the variance of \\(y_t\\) given these explanatory variables is constant. If the model contains lagged \\(y\\) or lagged explanatory variables, then we are explicitly ruling out dynamic forms of heteroskedasticity. But, in a static model, we are only concerned with \\(\\var(y_t\\mid z_t).\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS04_highly-persistent.html",
    "href": "TS04_highly-persistent.html",
    "title": "12  Highly Persistent TS",
    "section": "",
    "text": "Provided the time series we use are weakly dependent, usual OLS inference procedures are valid under assumptions weaker than the classical linear model assumptions. Unfortunately, many economic time series cannot be characterized by weak dependence.\nUsing time series with strong dependence in regression analysis poses no problem, if the CLM assumptions in finite samples hold. But the usual inference procedures are very susceptible to violation of these assumptions when the data are not weakly dependent, because then we cannot appeal to the law of large numbers and the central limit theorem. In this section, we provide some examples of highly persistent (or strongly dependent) time series and show how they can be transformed for use in regression analysis.\nWhen the time series are highly persistent (they have unit roots), we must exercise extreme caution in using them directly in regression models. An alternative to using the levels is to use the first differences of the variables. For most highly persistent economic time series, the first difference is weakly dependent. Using first differences changes the nature of the model, but this method is often as informative as a model in levels. When data are highly persistent, we usually have more faith in first-difference results.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Highly Persistent TS</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html",
    "href": "PA01_Hetero_lm.html",
    "title": "13  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "13.1 Linearity and Heterogeneity",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html#linearity-and-heterogeneity",
    "href": "PA01_Hetero_lm.html#linearity-and-heterogeneity",
    "title": "13  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "13.1.1 Models with Homogeneous Slopes\nWe begin our journey where standard textbooks and first-year foundational courses in econometrics leave off. The “standard” linear models considered in such courses often assume homogeneity in individual responses to covariates (e.g., Hansen (2022)). A common cross-sectional specification is:\n\\[\ny_i = \\bbeta'\\bx_i + u_{i},\n\\tag{13.1}\\] where \\(i=1, \\dots, N\\) indexes cross-sectional units.\nIn panel data, models often include unit-specific \\((i)\\) and time-specific \\((t)\\) intercepts while maintaining a common slope vector \\(\\bbeta\\):\n\\[\ny_{it} = \\alpha_i + \\delta_t +  \\bbeta'\\bx_{it} + u_{it}.\n\\tag{13.2}\\]\n\n\n13.1.2 Heterogeneity in Slopes.\nHowever, modern economic theory rarely supports the assumption of homogeneous slopes \\(\\bbeta.\\) Theoretical models recognize that observationally identical individuals, firms, and countries can respond differently to the same stimulus. In a linear model, this requires us to consider more flexible models with heterogeneous coefficients:\n\nCross-sectional model (13.1) generalizes to\n\\[\ny_i = \\bbeta_{i}'\\bx + u_i.\n\\tag{13.3}\\]\nPanel data model (13.2) generalizes to\n\\[\ny_{it}  = \\bbeta_{it}'\\bx_{it} + u_{it}.\n\\tag{13.4}\\]\n\nSuch models are worth studying, as they naturally arise in a variety of contexts:\n\nStructural models with parametric restrictions: Certain parametric restrictions yield linear relationships in coefficients. An example is given by firm-level Cobb-Douglas production functions where firm-specific productivity differences induce heterogeneous coefficients (Combes et al. (2012); Sury (2011)).\nBinary covariates and interaction terms: if all covariates are binary and all interactions are included, a linear model encodes all treatment effects without loss of generality (see, e.g., Wooldridge (2005)).\nLog-linearized models: Nonlinear models may be approximated by linear models around a steady-state. For example, Heckman and Vytlacil (1998) demonstrate how the nonlinear Card (2001) education model simplifies to a heterogeneous linear specification after linearization.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html#references",
    "href": "PA01_Hetero_lm.html#references",
    "title": "13  Linear Models with Heterogeneous Coefficients",
    "section": "References",
    "text": "References\n\nVladislav Morozov, Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-introduction.html\nVladislav Morozov, GitHub course repository, https://github.com/vladislav-morozov/econometrics-heterogeneity/blob/fix/linear/src/linear/linear-introduction.qmd\n\n\n\n\n\nCard, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga, and Sébastien Roux. 2012. “The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random coefficient model.” Journal of Human Resources 33 (4): 974–87.\n\n\nSury, Tavneet. 2011. “Selection and Comparative Advantage in Technology Adoption.” Econometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  }
]