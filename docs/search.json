[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metrics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability_theory.html",
    "href": "probability_theory.html",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "1.1 Notations\n\\(\\Omega\\): A sample space, a set of possible outcomes of a random experiment.\n\\(X\\): A random variable, a function from the sample space to the real numbers: \\(X: \\Omega \\to \\R\\).\nStochastic Process\nA stochastic process is a family of random variables, \\(\\{X(t): t\\in T\\},\\) where \\(t\\) usually denotes time. That is, at every time \\(t\\) in the set \\(T\\), a random number \\(X(t)\\) is observed.\nThe state space, \\(S\\), is the set of real values that \\(X(t)\\) can take.\nYou can think of “conditioning” as “changing the sample space.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "probability_theory.html#notations",
    "href": "probability_theory.html#notations",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "Discrete-time process: \\(T=\\{0,1,2,3\\}\\), the discrete process is \\(\\{X(0), X(1), X(2), \\dots\\}\\)\nContinuous-time process: \\(T=[0, \\infty]\\) or \\(T=[0, K]\\) for some \\(K\\).\n\n\n\n\nFrom unconditional to conditional\n\\[\n  \\P (B) = \\P(B\\mid \\Omega)\n  \\]\n\\(\\Omega\\) denotes the sample space, \\(\\P (B) = \\P(B\\mid \\Omega)\\) just means that we are looking for the probability of the event \\(B\\), out of all possible outcomes in the set \\(\\Omega.\\)\nPartition Theorem\n\\[\n  \\P(A) = \\sum_{i=1}^m \\P(A\\cap B_i) = \\sum_{i=1}^m \\P(A\\mid B_i) \\P(B_i)\n  \\]\nwhere \\(B_i, i=1,\\dots,m,\\) are a partition of \\(\\Omega.\\) The intuition behind the Partition Theorem is that the whole is the sum of its parts.\n\nA partition of \\(\\Omega\\) is a collection of mutually exclusive events whose union is \\(\\Omega.\\)\nThat is, sets \\(B_1, B_2, \\dots, B_m\\) form a partition of \\(\\Omega\\) if\n\\[\n  \\begin{split}\n  B_i \\cap B_j &= \\emptyset \\;\\text{ for all $i, j$ with $i\\ne j,$} \\\\\n  \\text{and }  \\bigcup_{i=1}^m B_i &= B_1 \\cup B_2 \\cup \\dots \\cup B_m = \\Omega.\n  \\end{split}\n  \\]\nBayes’ Theorem\nBayes’ Theorem allows us to invert a conditional statement, i.e., the express \\(\\P(B\\mid A)\\) in terms of \\(\\P(A\\mid B).\\)\nFor any events \\(A\\) and \\(B\\):\n\\[\n  \\P(B\\mid A) = \\frac{\\P(A\\cap B)}{\\P(A)} = \\frac{\\P(A\\mid B)\\P(B)}{\\P(A)}\n  \\]\nGeneralized Bayes’ Theorem\nFor any partition member \\(B_j\\),\n\\[\n  \\P(B_j\\mid A) = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\P(A)} = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\sum_{i=1}^m\\P(A\\mid B_i)\\P(B_i)}\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html",
    "href": "conditional_expectation.html",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "2.1 Generalized Adam’s Law\nIdentities for conditional expectations\n\\[\n  \\E\\left[ \\E[Y \\mid g(X)] \\mid f(g(X)) \\right] = \\E[Y\\mid f(g(X))]\n\\]\nShow that the following identify is a special case of the Generalized Adam’s Law:\n\\[\n\\E[\\E[Y\\mid X,Z] \\mid Z] = \\E[Y\\mid Z]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#generalized-adams-law",
    "href": "conditional_expectation.html#generalized-adams-law",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "Proof. If we take \\(f(g(x, z)) = z\\) and \\(g(x, z) = (x, z)\\) in the generalized Adam’s Law, we get the result.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#projection-interpretation",
    "href": "conditional_expectation.html#projection-interpretation",
    "title": "2  Conditional Expectation",
    "section": "2.2 Projection interpretation",
    "text": "2.2 Projection interpretation\nConditional expectation gives the best prediction\n\nTheorem 2.1 (Conditional expectation minimizes MSE) Suppose we have random element \\(X\\in \\Xcal\\) and random variable \\(Y\\in\\R.\\) Let \\(g(x)=\\E[Y\\mid X=x].\\) Then\n\\[\ng(x) = \\underset{f}{\\arg\\min}\\, \\E(Y-f(X))^2\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\E(Y-f(X))^2 &= \\E\\left[(Y-\\E[Y\\mid X]) + (\\E[Y\\mid X]-f(X)) \\right]^2 \\quad (\\text{plus and minus } \\E[Y\\mid X]) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2 \\\\\n& \\phantom{=}\\; + 2\\E[(Y-\\E[Y\\mid X])(\\underbrace{\\E[Y\\mid X]-f(X)}_{h(X)})] \\quad \\Bigl(\\E\\bigl[ \\bigl(Y-\\E[Y\\vert X]\\bigr) h(X) \\bigr] = 0\\Bigr) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2\n\\end{split}\n\\]\nThe first term is independent of \\(f\\), and the second term is minimized by taking \\(f(x)=\\E[Y\\mid X].\\)\n\n□\n\n\nIf we think of \\(\\E[Y\\mid X]\\) as a prediction/projection for \\(Y\\) given \\(X\\), then \\((Y-\\E[Y\\mid X])\\) is the residual of that prediction.\nIt’s helpful to think of decomposing \\(Y\\) as\n\\[\nY = \\underbrace{\\E[Y\\mid X]}_\\text{best prediction for $Y$ given $X$} + \\underbrace{(Y-E[Y\\mid X])}_\\text{residual}\n\\]\nNote that the two terms on the RHS are uncorrelated, by the projection interpretation.\nSince variance is additive for uncorrelated random variables (i.e., if \\(X\\) and \\(Y\\) are uncorrelated, then \\(\\var(X+Y)=\\var(X)+\\var(Y)\\)), we get the following theorem\n\nTheorem 2.2 (Variance decomposition with projection) For any random variable \\(X\\in \\Xcal\\) and random variable \\(Y\\in \\R,\\) we have\n\\[\n\\var(Y) = \\var(\\E[Y\\mid X]) + \\var(Y-\\E[Y\\mid X])\n\\]\n\nTheorem 2.1 tells us that \\(\\E[Y\\mid X]\\) is the best approximation of \\(Y\\) we can get from \\(X.\\) We can also think of \\(\\E[Y\\mid X]\\) as a “less random” version of \\(Y,\\) since \\(\\var(\\E[Y\\vert X]) \\le \\var(Y).\\)\nWe can say that \\(\\E[Y\\mid X]\\) only keeps the randomness in \\(Y\\) that is predictable from \\(X.\\) \\(\\E[Y\\mid X]\\) is a deterministic function of \\(X,\\) so there’s no other source of randomness in \\(\\E[Y\\mid X].\\)\n\nTheorem 2.3 (Projection interpretation) For any \\(h:\\Xcal \\to \\R,\\)\n\\[\n\\E[(Y-\\E[Y\\mid X])h(X)]=0\n\\]\n\nTheorem 2.3 says that the residual of \\(\\E[Y\\mid X]\\) is “orthogonal” to every random variable of the form \\(h(X).\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#keeping-just-what-is-needed",
    "href": "conditional_expectation.html#keeping-just-what-is-needed",
    "title": "2  Conditional Expectation",
    "section": "2.3 Keeping just what is needed",
    "text": "2.3 Keeping just what is needed\n\nTheorem 2.4 For any random variables \\(X, Y\\in \\R,\\)\n\\[\n\\E[XY] = \\E[X\\E[Y\\mid X]]\n\\]\n\nOne way to think about this is that for the purposes of computing \\(\\E [XY],\\) we only care about the randomness in \\(Y\\) that is predictable from \\(X\\).\n\nProof. \n\\[\n\\begin{split}\n\\E[XY] &= \\E[\\E[XY\\mid X]] \\quad (\\text{LIE}) \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Taking out what is known})\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof1). We can show this using the projection interpretation:\n\\[\n\\begin{split}\n\\E[XY] &= \\E\\left[ X \\left(\\E[Y\\mid X] + \\underbrace{Y-\\E[Y\\mid X]}_\\text{residuals uncorrelated with $X$} \\right)\\right] \\\\[1em]\n&= \\E[X\\E[Y\\mid X]] + \\E[X(Y-\\E[Y\\mid X])] \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Projection interpretation, } \\E[X(Y-\\E[Y\\mid X])]=0)\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof2). \n\\[\n\\begin{split}\n\\E[X\\E[Y\\mid X]] &= \\sum_x x\\E[Y\\mid X=x] \\P(X=x) \\\\\n&= \\sum_x\\sum_y xy\\P(Y=y\\mid X=x)\\P(X=x) \\\\\n&= \\sum_x\\sum_y xy \\P(Y=y, X=x)\n\\end{split}\n\\]\n\n□\n\n\nA more general case of \\(\\E[XY] = \\E[X\\E[Y\\mid X]]\\) is",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#references",
    "href": "conditional_expectation.html#references",
    "title": "2  Conditional Expectation",
    "section": "References",
    "text": "References\n\nDavid S. Rosenberg. Conditional Expectations: Review and Lots of Examples, https://davidrosenberg.github.io/ttml2021fall/background/conditional-expectation-notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "measure_theory.html",
    "href": "measure_theory.html",
    "title": "3  Measure Theory",
    "section": "",
    "text": "3.1 Definitions\nWe denote the collection of subsets, or power set, of a set \\(X\\) by \\(\\Pcal(X).\\)\nThe Cartesian product, or product, of sets \\(X, Y\\) is the collection of all ordered pairs\n\\[\nX\\times Y = \\{(x,y): x\\in X, y\\in Y\\}.\n\\]\nA topological space is a set equipped with a collection of open subsets that satisfies appropriate conditions.\nThe complement of an open set in \\(X\\) is called a closed set, and \\(\\Tcal\\) is called a topology on \\(X.\\)\nA \\(\\sigma\\)-algebra on a set \\(X\\) is a collection of subsets of a set \\(X\\) that contains \\(\\emptyset\\) and \\(X\\), and is closed under complements, finite unions, countable unions, and countable intersections.\nA measurable space \\((X, \\Acal)\\) is an non-empty set \\(X\\) equipped with a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X.\\)\nDifference between a measurable space and \\(\\sigma\\)-algebra:\nA measure \\(\\mu\\) is a countably additive, non-negative, extended real-valued function defined on a \\(\\sigma\\)-algebra.\nA measure space \\((X, \\Acal, \\mu)\\) consist of a set \\(X\\), a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X\\), and a measure \\(\\mu\\) defined on \\(\\Acal.\\) When \\(\\Acal\\) and \\(\\mu\\) are clear from the context, we will refer to the measure space \\(X\\).\nAn abstract probability space \\((\\Omega, \\Fcal, \\P)\\)\nA random variable is any function \\(X: \\Omega \\to \\Xcal.\\) We say that \\(X\\) has distribution \\(P,\\) and write \\(X\\sim P\\), if\n\\[\n\\P(X\\in B) = \\P(\\{\\omega: X(\\omega)\\in B\\}) = \\P(B)\n\\]\nWe say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue measure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any function \\(f\\).\nThe expectation of a random variable is defined as an integral with respect to \\(\\P\\):\n\\[\n\\E[X] = \\int X(\\omega)\\, \\mathrm d \\P(\\omega),\n\\]\nand\n\\[\n\\E[f(X,Y)] = \\int f(X(\\omega), Y(\\omega))\\, \\mathrm d \\P(\\omega).\n\\]\nA measure \\(\\mu\\) on a measurable space \\((X,\\Acal)\\) is a function\n\\[\n\\mu: \\Acal \\to [0, \\infty]\n\\]\nsuch that\n\\[\n\\mu \\left( \\bigcup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nA measure \\(\\mu\\) on a set \\(X\\) is\nReferences:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "measure_theory.html#definitions",
    "href": "measure_theory.html#definitions",
    "title": "3  Measure Theory",
    "section": "",
    "text": "Definition 3.1 (Topological Space) A topological space \\((X, \\Tcal)\\) is a set \\(X\\) and a collection \\(\\Tcal \\subset \\Pcal(X)\\) of subsets of \\(X,\\) called open sets, such that\n\n\\(\\emptyset, X \\in \\Tcal;\\)\nIf \\(\\{U_\\alpha \\in \\Tcal: \\alpha \\in I \\}\\) is an arbitrary collection of open sets, then their union\n\\[\n\\bigcup_{\\alpha\\in I} U_\\alpha \\in \\Tcal\n\\]\nis open;\nIf \\(\\{U_i \\in \\Tcal: i=1,2,\\dots,N \\}\\) is a finite collection of open sets,Then their intersection\n\\[\n\\bigcap_{i=1}^N U_i \\in \\Tcal\n\\]\nis open.\n\n\n\n\n\nDefinition 3.2 A \\(\\sigma\\)-algebra on a set \\(X\\) is a collection \\(\\Acal\\) of subsets of a set \\(X\\) such that:\n\n\\(\\emptyset, X \\in \\Acal;\\)\nIf \\(A\\in\\Acal\\) then \\(A^c\\in\\Acal;\\)\nIf \\(A_i\\in\\Acal\\) then \\[\n\\bigcup_{i=1}^\\infty A_i \\in \\Acal, \\quad \\bigcap_{i=1}^\\infty A_i \\in \\Acal.\n\\]\n\n\n\nExample 3.1 If \\(X\\) is a set, then \\(\\{\\emptyset,X\\}\\) and \\(\\Pcal(X)\\) are \\(\\sigma\\)-algebras on \\(X\\); they are the smallest and largest \\(\\sigma\\)-algebras on \\(X\\), respectively.\n\n\n\n\nThe complement of a measurable set is measurable, but the complement of an open set is not, in general, open, excluding special cases such as the discrete topology \\(\\Tcal = \\Pcal (X)\\)\nCountable intersections and unions of measurable sets are measurable, but only finite intersections of open sets are open while arbitrary (even uncountable) unions of open sets are open.\n\n\n\n\n\n\\(\\omega\\in \\Omega\\) is called an outcome;\n\\(A\\in \\Fcal\\) is called an event;\n\\(\\P(A)\\) is called the probability of \\(A.\\)\n\\(\\P(\\Omega)=1\\) the sum of probability of all possible outcomes is 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu(\\emptyset)=0;\\)\nIf \\(\\{A_i\\in \\Acal: i\\in \\N\\}\\) is a countable disjoint collection of sets in \\(\\Acal,\\) then\n\n\n\n\nfinite if \\(\\mu(X)&lt;\\infty,\\) and\n\\(\\sigma\\)-finite if \\(X=\\bigcup_{n=1}^\\infty A_n\\) is a countable union of measurable sets \\(A_n\\) with finite measure, \\(\\mu(A_n)&lt;\\infty.\\)\n\n\n\nJ. K. Hunter (2011). Measure Theory. Department of Mathematics, University of California at Davis. https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html",
    "href": "TS01_AR1.html",
    "title": "4  AR(1)",
    "section": "",
    "text": "4.1 Reasons for Delayed Effects and Autocorrelation\nUnlike linear regression, which typically deals with cross-sectional data, time series regression in two ways:\nTime series data often display autocorrelation, or serial correlation of the disturbances across periods.\nIf you plot the residuals and observe that the effect of a given disturbance is carried, at least in part, across periods, then it is a strong signal of serial correlation. It’s like the disturbances exhibiting a sort of “memory” over time.\nConsider the dynamic regression model\n\\[\ny_t = \\beta_1 + \\beta_2x_t + \\beta_3x_{t-1} + \\gamma y_{t-1} + \\varepsilon_t.\n\\]\nAutocorrelation in the error can arise from an autocorrelated omitted variable, or it can arise if a dependent variable \\(y\\) is autocorrelated and this autocorrelation is not adequately explained by the \\(x\\)’s and their lags that are included in the equation.\nThere are several reasons why lagged effects might appear in an empirical model.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#reasons-for-delayed-effects-and-autocorrelation",
    "href": "TS01_AR1.html#reasons-for-delayed-effects-and-autocorrelation",
    "title": "4  AR(1)",
    "section": "",
    "text": "In modeling the response of economic variables to policy stimuli, it is expected that there will be possibly long lags between policy changes and their impacts. The length of lag between changes in monetary policy and its impact on important economic variables such as output and investment has been a subject of analysis for several decades.\nEither the dependent variable or one of the independent variables is based on expectations. Expectations about economic events are usually formed by aggregating new information and past experience. Thus, we might write the expectation of a future value of variable \\(x\\), formed this period, as\n\\[\n  x_t = \\E_t [x^*_{t+1} \\mid z_t, x_{t-1}, x_{t-2}, \\ldots] = g(z_t, x_{t-1}, x_{t-2}, \\ldots) .\n  \\]\nFor example, forecasts of prices and income enter demand equations and consumption equations.\nCertain economic decisions are explicitly driven by a history of related activities.For example, energy demand by individuals is clearly a function not only of current prices and income, but also the accumulated stocks of energy using capital. Even energy demand in the macroeconomy behaves in this fashion–the stock of automobiles and its attendant demand for gasoline is clearly driven by past prices of gasoline and automobiles. Other classic examples are the dynamic relationship between investment decisions and past appropriation decisions and the consumption of addictive goods such as cigarettes and theater performances.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#ar1-visualization",
    "href": "TS01_AR1.html#ar1-visualization",
    "title": "4  AR(1)",
    "section": "4.2 AR(1) Visualization",
    "text": "4.2 AR(1) Visualization\nThe first-order autoregressive process, denoted AR(1), is \\[\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\] where \\(w_t\\) is a strictly stationary and ergodic white noise process with 0 mean and variance \\(\\sigma^2_w\\).\n\n\n\n\n\n\n\n\nFigure 4.1: White noise process with \\(\\sigma=20\\).\n\n\n\n\n\nTo illustrate the behavior of the AR(1) process, Figure 4.2 plots two simulated AR(1) processes. Each is generated using the white noise process et displayed in Figure 4.1.\nThe plot in Figure 4.2(a) sets \\(\\rho=0.5\\) and the plot in Figure 4.2(b) sets \\(\\rho=0.95\\).\nRemarks\n\nFigure 4.2(b) is more smooth than Figure 4.2(a).\nThe smoothing increases with \\(\\rho\\).\n\n\n\n\n\n\n\n\n\nFigure 4.2: Simulated AR(1) processes with positive \\(\\rho\\). (a) \\(\\rho=0.5\\), (b) \\(\\rho=0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\n\n\n\n\n\n\nWe have seen the cases when \\(\\rho\\) is positive, now let’s consider when \\(\\rho\\) is negative. Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.5\\), and Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.95\\,.\\)\nWe see that the sample path is very choppy when \\(\\rho\\) is negative. The different patterns for positive and negative \\(\\rho\\)’s are due to their autocorrelation functions (ACFs).\n\n\n\n\n\n\n\n\nFigure 4.3: Simulated AR(1) processes with negtive \\(\\rho\\). (a) \\(\\rho=-0.5\\), (b) \\(\\rho=-0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\n\n\n\n\n\n\nPossible causes of serial correlation: Incomplete or flawed model specification. Relevant factors omitted from the time series regression are correlated across periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#mathematical-representation",
    "href": "TS01_AR1.html#mathematical-representation",
    "title": "4  AR(1)",
    "section": "4.3 Mathematical Representation",
    "text": "4.3 Mathematical Representation\nLet’s formulate an AR(1) model as follows:\n\\[\n\\begin{align}\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\end{align}\n\\tag{4.1}\\]\nwhere \\(w_t\\) is a white noise series with mean zero and variance \\(\\sigma^2_w\\). We also assume \\(|\\rho|&lt;1\\).\nWe can represent the AR(1) model as a linear combination of the innovations \\(w_t\\).\nBy iterating backwards \\(k\\) times, we get\n\\[\n\\begin{aligned}\n\\varepsilon_t &= \\rho \\,\\varepsilon_{t-1} + w_t \\\\\n&= \\rho\\, (\\rho \\, \\varepsilon_{t-2} + w_{t-1}) + w_t \\\\\n&= \\rho^2 \\varepsilon_{t-2} + \\rho w_{t-1} + w_t \\\\\n&\\quad \\vdots \\\\\n&= \\rho^k \\varepsilon_{t-k} + \\sum_{j=0}^{k-1} \\rho^j \\,w_{t-j} \\,.\n\\end{aligned}\n\\] This suggests that, by continuing to iterate backward, and provided that \\(|\\rho|&lt;1\\) and \\(\\sup_t \\text{Var}(\\varepsilon_t)&lt;\\infty\\), we can represent \\(\\varepsilon_t\\) as a linear process given by\n\\[\n\\color{#EE0000FF}{\\varepsilon_t = \\sum_{j=0}^\\infty \\rho^j \\,w_{t-j}} \\,.\n\\]\n\n\n4.3.1 Expectation\n\\(\\varepsilon_t\\) is stationary with mean zero.\n\\[\nE(\\varepsilon_t) = \\sum_{j=0}^\\infty \\rho^j \\, E(w_{t-j})\n\\]\n\n\n\n4.3.2 Autocovariance\nThe autocovariance function of the AR(1) process is \\[\n\\begin{aligned}\n\\gamma (h) &= \\text{Cov}(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E\\left[\\left(\\sum_{j=0}^\\infty \\rho^j \\,w_{t+h-j}\\right)  \\left(\\sum_{k=0}^\\infty \\rho^k \\,w_{t-k}\\right) \\right] \\\\\n&= \\sum_{l=0}^{\\infty} \\rho^{h+l} \\rho^l \\sigma_w^2 \\\\\n&= \\sigma_w^2 \\cdot \\rho^{h} \\cdot \\sum_{l=0}^{\\infty} \\rho^{2l}  \\\\\n&= {\\color{red} \\frac{\\sigma_w^2 \\cdot \\rho^{h} }{1-\\rho^2} }, \\quad h&gt;0 \\,.\n\\end{aligned}\n\\] When \\(h=0\\), \\[\n{\\color{red} \\gamma(0) = \\var(\\varepsilon_t) = \\frac{\\sigma_w^2}{1-\\rho^2}}\n\\] is the variance of the process \\(\\text{Var}(\\varepsilon_t)\\).\nNote that\n\n\\(\\gamma(0) \\ge |\\gamma (h)|\\) for all \\(h\\). Maximum value at 0 lag.\n\\(\\gamma (h)\\) is symmetric, i.e., \\(\\gamma (-h) = \\gamma (h)\\)\n\nThe autocovariance matrix is\n\\[\n\\bOmega = \\frac{\\sigma_w^2}{1-\\rho^2}\n\\begin{bmatrix}\n1 & \\rho & \\cdots & \\rho^{n-1} \\\\\n\\rho & 1 & \\cdots & \\rho^{n-2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\cdots & 1 \\\\\n\\end{bmatrix} = \\bOmega (\\rho)\n\\]\n\n\n\n4.3.3 Autocorrelation\nThe autocorrelation function (ACF) is given by\n\\[\n{\\color{#337ab7} \\rho(h) = \\frac{\\gamma (h)}{\\gamma (0)} = \\rho^h },\n\\] which is simply the correlation between \\(\\varepsilon_{t+h}\\) and \\(\\varepsilon_{t}\\,.\\)\nThe autocorrelation of the stationary AR(1) 18.3 is a simple geometric decay (\\(\\abs{\\rho}&lt;1\\)):\n\nIf \\(\\abs{\\rho}\\) is small, the autocorrelations decay rapidly to zero with \\(h\\)\nIf \\(\\abs{\\rho}\\) is large (close to unity), then the autocorrelations decay moderately\nThe AR(1) parameter \\(\\abs{\\rho}\\) describes the persistence in the time series\n\nNote that \\(\\rho(h)\\) satisfies the recursion \\[\n\\rho(h) = \\rho\\cdot \\rho(h-1) \\,.\n\\]\n\nFor \\(\\rho &gt;0\\), \\(\\rho(h)=\\rho^h&gt;0\\) observations close together are positively correlated with each other. The larger the \\(\\rho\\), the larger the correlation.\nFor \\(\\rho &lt;0\\), the sign of the ACF \\(\\rho(h)=\\rho^h\\) depends on the time interval.\n\nWhen \\(h\\) is even, \\(\\rho(h)\\) is positive;\nwhen \\(h\\) is odd, \\(\\rho(h)\\) is negative.\n\nThis result means that observations at contiguous time points are negatively correlated, but observations two time points apart are positively correlated.\n\nFor example, if an observation, \\(\\varepsilon_t\\), is positive, the next observation, \\(\\varepsilon_{t+1}\\), is typically negative, and the next observation, \\(\\varepsilon_{t+2}\\), is typically positive. Thus, in this case, the sample path is very choppy.\n\n\nAnother interpretation of \\(\\rho(h)\\) is the optimal weight for scaling \\(\\varepsilon_t\\) into \\(\\varepsilon_{t+h}\\), i.e., the weight, \\(a\\), that minimizes \\(E[(\\varepsilon_{t+h} - a\\,\\varepsilon_{t})^2]\\,.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#references",
    "href": "TS01_AR1.html#references",
    "title": "4  AR(1)",
    "section": "References",
    "text": "References\n\nChap 19 Models with Lagged Variables, Econometric Analysis, Greene 5th Edition, pp 558.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html",
    "href": "TS01_AR_example.html",
    "title": "5  AR – Example",
    "section": "",
    "text": "5.1 Dataset Description\nThis script provides an example of autocorrelated residuals using expectations augmented Phillips Curve.\nUS Macroeconomics Data Set, Quarterly, 1950I to 2000IV, 204 Quarterly Observations\nSource: Department of Commerce, BEA website and www.economagic.com\n# data preview\ndata &lt;- read.table(\"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/TableF5-2.txt\", header = TRUE)\ndata &lt;- data %&gt;% \n    mutate(delta_infl = infl-lag(infl))\ndata %&gt;% \n    head() %&gt;% \n    knitr::kable(digits = 5) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, latex_options=\"scale_down\") %&gt;% \n    scroll_box(width = \"100%\")\n\n\n\n\n\nYear\nqtr\nrealgdp\nrealcons\nrealinvs\nrealgovt\nrealdpi\ncpi_u\nM1\ntbilrate\nunemp\npop\ninfl\nrealint\ndelta_infl\n\n\n\n\n1950\n1\n1610.5\n1058.9\n198.1\n361.0\n1186.1\n70.6\n110.20\n1.12\n6.4\n149.461\n0.0000\n0.0000\n\n\n\n1950\n2\n1658.8\n1075.9\n220.4\n366.4\n1178.1\n71.4\n111.75\n1.17\n5.6\n150.260\n4.5071\n-3.3404\n4.5071\n\n\n1950\n3\n1723.0\n1131.0\n239.7\n359.6\n1196.5\n73.2\n112.95\n1.23\n4.6\n151.064\n9.9590\n-8.7290\n5.4519\n\n\n1950\n4\n1753.9\n1097.6\n271.8\n382.5\n1210.0\n74.9\n113.93\n1.35\n4.2\n151.871\n9.1834\n-7.8301\n-0.7756\n\n\n1951\n1\n1773.5\n1122.8\n242.9\n421.9\n1207.9\n77.3\n115.08\n1.40\n3.5\n152.393\n12.6160\n-11.2160\n3.4326\n\n\n1951\n2\n1803.7\n1091.4\n249.2\n480.1\n1225.8\n77.6\n116.19\n1.53\n3.1\n152.917\n1.5494\n-0.0161\n-11.0666",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#dataset-description",
    "href": "TS01_AR_example.html#dataset-description",
    "title": "5  AR – Example",
    "section": "",
    "text": "Feild Name\nDefinition\n\n\n\n\nyear\nYear\n\n\nqtr\nQuarter\n\n\nrealgdp\nReal GDP ($bil)\n\n\nrealcons\nReal consumption expenditures\n\n\nrealinvs\nReal investment by private sector\n\n\nrealgovt\nReal government expenditures\n\n\nrealdpi\nReal disposable personal income\n\n\ncpi_u\nConsumer price index\n\n\nM1\nNominal money stock\n\n\ntbilrate\nQuarterly average of month end 90 day t bill rate\n\n\nunemp\nUnemployment rate\n\n\npop\nPopulation, mil. interpolate of year end figures using constant growth rate per quarter\n\n\ninfl\nRate of inflation (First observation is missing)\n\n\nrealint\nEx post real interest rate = Tbilrate - Infl. (First observation is missing)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#empirical-model",
    "href": "TS01_AR_example.html#empirical-model",
    "title": "5  AR – Example",
    "section": "5.2 Empirical Model",
    "text": "5.2 Empirical Model\n\\[\n\\Delta I_t =  \\beta_1 + \\beta_2 u_t + \\varepsilon_t\n\\] where\n\n\\(I_t\\) is the inflation rate; \\(\\Delta I_t = I_t - I_{t-1}\\) is the first difference of the inflation rate;\n\\(u_t\\) is the unemployment rate;\n\\(\\varepsilon_t\\) is the error term.\n\nWe remove the first two quarters due to missing value in the first observation and the change in the rate of inflation.\nRegression result for OLS.\n\nlm_phillips &lt;- lm(delta_infl ~ unemp, data = data %&gt;% tail(-2))\nstargazer(lm_phillips, \n          type = \"html\", \n          title = \"Phillips Curve Regression\",\n          notes = \"&lt;span&gt;&#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&#42;&#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&#42;&#42;&#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.\",\n          notes.append = F)\n\n\nPhillips Curve Regression\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndelta_infl\n\n\n\n\n\n\n\n\nunemp\n\n\n-0.090\n\n\n\n\n\n\n(0.126)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.492\n\n\n\n\n\n\n(0.740)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n202\n\n\n\n\nR2\n\n\n0.003\n\n\n\n\nAdjusted R2\n\n\n-0.002\n\n\n\n\nResidual Std. Error\n\n\n2.822 (df = 200)\n\n\n\n\nF Statistic\n\n\n0.513 (df = 1; 200)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.\n\n\n\n\n\nvcov(lm_phillips)\n\n            (Intercept)       unemp\n(Intercept)  0.54830829 -0.08973175\nunemp       -0.08973175  0.01582211\n\n\nHAC (heteroskedasticity and autocorrelation consistent) standard errors\n\nlibrary(lmtest)\nlibrary(sandwich)\nvcovHAC(lm_phillips)\n\n            (Intercept)        unemp\n(Intercept)  0.23561076 -0.039847043\nunemp       -0.03984704  0.006986272\n\nvcovHC(lm_phillips)\n\n            (Intercept)       unemp\n(Intercept)   0.9319139 -0.16120691\nunemp        -0.1612069  0.02912351\n\n\nAutocorrelated residuals\nPlot the residuals.\n\nplot(lm_phillips$residuals, type=\"l\")\n\n\n\n\n\n\n\nFigure 5.1: Phillips Curve Deviations from Expected Inflation\n\n\n\n\n\nFigure 5.1 shows striking negative autocorrelation. The correlogram tells the same story (Figure 5.2). The blue dotted lines give the values beyond which the autocorrelations are (statistically) significantly different from zero.\n\nacf(lm_phillips$residuals, type='correlation')\n\n\n\n\n\n\n\nFigure 5.2: Correlogram of the residuals\n\n\n\n\n\nWe can get the autocorrelation coefficients by setting plot = FALSE\n\nacf(lm_phillips$residuals, type='correlation', plot = FALSE)$acf %&gt;%\n    as.vector() %&gt;% \n    head(10)\n\n [1]  1.000000000 -0.424730192 -0.112169741  0.073423178  0.147639239\n [6] -0.111740533 -0.036632121  0.009911709  0.036784781 -0.020769323\n\n\nNow we test the serial correlation of the residuals by regressing \\(\\varepsilon_t\\) on \\(\\varepsilon_{t-1}\\).\n\\[\n\\varepsilon_t = \\phi\\varepsilon_{t-1} + e_t\n\\]\n\nres &lt;- tibble(\n    res_t = lm_phillips$residuals,\n    res_t1 = lag(lm_phillips$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8694 -1.4800  0.0718  1.4990  8.3258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.02155    0.17854  -0.121    0.904    \nres_t1      -0.42630    0.06355  -6.708    2e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1844,    Adjusted R-squared:  0.1803 \nF-statistic: 44.99 on 1 and 199 DF,  p-value: 2.002e-10\n\n\nThe regression of the least squares residuals on their past values gives a slope of -0.4263 with a highly significant \\(t\\) ratio of -6.7078. We thus conclude that the residuals in this models are highly negatively autocorrelated.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#test-for-serial-correlation",
    "href": "TS01_AR_example.html#test-for-serial-correlation",
    "title": "5  AR – Example",
    "section": "5.3 Test for Serial Correlation",
    "text": "5.3 Test for Serial Correlation\n\nDurbin-Watson (DW) test for AR(1)\nBreusch-Godfrey test for AR(q)\n\n\nlibrary(lmtest)\ndwtest(lm_phillips, alternative = \"two.sided\") # Durbin Watson test \n\n\n    Durbin-Watson test\n\ndata:  lm_phillips\nDW = 2.8276, p-value = 5.212e-09\nalternative hypothesis: true autocorrelation is not 0\n\nbgtest(lm_phillips, order=1) # Breusch-Godfrey test \n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  lm_phillips\nLM test = 36.601, df = 1, p-value = 1.45e-09\n\n\nBoth tests show strong evidence of AR(1) serial correlation in the errors.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#consequence-for-serial-correlation",
    "href": "TS01_AR_example.html#consequence-for-serial-correlation",
    "title": "5  AR – Example",
    "section": "5.4 Consequence for Serial Correlation",
    "text": "5.4 Consequence for Serial Correlation\nThe presence of autocorrelation can lead to misleading results as they violate the assumptions of least squares.\nThe least squares estimator is still a linear unbiased estimator, but is no longer best.\nOne consequence of the serial correlated errors is that the standard error and \\(t\\) statistics are not valid anymore. In the case if serial correlation, you can either\n\nTransform the model to remove the serial correlation, or alternatively,\nFGLS (Feasible Genralized Least Squares), transform the original equation using, e.g., Cochrane-Orcutt or Prais-Winsten transformation.\nThis approach assumes strictly exogeneous regressors, that is, no lagged \\(y\\) in the RHS of the equation. See Chapter 12.3 in Wooldridge (2013), Introductory Econometrics: A Modern Approach.\nUse serial correlation-robust standard errors\nHAC (Heteroskedasticity and autocorrelation consistent) standard errors or Newey-West standard errors.\nInfinite Distributed Lag Models\nGeomoetric (or Koyck) and Rational Distributed Lag Models.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#references",
    "href": "TS01_AR_example.html#references",
    "title": "5  AR – Example",
    "section": "References",
    "text": "References\n\nEx. 12.3, Chap 12 Serial Correlation, Econometric Analysis, Greene 5th Edition, pp 251.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html",
    "href": "TS01_phillips-curve.html",
    "title": "6  Phillips Curve",
    "section": "",
    "text": "6.1 Static Phillips Curve\nA static Phillips curve is given by:\n\\[\ninf_t = \\beta_0 + \\beta_1\\, unem_t + u_t,\n\\]\nwhere \\(inf_t\\) is the annual inflation rate and \\(unem_t\\) is the unemployment rate.\nThis form of the Phillips curve assumes a constant natural rate of unemployment and constant inflationary expectations, and it can be used to study the contemporaneous tradeoff between inflation and unemployment.\n# data preview\ndata &lt;- read.table(\"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/TableF5-2.txt\", header = TRUE)\ndata &lt;- data %&gt;% \n    mutate(delta_infl = infl-lag(infl))\ndata %&gt;% \n    head() %&gt;% \n    knitr::kable(digits = 5) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, latex_options=\"scale_down\") %&gt;% \n    scroll_box(width = \"100%\")\n\n\n\n\n\nYear\nqtr\nrealgdp\nrealcons\nrealinvs\nrealgovt\nrealdpi\ncpi_u\nM1\ntbilrate\nunemp\npop\ninfl\nrealint\ndelta_infl\n\n\n\n\n1950\n1\n1610.5\n1058.9\n198.1\n361.0\n1186.1\n70.6\n110.20\n1.12\n6.4\n149.461\n0.0000\n0.0000\n\n\n\n1950\n2\n1658.8\n1075.9\n220.4\n366.4\n1178.1\n71.4\n111.75\n1.17\n5.6\n150.260\n4.5071\n-3.3404\n4.5071\n\n\n1950\n3\n1723.0\n1131.0\n239.7\n359.6\n1196.5\n73.2\n112.95\n1.23\n4.6\n151.064\n9.9590\n-8.7290\n5.4519\n\n\n1950\n4\n1753.9\n1097.6\n271.8\n382.5\n1210.0\n74.9\n113.93\n1.35\n4.2\n151.871\n9.1834\n-7.8301\n-0.7756\n\n\n1951\n1\n1773.5\n1122.8\n242.9\n421.9\n1207.9\n77.3\n115.08\n1.40\n3.5\n152.393\n12.6160\n-11.2160\n3.4326\n\n\n1951\n2\n1803.7\n1091.4\n249.2\n480.1\n1225.8\n77.6\n116.19\n1.53\n3.1\n152.917\n1.5494\n-0.0161\n-11.0666\nlm_phillips_stat &lt;- lm(infl ~ unemp, data = data %&gt;% tail(-2))\nsummary(lm_phillips_stat)\n\n\nCall:\nlm(formula = infl ~ unemp, data = data %&gt;% tail(-2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7020 -2.1922 -0.6098  1.4644 12.7362 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.2028     0.8873   2.483   0.0139 *\nunemp         0.3056     0.1507   2.028   0.0439 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.381 on 200 degrees of freedom\nMultiple R-squared:  0.02014,   Adjusted R-squared:  0.01524 \nF-statistic: 4.111 on 1 and 200 DF,  p-value: 0.04394\nThe simple regression estimates are\n\\[\n\\begin{aligned}\n\\widehat{inf}_t &= 2.22 + 0.30\\, unem_t \\\\\n&\\phantom{={ }} (0.89)\\;\\; (.15)\n\\end{aligned}\n\\tag{6.1}\\]\nThe regression indicates a positive relationship (\\(\\hat{\\beta}_1&gt;0\\)) between inflation and unemployment at 5% significance level.\nThere are some problems with this analysis that we cannot address in detail now. The Classical Linear Model assumptions do not hold. In addition, the static Phillips curve is probably not the best model for determining whether there is a short run tradeoff between inflation and unemployment. Macroeconomists generally prefer the expectations augmented Phillips curve, while we will see shortly.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "href": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "title": "6  Phillips Curve",
    "section": "6.2 Expectations Augmented Phillips Curve",
    "text": "6.2 Expectations Augmented Phillips Curve\nExample 12.3 in Greene (2003), 5ed, Econometric Analysis.\nA linear version of the expectations augmented Phillips curve can be written as\n\\[\ninf_t - inf^e_t = \\beta_1 (unem_t - \\mu_0) + e_t,\n\\]\nwhere \\(\\mu_0\\) is the natural rate of unemployment, which we assume constant over time, and \\(inf^e_t\\) is the expected rate of inflation formed in year \\(t-1.\\)\nThe difference between actual unemployment and the natural rate is called cyclical unemployment, while the difference between actual and expected inflation is called unanticipated inflation.\nIf there is a tradeoff between unanticipated inflation and cyclical unemployment, then \\(\\beta_1&lt;0.\\)\nThe error term, \\(e_t\\), is called a supply shock by macroeconomists.\nTo complete this model, we need to make an assumption about inflationary expectations. Under adaptive expectations, the expected value of current inflation depends on recently observed inflation. A particularly simple formulation is that expected inflation this year is last year’s inflation:\n\\[\ninf^e_t = inf_{t-1}\n\\]\nUnder this assumption, we can write the following empirical model:\n\\[\ninf_t - inf_{t-1} = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] or\n\\[\n\\Delta  inf_t = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] where \\(\\Delta  inf_t = inf_t - inf_{t-1}\\) and \\(\\beta_0=-\\beta_1\\mu_0.\\)\nHence, the natural unemployment rate can be obtained by:\n\\[\n\\mu_0 = -\\frac{\\beta_0}{\\beta_1}.\n\\]\n\nlm_phillips_aug &lt;- lm(delta_infl ~ unemp, data = data %&gt;% tail(-2))\nsummary(lm_phillips_aug)\n\n\nCall:\nlm(formula = delta_infl ~ unemp, data = data %&gt;% tail(-2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2791  -1.6635  -0.0117   1.7813   8.5472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.49189    0.74048   0.664    0.507\nunemp       -0.09013    0.12579  -0.717    0.474\n\nResidual standard error: 2.822 on 200 degrees of freedom\nMultiple R-squared:  0.002561,  Adjusted R-squared:  -0.002427 \nF-statistic: 0.5134 on 1 and 200 DF,  p-value: 0.4745\n\n\nThe OLS estimates are\n\\[\n\\begin{aligned}\n\\Delta\\widehat{inf}_t &= 0.49 - 0.09 \\,unem_t \\\\\n&\\phantom{={}} (0.74) \\;\\; (0.13)\n\\end{aligned}\n\\tag{6.2}\\]\n\\(\\hat{\\beta}_1&lt;0\\) indicates a tradeoff between cyclical unemployment and unanticipated inflation. But the effect is statistically insignificant.\nUnder expectations augmented Phillips Curve, one-point increase in \\(unem\\) lowers unanticipated inflation by about 0.1 of a point. We can contrast this with the static Phillips curve in Equation 6.1, where we found a positive relationship between inflation and unemployment.\nThe natural employment rate is 5.78 percent.\n\\[\n\\mu_0 = -\\frac{\\beta_0}{\\beta_1} = \\frac{0.49}{0.09} \\approx 5.44\n\\]\nVariance can be obtained by the Delta Method. In R, you can use car::deltaMethod to get the confidence interval for your parameter of interest.\n\nlibrary(car)\ndeltaMethod(lm_phillips_aug, \"-b0/b1\", \n            parameterNames = paste(\"b\", 0:1, sep=\"\"))\n\n       Estimate     SE  2.5 % 97.5 %\n-b0/b1   5.4575 2.2228 1.1009 9.8141\n\n\nSerial correlation in errors\n\nres &lt;- tibble(\n    res_t = lm_phillips_aug$residuals,\n    res_t1 = lag(lm_phillips_aug$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8694 -1.4800  0.0718  1.4990  8.3258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.02155    0.17854  -0.121    0.904    \nres_t1      -0.42630    0.06355  -6.708    2e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1844,    Adjusted R-squared:  0.1803 \nF-statistic: 44.99 on 1 and 199 DF,  p-value: 2.002e-10\n\n\nThere is strong evidence of serial correlation in the residuals.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#remedies-for-ar-errors",
    "href": "TS01_phillips-curve.html#remedies-for-ar-errors",
    "title": "6  Phillips Curve",
    "section": "6.3 Remedies for AR errors",
    "text": "6.3 Remedies for AR errors\nExample 12.5 in Wooldridge (2013), 5ed, Introductory Econometrics: A Modern Approach.\nExample 19.2 in Greene (2003), 5ed, Econometric Analysis.\n\n6.3.1 Prais-Winsten Estimation (Transformation)\nStatic model\nStatic model indicates positive autocorrelation.\n\nres &lt;- tibble(\n    res_t = lm_phillips_stat$residuals,\n    res_t1 = lag(lm_phillips_stat$residuals))\nlm_res &lt;- lm(res_t ~ res_t1, data = res)\nsummary(lm_res)\n\n\nCall:\nlm(formula = res_t ~ res_t1, data = res)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5887 -1.6354 -0.1089  1.2856  7.9793 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.04062    0.18009  -0.226    0.822    \nres_t1       0.64520    0.05349  12.062   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.553 on 199 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4223,    Adjusted R-squared:  0.4194 \nF-statistic: 145.5 on 1 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n\nlibrary(prais) # install.packages(\"prais\")\ndata &lt;- data %&gt;% \n    mutate(yrQ = as.yearqtr(paste(Year, qtr, sep=\"-\")))\npw_est_stat &lt;- prais_winsten(lm_phillips_stat, data = data %&gt;% tail(-2), index = \"yrQ\" )\n\nIteration 0: rho = 0\nIteration 1: rho = 0.6452\nIteration 2: rho = 0.655\nIteration 3: rho = 0.6558\nIteration 4: rho = 0.6559\nIteration 5: rho = 0.6559\nIteration 6: rho = 0.6559\n\nsummary(pw_est_stat)\n\n\nCall:\nprais_winsten(formula = lm_phillips_stat, data = data %&gt;% tail(-2), \n    index = \"yrQ\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4276 -2.2063 -0.8301  1.6560 12.8870 \n\nAR(1) coefficient rho after 6 iterations: 0.6559\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.82017    1.69871   2.249   0.0256 *\nunemp        0.02493    0.28647   0.087   0.9307  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.564 on 200 degrees of freedom\nMultiple R-squared:  0.01303,   Adjusted R-squared:  0.008099 \nF-statistic: 2.641 on 1 and 200 DF,  p-value: 0.1057\n\nDurbin-Watson statistic (original): 0.6931 \nDurbin-Watson statistic (transformed): 2.378\n\n\n\nExpectations augmented model\nExpectations augmented model indicates negative autocorrelation.\n\npw_est_aug &lt;- prais_winsten(lm_phillips_aug, data = data %&gt;% tail(-2), index = \"yrQ\" )\n\nIteration 0: rho = 0\nIteration 1: rho = -0.4263\nIteration 2: rho = -0.4263\n\nsummary(pw_est_aug)\n\n\nCall:\nprais_winsten(formula = lm_phillips_aug, data = data %&gt;% tail(-2), \n    index = \"yrQ\")\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2668  -1.6601  -0.0065   1.7870   8.5401 \n\nAR(1) coefficient rho after 2 iterations: -0.4263\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.47007    0.47247   0.995    0.321\nunemp       -0.08705    0.08024  -1.085    0.279\n\nResidual standard error: 2.548 on 200 degrees of freedom\nMultiple R-squared:  0.005932,  Adjusted R-squared:  0.000962 \nF-statistic: 1.194 on 1 and 200 DF,  p-value: 0.2759\n\nDurbin-Watson statistic (original): 2.828 \nDurbin-Watson statistic (transformed): 2.285",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#references",
    "href": "TS01_phillips-curve.html#references",
    "title": "6  Phillips Curve",
    "section": "References",
    "text": "References\n\nTomas Formanek, Materials for the Advanced Econometric 1 – courses 4EK608 and 4EK416 https://github.com/formanektomas/4EK608_4EK416/tree/master?tab=readme-ov-file",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html",
    "href": "TS01_examples.html",
    "title": "7  TS Examples",
    "section": "",
    "text": "7.1 Example 11.6: Fertility and Personal Exemption\nTextbook: Chapter 11, Introductory Econometrics: A Modern Approach, 7e by Jeffrey M. Wooldridge\nSummary notes by Marius v. Oordt: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3401712\ngfr: general fertility rate\npe: personal exemption\ndata('fertil3')\nfertility_diff &lt;- lm(diff(gfr) ~ diff(pe), data = fertil3)\nfertility_lag &lt;- lm(diff(gfr) ~ diff(pe) + diff(pe_1) + diff(pe_2), data = fertil3)\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndiff(gfr)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndiff(pe)\n\n\n-0.04268 (0.02837)\n\n\n-0.03620 (0.02677)\n\n\n\n\ndiff(pe_1)\n\n\n\n\n-0.01397 (0.02755)\n\n\n\n\ndiff(pe_2)\n\n\n\n\n0.10999*** (0.02688)\n\n\n\n\nConstant\n\n\n-0.78478 (0.50204)\n\n\n-0.96368** (0.46776)\n\n\n\n\n\n\n\n\nObservations\n\n\n71\n\n\n69\n\n\n\n\nR2\n\n\n0.03176\n\n\n0.23248\n\n\n\n\nAdjusted R2\n\n\n0.01773\n\n\n0.19705\n\n\n\n\nResidual Std. Error\n\n\n4.22082 (df = 69)\n\n\n3.85945 (df = 65)\n\n\n\n\nF Statistic\n\n\n2.26343 (df = 1; 69)\n\n\n6.56266*** (df = 3; 65)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.\nThe first regression uses first differences:\n\\[\n\\begin{aligned}\n\\Delta\\widehat{gfr} &= -.785 - .043\\, \\Delta pe \\\\\n&\\phantom{=}\\;\\; (.502)\\;\\; (.028) \\\\\nn &= 71, R^2=.032, \\bar{R^2} = .018.\n\\end{aligned}\n\\tag{7.1}\\]\nThe estimates indicate an increase in \\(pe\\) lowers \\(gfr\\) contemporaneously, although the estimate is not statistically different from zero at the 5% level.\nIf we add two lags of \\(\\Delta pe,\\) things improve:\n\\[\n\\begin{aligned}\n\\Delta\\widehat{gfr} &= -.964 - .036\\, \\Delta pe - .014\\, \\Delta pe_{-1} + .110\\, \\Delta pe_{-2} \\\\\n&\\phantom{=}\\;\\; (.468)\\quad (.027) \\qquad\\; (.028) \\qquad\\quad\\; (.027)\\\\\nn &= 69, R^2=.232, \\bar{R^2} = .197.\n\\end{aligned}\n\\tag{7.2}\\]\nEven though \\(\\Delta pe\\) and \\(\\Delta pe_{-1}\\) have negative coefficients, their coefficients are small and jointly insignificant (\\(p\\text{-value}=.28,\\) see Anova test below).\n# Compare the restricted with the full model\nfertility_lag2 &lt;- lm(diff(gfr) ~ diff(pe_2), data = fertil3)\nanova(fertility_lag2, fertility_lag)\n\nAnalysis of Variance Table\n\nModel 1: diff(gfr) ~ diff(pe_2)\nModel 2: diff(gfr) ~ diff(pe) + diff(pe_1) + diff(pe_2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     67 1006.6                           \n2     65  968.2  2    38.413 1.2894 0.2824\nThe second lag (\\(\\Delta pe_{-2}\\)) is very significant and indicates a positive relationship between changes in \\(pe\\) and subsequent changes in \\(gfr\\) two years hence. This makes more sense than having a contemporaneous effect.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html#example-11.6-fertility-and-personal-exemption",
    "href": "TS01_examples.html#example-11.6-fertility-and-personal-exemption",
    "title": "7  TS Examples",
    "section": "",
    "text": "7.1.1 Example 11.8\nIn this example, we want to test whether the Finite Distributed Lag model (7.2) for \\(\\Delta\\widehat{gfr}\\) and \\(\\Delta pe\\) is dynamically complete.\nBeing dynamically complete indicates that neither lags of \\(\\Delta\\widehat{gfr}\\) nor further lags of \\(\\Delta pe\\) should appear in the equation. Mathematically, given the following finite distributed lag model:\n\\[\n\\Delta gfr_t = \\beta_0 + \\beta_1\\Delta pe_t + \\beta_2\\Delta pe_{t-1} + \\beta_3 \\Delta pe_{t-2} + u_t .\n\\] Rewrite it as\n\\[\n\\begin{aligned}\n\\Delta gfr_t &= \\beta_0 + \\beta_1x_{t1} + \\beta_2x_{t2} + \\beta_3 x_{t3} + u_t \\\\\ny_t &= \\bx_t'\\bbeta + u_t\n\\end{aligned}\n\\] where the explanatory variables \\(\\bx_t=(x_{t1}, x_{t2}, x_{t3})' = (\\Delta pe_t, \\Delta pe_{t-2}, \\Delta pe_{t-3})'\\) and the dependent variable \\(y_t=\\Delta gfr_t.\\)\nA dynamically complete model requires the following condition:\n\\[\n\\E(u_t\\mid \\bx_t, y_{t-1}, \\bx_{t-1}, \\ldots) = 0.\n\\tag{7.3}\\] Written in terms of \\(y_t,\\)\n\\[\n\\E(y_t\\mid \\bx_t, y_{t-1}, \\bx_{t-1}, \\ldots) = \\E(y_t\\mid \\bx_t).\n\\tag{7.4}\\]\nWe can test for dynamic completeness by adding \\(\\Delta gfr_{t-1}.\\)\n\nfertility_lag_dep &lt;- lm(diff(gfr) ~ lag(diff(gfr)) + diff(pe) + diff(pe_1) + diff(pe_2), data = fertil3)\ntidy(fertility_lag_dep) %&gt;% \n    knitr::kable(digits = 3) \n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.702\n0.454\n-1.547\n0.127\n\n\nlag(diff(gfr))\n0.300\n0.106\n2.835\n0.006\n\n\ndiff(pe)\n-0.045\n0.026\n-1.773\n0.081\n\n\ndiff(pe_1)\n0.002\n0.027\n0.077\n0.939\n\n\ndiff(pe_2)\n0.105\n0.026\n4.108\n0.000\n\n\n\n\n\nThe coefficient estimate is .300 and its \\(t\\) statistic is 2.84. Thus, the model is NOT dynamically complete in the sense of (7.4).\nThe fact that (7.2) is not dynamically complete suggests that there may be serial correlation in the errors. We will need to test and correct for this.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS01_examples.html#example-11.7-wages-and-productivity",
    "href": "TS01_examples.html#example-11.7-wages-and-productivity",
    "title": "7  TS Examples",
    "section": "7.2 Example 11.7: Wages and Productivity",
    "text": "7.2 Example 11.7: Wages and Productivity\n\\[\\log(hrwage_t) = \\beta_0 + \\beta_1\\log(outphr_t) + \\beta_2t + u_t\\] Data from the Economic Report of the President, 1989, Table B-47. The data are for the non-farm business sector.\n\ndata(\"earns\")\nwage_time &lt;- lm(lhrwage ~ loutphr + t, data = earns)\nwage_diff &lt;- lm(diff(lhrwage) ~ diff(loutphr), data = earns)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlhrwage\n\n\ndiff(lhrwage)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nloutphr\n\n\n1.63964*** (0.09335)\n\n\n\n\n\n\nt\n\n\n-0.01823*** (0.00175)\n\n\n\n\n\n\ndiff(loutphr)\n\n\n\n\n0.80932*** (0.17345)\n\n\n\n\nConstant\n\n\n-5.32845*** (0.37445)\n\n\n-0.00366 (0.00422)\n\n\n\n\n\n\n\n\nObservations\n\n\n41\n\n\n40\n\n\n\n\nR2\n\n\n0.97122\n\n\n0.36424\n\n\n\n\nAdjusted R2\n\n\n0.96971\n\n\n0.34750\n\n\n\n\nResidual Std. Error (df = 38)\n\n\n0.02854\n\n\n0.01695\n\n\n\n\nF Statistic\n\n\n641.22430*** (df = 2; 38)\n\n\n21.77054*** (df = 1; 38)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>TS Examples</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html",
    "href": "TS02_lag_poly.html",
    "title": "8  Lag polynomials",
    "section": "",
    "text": "8.1 Product of Filters\nA \\(p\\)-th degree lag polynomial is given by:\n\\[\n\\alpha(L) = \\alpha_0 + \\alpha_1L + \\cdots + \\alpha_pL^p,\n\\] where \\(L\\) is the lag operator, defined by the relation \\(L^jx_t=x_{t-j}.\\)\nWe define a filter given by \\(\\alpha(L)\\) to an input process \\(\\{x_t\\}\\), we get a weighted average of the current and \\(p\\) most recent values of the process:\n\\[\n\\begin{aligned}\n\\alpha(L)x_t &= \\alpha_0x_t + \\alpha_1Lx_t + \\alpha_2L^2x_t + \\cdots + \\alpha_pL^px_t \\\\\n&= \\alpha_0x_t + \\alpha_1x_{t-1}+ \\alpha_2x_{t-2} + \\cdots + \\alpha_px_{t-p} \\\\\n&= \\sum_{j=0}^p \\alpha_jx_{t-j}\n\\end{aligned}\n\\]\nLet \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}\\) be two arbitrary sequences of real numbers and define the sequence \\(\\{\\delta_j\\}\\) by the relation\n\\[\n\\begin{gathered}\n\\delta_0 = \\alpha_0\\beta_0, \\\\\n\\delta_1 = \\alpha_0\\beta_1 + \\alpha_1\\beta_0, \\\\\n\\delta_2 = \\alpha_0\\beta_2 + \\alpha_1\\beta_1 + \\alpha_2\\beta_0, \\\\\n\\vdots \\\\\n\\delta_j = \\alpha_0\\beta_j + \\alpha_1\\beta_{j-1} + \\alpha_2\\beta_{j-2} + \\cdots + \\alpha_{j-1}\\beta_{1} + \\alpha_{j}\\beta_{0}, \\\\\n\\vdots \\\\\n\\end{gathered}\n\\tag{8.1}\\]\nThe sequence \\(\\{\\delta_j\\}\\) created from this convoluted formula is called the convolution of \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}.\\)\nFor example, for \\(\\alpha(L)=1+\\alpha_1L\\) and \\(\\beta(L)=1+\\beta_1L\\), we have\n\\[\n\\delta(L)=(1+\\alpha_1L)(1+\\beta_1L) = 1+ (\\alpha_1+\\beta_1)L + \\alpha_1\\beta_1L^2.\n\\]\nFilters are commutative:\n\\[\n\\alpha(L)\\beta(L) = \\beta(L)\\alpha(L)\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#inverses",
    "href": "TS02_lag_poly.html#inverses",
    "title": "8  Lag polynomials",
    "section": "8.2 Inverses",
    "text": "8.2 Inverses\nThe inverse of \\(\\alpha(L)\\) is denoted as \\(\\alpha(L)^{-1}\\) or \\(1/\\alpha(L)\\):\n\\[\n\\alpha(L)\\alpha(L)^{-1}=1\n\\] Define a \\(p\\)-th degree lag polynomial \\(\\phi(L)\\)\n\\[\n\\phi(L) = 1-\\phi_1L-\\phi_2L^2-\\cdots-\\phi_pL^p.\n\\tag{8.2}\\]\nEquation 8.2 is often used to construct AR processes.\nNow let’s calculate its inverse, \\(\\psi(L) = \\phi(L)^{-1}.\\)\n\\[\n\\psi(L) = \\psi_0 + \\psi_1L + \\psi_2L^2 + \\cdots\n\\]\nBy the convolution formula (8.1), we have\n\\[\n\\begin{aligned}\n\\text{constant}:&\\quad  \\psi_0 =1 \\\\\nL: &\\quad  \\psi_1-\\psi_0\\phi_1 = 0 \\Longrightarrow \\psi_1 = \\phi_1 \\\\\nL^2: &\\quad  \\psi_2-\\psi_1\\phi_1-\\psi_0\\phi_2 = 0 \\Longrightarrow \\psi_2 = \\phi_1^2 + \\phi_2 \\\\\n\\vdots \\\\\nL^p: &\\quad  \\psi_p - \\psi_{p-1}\\phi_1 - \\psi_{p-2}\\phi_2 - \\cdots - \\psi_{1}\\phi_{p-1} - \\psi_{0}\\phi_p = 0 \\\\\nL^{p+1}: &\\quad  \\psi_{p+1} - \\psi_{p}\\phi_1 - \\psi_{p-1}\\phi_2 - \\cdots - \\psi_{2}\\phi_{p-1} - \\psi_{1}\\phi_p = 0 \\\\\n\\vdots\n\\end{aligned}\n\\]\n\nExample 8.1 Consider a 1st degree lag polynomial \\(\\phi(L)=1-\\phi L\\), its inverse \\(\\psi(L)\\) can be calculated as\n\\[\n\\begin{aligned}\n\\text{constant}: &\\quad \\psi_0 =1  \\\\\nL: &\\quad \\psi_1 - \\psi_0\\phi = 0 \\Longrightarrow \\psi_1 = \\phi \\\\\nL^2: &\\quad \\psi_2 - \\psi_1\\phi = 0 \\Longrightarrow \\psi_2 = \\phi^2 \\\\\nL^3: &\\quad \\psi_3 - \\psi_2\\phi = 0 \\Longrightarrow \\psi_3 = \\phi^3 \\\\\n\\vdots\n\\end{aligned}\n\\] Hence\n\\[\n\\begin{aligned}\n\\psi(L) &= (1-\\phi L)^{-1} \\\\\n&=1 + \\phi L + \\phi^2 L^2 + \\phi^3 L^3 + \\cdots \\\\\n&= \\sum_{j=0}^\\infty \\phi^jL^j.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#stability-condition",
    "href": "TS02_lag_poly.html#stability-condition",
    "title": "8  Lag polynomials",
    "section": "8.3 Stability Condition",
    "text": "8.3 Stability Condition\nThe solution sequence \\(\\{\\psi_j\\}\\) eventually starts declining at a geometric rate if the stability condition holds. The condition states:\nAll the roots of the \\(p\\)-th degree polynomial equation in \\(z\\)\n\\[\n\\phi(z) = 0 \\text{ where } \\phi(z) \\equiv 1-\\phi_1z-\\phi_2z^2-\\cdots-\\phi_pz^p\n\\] are greater than 1 in absolute value (lie outside the unit circle).\nEquivalently, we can consider the roots of the reciprocal polynomial defined as (basically this means inverting the order of the coefficients)\n\\[\n\\phi^*(z) = z^p\\phi(z^{-1}) = z^p - \\phi_1z^{p-1} - \\dots - \\phi_p.\n\\] The stability condition can be stated as:\nAll the roots of\n\\[\n\\phi^*(z) \\equiv z^p - \\phi_1z^{p-1} - \\dots - \\phi_p =0\n\\] are less than 1 in the absolute value (i.e., lie inside the unit circle).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html",
    "href": "TS02_AR-MA-representation.html",
    "title": "9  AR(1) and Its MA representation",
    "section": "",
    "text": "9.1 Case 1: \\(\\abs{\\phi}&lt;1\\)\nA first-order autoregressive process (AR(1)) satisfies the following stochastic difference equation:\n\\[\n\\begin{aligned}\ny_t &= c + \\phi y_{t-1} + \\varepsilon_t,  \\quad \\text{or} \\\\\ny_t - \\phi y_{t-1} &= c + \\varepsilon_t,  \\quad \\text{or} \\\\\n(1-\\phi L) y_t &= c + \\varepsilon_t,\n\\end{aligned}\n\\tag{9.1}\\] where \\(\\{\\varepsilon_t\\}\\) is white noise.\nIf \\(\\phi\\ne 1,\\) let \\(\\mu\\equiv c/(1-\\phi)\\) and rewrite the equation as\n\\[\n\\begin{aligned}\n(y_t-\\mu) - \\phi(y_{t-1}-\\mu) &= \\varepsilon_t \\quad \\text{or} \\\\\n(1-\\phi L) (y_t-\\mu) &= \\varepsilon_t.\n\\end{aligned}\n\\tag{9.2}\\]\n\\(\\mu\\) is the mean of \\(y_t\\) if \\(y_t\\) is covariance-stationary. For this reason, we call (9.2) a deviation-from-the-mean form. Note that the moving average is on the successive values of \\(\\{y_t\\},\\) not on \\(\\{\\varepsilon_t\\}.\\) The difference equation is called stochastic because of the presence of the random variable \\(\\varepsilon_t.\\)\nWe seek a covariance-stationary solution \\(\\{y_t\\}\\) to this stochastic difference equation. The solution depends on whether \\(\\abs{\\phi}\\) is less than, equal to, or greater than 1.\nSumming up:\nCase 1: \\(\\abs{\\phi}&lt;1\\)\nThe AR(1) process is stationary and causal, i.e., allows us to write an AR(1) as an MA(\\(\\infty\\)) using past values of \\(\\varepsilon_t.\\)\nCase 2: \\(\\abs{\\phi}&gt;1\\)\nThe AR(1) process is stationary but not causal. \\(y_t\\) is correlated with future values of \\(\\varepsilon_t.\\) This is a feasible representation but it is unnatural.\nCase 3: \\(\\abs{\\phi}=1\\)\nWe have a non-stationary process (called random walk when \\(\\phi = 1\\)) and we say that this process has a unit root.\nThe solution can be obtained easily by the use of the inverse \\((1 - \\phi L)^{-1}\\). Since this filter is absolutely summable when \\(|\\phi| &lt; 1\\), we can apply it to both sides of the AR(1) (9.2) to obtain\n\\[\n(1 - \\phi L)^{-1}(1 - \\phi L)(y_t - \\mu) = (1 - \\phi L)^{-1} \\varepsilon_t.\n\\]\nSo\n\\[\ny_t - \\mu = (1 - \\phi L)^{-1} \\varepsilon_t = (1 + \\phi L + \\phi^2 L^2 + \\cdots)\\varepsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\]\n\\[\n\\text{or} \\quad y_t = \\mu + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{9.3}\\]\nWhat we have shown is that, if \\(\\{y_t\\}\\) is a covariance-stationary solution to the stochastic difference equation (18.3) or (9.2), then \\(y_t\\) has the moving-average representation as in (9.3). Conversely, if \\(y_t\\) has the representation (9.3), then it satisfies the difference equation.\nThe condition \\(\\abs{\\phi}&lt;1,\\) which is the stability condition associated with the first-degree polynomial equation \\(1-\\phi z=0,\\) is called the stationary condition in the context of autoregressive processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-2-absphi1",
    "href": "TS02_AR-MA-representation.html#case-2-absphi1",
    "title": "9  AR(1) and Its MA representation",
    "section": "9.2 Case 2: \\(\\abs{\\phi}>1\\)",
    "text": "9.2 Case 2: \\(\\abs{\\phi}&gt;1\\)\nBy shifting time forward by one period (i.e., by replacing \\(t\\) by \\(t+1\\)), multiplying both sides by \\(\\phi^{-1}\\), and rearranging, the stochastic difference equation (9.2) can be written as\n\\[\ny_t - \\mu = \\phi^{-1}(y_{t+1} - \\mu) - \\phi^{-1} \\varepsilon_{t+1}.\n\\] Keep this substitution,\n\\[\ny_{t+1} - \\mu = \\phi^{-1}(y_{t+2} - \\mu) - \\phi^{-1} \\varepsilon_{t+2}\n\\] then\n\\[\ny_t - \\mu = \\phi^{-2}(y_{t+2} - \\mu) - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.\n\\]\nSubstituting \\((y_{t+2} - \\mu)\\) for the corresponding next period equation:\n\\[\n\\begin{aligned}\ny_{t+2} - \\mu &= \\phi^{-1}(y_{t+3} - \\mu) - \\phi^{-1} \\varepsilon_{t+3}, \\\\\ny_t - \\mu &= \\phi^{-3}(y_{t+3} - \\mu) - \\phi^{-3}\\varepsilon_{t+3} - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.  \\\\\n\\end{aligned}\n\\]\nThen likewise for \\((y_{t+3} - \\mu)\\) and so on. Iterating \\(k\\) times, we get the following representation:\n\\[\n\\begin{aligned}\ny_t - \\mu &= \\phi^{-k}(y_{t+k} - \\mu) - \\phi^{-k}\\varepsilon_{t+k}  - \\phi^{-k+1}\\varepsilon_{t+k-1} - \\cdots - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1} \\\\\n&= \\phi^{-k}(y_{t+k} - \\mu) - \\sum_{j=1}^k \\phi^{-j}\\varepsilon_{t+j}.\n\\end{aligned}\n\\] As \\(k\\to\\infty\\), \\(\\phi^{-k}\\to 0\\), we have\n\\[\ny_t = \\mu - \\sum_{j=1}^{\\infty} \\phi^{-j} \\varepsilon_{t+j}.\n\\]\nThat is, the current value of \\(y\\) is a moving average of future values of \\(\\varepsilon\\). The infinite sum is well defined because the sequence \\(\\{\\phi^{-j}\\}\\) is absolutely summable if \\(|\\phi| &gt; 1\\). This is a feasible representation but unnatural. Moreover, it is unstable as the initial condition should now be set into the future as for example \\(y_T=0\\) as \\(T\\to\\infty\\) which is not likely to be the case.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-3-absphi1",
    "href": "TS02_AR-MA-representation.html#case-3-absphi1",
    "title": "9  AR(1) and Its MA representation",
    "section": "9.3 Case 3: \\(\\abs{\\phi}=1\\)",
    "text": "9.3 Case 3: \\(\\abs{\\phi}=1\\)\nThe stochastic difference equation has no covariance-stationary solution. For example, if \\(\\phi = 1\\), the stochastic difference equation becomes\n\\[\n\\begin{aligned}\ny_t &= c + y_{t-1} + \\varepsilon_t \\\\\n    &= c + (c + y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\quad \\text{(since } y_{t-1} = c + y_{t-2} + \\varepsilon_{t-1}) \\\\\n    &= c + (c + (c + y_{t-3} + \\varepsilon_{t-2}) + \\varepsilon_{t-1}) + \\varepsilon_t, \\quad \\text{etc.}\n\\end{aligned}\n\\]\nRepeating this type of successive substitution \\(j\\) times, we obtain\n\\[\ny_t - y_{t-j} = c \\cdot j + (\\varepsilon_t + \\varepsilon_{t-1}+ \\varepsilon_{t-2} + \\cdots + + \\varepsilon_{t-j+1})\n\\] If \\(\\{\\varepsilon_t\\}\\) is independent white noise, \\(\\{y_t - y_{t-j}\\}\\) is a random walk with drift \\(c.\\)\nUnit root processes are non-stationary not because of the presence of a linear deterministic trend but because they are driven by a stochastic trend which makes their variance time dependent and are called Difference Stationary processes as opposed to Trend Stationary processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#references",
    "href": "TS02_AR-MA-representation.html#references",
    "title": "9  AR(1) and Its MA representation",
    "section": "References",
    "text": "References\n\n§6.2, F. Hayashi (2021), Econometrics, Princeton University Press, IBSN: 9780691010182.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html",
    "href": "TS03_finite-property.html",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "",
    "text": "10.1 Finite Sample Classical Assumptions of OLS\nIn this section, we give a complete listing of the finite sample, or small sample, properties of OLS under standard assumptions.\nIn the notation \\(x_{tj}\\), \\(t\\) denotes the time period, and \\(j\\) is a label to indicate one of the \\(K\\) explanatory variables.\nFor example, a Finite Distributed Lag (FDL) model of order two\n\\[\ny_t = \\alpha_0 + \\delta_0 z_t + \\delta_1 z_{t-1} + \\delta_2 z_{t-2} + u_t,\n\\] is obtained by setting \\(x_{t1}=z_t,\\) \\(x_{t2}=z_{t-1},\\) and \\(x_{t3}=z_{t-2}.\\)\nFor the remaining assumptions, let \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) denote the set of all independent variables at time \\(t.\\) Further, \\(\\bX\\) denote the collection of all independent variables for all time periods. It is useful to think of \\(\\bX\\) as being an array, with \\(n\\) rows and \\(k\\) columns.\n\\[\n\\bX = \\begin{bmatrix}\n\\bx_1 \\\\\n\\bx_2 \\\\\n\\vdots \\\\\n\\bx_T\n\\end{bmatrix}\n= \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1K} \\\\\nx_{21} & x_{22} & \\cdots & x_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{T1} & x_{T2} & \\cdots & x_{TK} \\\\\n\\end{bmatrix}\n\\]\nAssumption 10.2 allows the explanatory variables to be correlated, but it rules out perfect correlation in the sample.\nAssumption 10.3 (strict mean independence) implies that the error at time \\(t\\), \\(u_t,\\) is uncorrelated with each explanatory in every time period. \\[\n\\E(\\bx_s u_t) = \\bold{0}, \\quad \\text{for } s=1,\\ldots,T.\n\\tag{10.2}\\] This is called the strict exogeneity.\nNote that (10.1) implies (10.2), but not conversely. While strict exogeneity is sufficient for identification and asymptotic theory, we will use the stronger condition (10.1) for finite sample analysis.\nStrict exogeneity is typically inappropriate in dynamic models. A relaxed version is contemporaneously exogenous, which only requires \\(u_t\\) to be uncorrelated with the explanatory variables dated at time \\(t\\): in conditional mean terms,\n\\[\n\\E(u_t\\mid x_{t1}, x_{t2}, \\ldots, x_{tK} ) = \\E(u_t\\mid \\bx_t) = 0.\n\\] Contemporaneously exogeneity is much weaker than strict exogeneity because it puts no restrictions on how \\(u_t\\) is related to the explanatory variables in other time periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#finite-sample-classical-assumptions-of-ols",
    "href": "TS03_finite-property.html#finite-sample-classical-assumptions-of-ols",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "",
    "text": "Assumption 10.1 (Linear in Parameters) The stochastic process \\(\\{(x_{t1}, x_{t2}, \\ldots, x_{tK}, y_t): t=1,2,\\ldots, n\\}\\) follows the linear model\n\\[\n\\begin{split}\ny_t &= \\bx'\\bbeta + u_t \\\\\n&= \\beta_0 + \\beta_1x_{t1} + \\beta_2x_{t2} + \\cdots + + \\beta_Kx_{tK} + u_t,\n\\end{split}\n\\]\nwhere \\(\\{u_t: t=1,2,\\ldots, n\\}\\) is the sequence of errors or disturbances. Here \\(n\\) is the number of observations (time periods).\n\n\n\n\n\n\n\nAssumption 10.2 (No Perfect Collinearity) In the sample (and therefore in the underlying time series process), no independent variable is constant nor a perfect linear combination of the others.\n\n\n\nAssumption 10.3 (Zero Conditional Mean) For each \\(t\\), the expected value of the error \\(u_t,\\) given the explanatory variables for all time periods, is zero. Mathematically,\n\\[\n\\E(u_t\\mid \\bx_1, \\ldots, \\bx_T) = \\E(u_t\\mid \\bX) = 0,\\, t=1,2,\\ldots, T.\n\\tag{10.1}\\]\n\n\n\n\n\n\nAssumption 10.4 (Homoskedasticity) Conditional on \\(\\bX,\\) the variance of \\(u_t\\) is the same for all \\(t:\\) \\(\\var(u_t\\mid \\bX)=\\var(u_t)=\\sigma^2,\\) \\(t=1,2,\\ldots,T.\\)\n\n\nAssumption 10.5 (No Serial Correlation) Conditional on \\(\\bX,\\) the errors in two different time periods are uncorrelated: \\(\\cor(u_t, u_s\\mid \\bX)=0,\\) for all \\(t\\ne s.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#finite-sample-properties-of-ols",
    "href": "TS03_finite-property.html#finite-sample-properties-of-ols",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.2 Finite Sample Properties of OLS",
    "text": "10.2 Finite Sample Properties of OLS\nUnder Assumptions 10.1 through 10.3, the OLS estimators are unbiased:\n\\[\n\\E(\\hat{\\beta}_j\\mid \\bX) = \\E(\\hat{\\beta}_j) = \\beta_j, \\, j=0,1,\\ldots,K.\n\\]\n\nUnder Assumptions 10.1 through 10.5, the OLS estimators are the best linear unbiased estimators conditional on \\(\\bX.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "href": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.3 Violation of Strict Exogeneity",
    "text": "10.3 Violation of Strict Exogeneity\nStrict exogeneity (Assumption 10.3) requires \\(u_t\\) to be uncorrelated with \\(\\bx_s\\), for \\(s=1,2,\\ldots,T.\\)\nIf the unobservables at time \\(t\\)are correlated with any of the explanatory variables in any time period, then Zero Conditional Mean fails. Two leading candidates for failure are\n\nomitted variables and\nmeasurement error in some of the regressors.\n\nOther more subtle causes:\n\nLagged effects\nEither the dependent variable of one of the independent variables is based on expectation.\n\n\nExample 10.1 For example, consider a static model to explain a city’s murder rate (\\(mrdrte_t\\)) in terms of police officers per capita (\\(polpc_t\\)):\n\\[\nmrdrte_t = \\beta_0 + \\beta_1\\, polpc_t + u_t.\n\\]\nSuppose that the city adjusts the size of its police force based on past values of the murder rate, such as\n\\[\npolpc_{t+1} = \\delta_0 + \\delta_1 mrdrte_t + v_t.\n\\]\nThis means that, say, \\(polpc_{t+1}\\) might be correlated with \\(u_t\\) (since a higher \\(u_t\\) leads to a higher \\(mrdrte_t\\)). If this is the case, strict exogeneity is violated.\nMathematically,\n\\[\n\\begin{aligned}\n\\E [u_t polpc_{t+1}] &= \\E [u_t (\\delta_0 + \\delta_1 mrdrte_t + v_t)] \\\\\n&= \\delta_1 \\E[u_t\\, mrdrte_t] \\\\\n&= \\delta_1 \\E[u_t (\\beta_0 + \\beta_1\\, polpc_t + u_t)] \\\\\n&= \\delta_1 \\E[u_t^2] \\\\\n&= \\delta_1 \\sigma^2_u.\n\\end{aligned}\n\\]\n\n\nExample 10.2 A general representation with two explanatory variables:\n\\[\ny_t = \\beta_0 + \\beta_1z_{t1} + \\beta_2z_{t2} + u_t.\n\\] Under weak dependence, the condition sufficient for consistency of OLS is\n\\[\n\\E(u_t\\mid z_{t1}, z_{t2}) = 0\n\\] Importantly, the condition does NOT rule out correlation between, say, \\(u_{t-1}\\) and \\(z_{t1}\\). This type of correlation could arise if \\(z_{t1}\\) is related to past \\(y_{t-1}\\), such as\n\\[\nz_{t1} = \\delta_0 + \\delta_1y_{t-1} + v_t.\n\\] For example, \\(z_{t1}\\) might be a policy variable, such as monthly percentage change in the money supply, and this change might depend on last month’s rate of inflation (\\(y_{t-1}\\)). Such a mechanism generally causes \\(z_{t1}\\) and \\(u_{t-1}\\) to be correlated (as can be seen by plugging in for \\(y_{t-1}\\)). This kind of feedback is allowed under contemporaneous exogeneity.\n\nIn the social sciences, many explanatory variables may very well violate the strict exogeneity assumption. For example, the amount of labor input might not be strictly exogenous, as it is chosen by the farmer, and the farmer may adjust the amount of labor based on last year’s yield. Policy variables, such as growth in the money supply, expenditures on welfare, and highway speed limits, are often influenced by what has happened to the outcome variable in the past.\nThere are similar considerations in distributed lag models. Usually, we do not worry that \\(u_t\\) might be correlated with past \\(z\\) because we are controlling for past \\(z\\) in the model. But feedback from \\(u\\) to future \\(z\\) is always an issue.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "href": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "title": "10  OLS Finite Sample Assumptions and Properties",
    "section": "10.4 Violation of No Serial Correlation",
    "text": "10.4 Violation of No Serial Correlation\nWhen Assumption 10.5 is false, we say that the error suffers from serial correlation, or autocorrelation, because they are correlated across time.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>OLS Finite Sample Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html",
    "href": "TS03_asymptotic-property.html",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "",
    "text": "11.1 Large sample assumptions of OLS\nEquation 11.1 implies\n\\[\n\\E(u_t) = 0 ,\n\\] and\n\\[\n\\E(u_t\\bx_t) = \\bold{0} .\n\\]\nIt is equivalent to say that \\(u_t\\) has zero unconditional mean and is uncorrelated with each \\(x_{tj}, j=1,\\ldots,K\\):\n\\[\n\\cov(x_{tj}, u_t)=0, \\quad j=1,\\ldots,K.\n\\]\nNote that the condition is only on the explanatory variables at time \\(t.\\)\nWe condition only on the explanatory variables in the time periods coinciding with \\(u_t\\) and \\(u_s.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html#large-sample-assumptions-of-ols",
    "href": "TS03_asymptotic-property.html#large-sample-assumptions-of-ols",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "",
    "text": "Assumption 11.1 (Linear and Weak Dependence) We assume the model is exactly as in Assumption 10.1, but now we add the assumption that \\(\\{(\\bx_t, y_t): t=1, 2, \\ldots\\}\\) is stationary and weakly dependent. In particular, the law of large numbers and the central limit theorem can be applied to sample averages.\n\n\nAssumption 11.2 (No Perfect Collinearity) Same as Assumption 10.2.\n\n\nAssumption 11.3 (Zero Conditional Mean) The explanatory variables \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) are contemporaneously exogenous:\n\\[\n\\E(u_t\\mid \\bx_t) = 0.\n\\tag{11.1}\\]\n\n\n\n\n\n\n\nAssumption 11.4 (Homoskedasticity) The errors are contemporaneously homoskedastic, that is, conditional on \\(\\bx_t,\\) the variance of \\(u_t\\) is the same for all \\(t:\\)\n\\[\n\\var(u_t\\mid \\bx_t)=\\var(u_t)=\\sigma^2,\\quad t=1,2,\\ldots,T.\n\\]\n\n\n\nAssumption 11.5 (No Serial Correlation) Conditional on \\(\\bx_t\\) and \\(\\bx_s\\) the errors in two different time periods are uncorrelated:\n\\[\n\\cor(u_t, u_s\\mid \\bx_t, \\bx_s)=0, \\text{ for all } t\\ne s.\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html#large-sample-properties-of-ols",
    "href": "TS03_asymptotic-property.html#large-sample-properties-of-ols",
    "title": "11  OLS Asymptotic Assumptions and Properties",
    "section": "11.2 Large sample properties of OLS",
    "text": "11.2 Large sample properties of OLS\nUnder Assumptions 11.1 through 11.3, the OLS estimators are consistent: \\(\\mathrm{plim}\\, \\hat{\\beta}_j = \\beta_j,\\) \\(j=1,\\ldots,K.\\)\n\nOLS estimators are consistent, but not necessarily unbiased.\nWe have weakened the sense in which the explanatory variables must be exogenous, but weak dependence is required in the underlying time series.\n\n\nUnder Assumptions 11.1 through 11.5, the OLS estimators are asymptotically normally distributed. Further, the usual OLS standard errors, \\(t\\) statistics, \\(F\\) statistics, and \\(LM\\) statistics are asymptotically valid.\nModels with trending explanatory variables can effectively satisfy Assumptions 11.1 through 11.5, provided they are trend stationary. As long as time trends are included in the equations when needed, the usual inference procedures are asymptotically valid.\n\n\nExample 11.1 The AR(1) model cannot satisfy the strict exogeneity assumption (11.3).\nConsider the AR(1) model,\n\\[\ny_t = \\beta_0 + \\beta_1y_{t-1} + u_t,\n\\] where the error \\(u_t\\) has a zero expected value, given all past values of \\(y:\\)\n\\[\n\\E(u_t\\mid y_{t-1},y_{t-2},\\ldots)=0.\n\\tag{11.2}\\]\nCombined, these two equations imply that\n\\[\n\\E(y_t\\mid y_{t-1},y_{t-2},\\ldots) = \\E(y_t\\mid y_{t-1}) =  \\beta_0 + \\beta_1y_{t-1}.\n\\] It means that, once \\(y\\) lagged one period has been controlled for, no further lags of \\(y\\) affect the expected value of \\(y_t.\\)\nBecause \\(\\bx_t\\) contains only \\(y_{t-1},\\) Equation 11.2 implies that contemporaneous exogeneity holds.\nBy contrast, the strict exogeneity assumption is needed for unbiasedness. Since \\(u_t\\) is always correlated with \\(y_t\\) (\\(\\cov(u_t, y_t)=\\var(u_t)&gt;0\\)), strict exogeneity assumption cannot be true.\nTherefore, a model with a lagged dependent variable cannot satisfy the strict exogeneity assumption.\nFor the weak dependence of \\(y_t\\) to hold, we must assume that \\(\\abs{\\beta_1}&lt;1.\\) If this condition holds, then Assumptions 11.1 through 11.3 implies that the OLS estimator from the regression of \\(y_t\\) on \\(y_{t-1}\\) produces consistent estimators of \\(\\beta_0\\) and \\(\\beta_1.\\)\nUnfortunately, \\(\\hat{\\beta}_1\\) s biased, and this bias can be large if the sample size is small or if \\(\\beta_1\\) is near 1. For \\(\\beta_1\\) near 1, \\(\\hat{\\beta}_1\\) can have a severe downward bias. In moderate to large samples, \\(\\hat{\\beta}_1\\) should be a good estimator of \\(\\beta_1.\\)\n\n\nEquation 11.2 also indicates no serial correlation in \\(u_t.\\)\n\nProof. To show that the errors \\(\\{u_t\\}\\) are serially uncorrelated, we must show that \\(\\E(u_tu_s\\mid \\bx_t, \\bx_s)=0\\) for \\(t\\ne s.\\) The explanatory variable at \\(t\\) is \\(y_{t-1}\\), hence we need to prove\n\\[\n\\E(u_tu_s\\mid y_{t-1}, y_{s-1})=0\n\\] Assume \\(s&lt;t,\\) rewrite \\(u_s = y_s-beta_0-beta_1y_{s-1},\\) that is, \\(u_s\\) is a function of \\(y\\) dated before time \\(t.\\)\nBy Equation 11.2, we have\n\\[\n\\E(u_t\\mid u_s, y_{t-1}, y_{s-1}) = 0.\n\\] Then \\[\n\\begin{aligned}\n&\\phantom{=}\\E(u_tu_s \\mid u_s, y_{t-1}, y_{s-1} ) \\\\\n&= u_s\\E(u_t \\mid u_s, y_{t-1} , y_{s-1} )  \\quad (\\text{taking out what is known})\\\\\n&= 0 .\n\\end{aligned}\n\\] By ILE (Law of Iterated Expectations),\n\\[\n\\begin{aligned}\n\\E(u_tu_s \\mid y_{t-1}, y_{s-1} )\n&= \\E \\left[\\E(u_tu_s \\mid u_s, y_{t-1}, y_{s-1} ) \\mid y_{t-1}, y_{s-1} \\right] = 0.\n\\end{aligned}\n\\] Conclusion: as long as only one lag of \\(y\\) appears in \\(\\E(y_t\\mid y_{t-1}, y_{t-2}, \\ldots)\\), the errors \\(\\{u_t\\}\\) must be serially uncorrelated.\n\n\n□\n\n\nExample 11.2 In a static model \\(y_t = \\beta_0+\\beta_1z_t+u_t,\\) the homoskedasticity assumption requires that\n\\[\n\\var(u_t\\mid z_t) = \\var(y_t\\mid z_t) = \\sigma^2.\n\\]\nIn the AR(1) model, \\(y_t = \\beta_0+\\beta_1y_{t-1}+u_t,\\) the homoskedasticity assumption is\n\\[\n\\var(u_t\\mid y_{t-1}) = \\var(y_t\\mid y_{t-1}) = \\sigma^2.\n\\] If we have the model\n\\[\ny_t = \\beta_0 + \\beta_1z_t + \\beta_2y_{t-1} + \\beta_3z_{t-1}+u_t,\n\\] the homoskedasticity assumption is\n\\[\n\\var(u_t\\mid z_t, y_{t-1}, z_{t-1}) = \\var(y_t\\mid z_t, y_{t-1}, z_{t-1}) = \\sigma^2,\n\\] so that the variance of \\(u_t\\) cannot depend on \\(z_t, y_{t-1},\\) or \\(z_{t-1}\\) (or some other function of time).\nGenerally, whatever explanatory variables appear in the model, we must assume that the variance of \\(y_t\\) given these explanatory variables is constant. If the model contains lagged \\(y\\) or lagged explanatory variables, then we are explicitly ruling out dynamic forms of heteroskedasticity. But, in a static model, we are only concerned with \\(\\var(y_t\\mid z_t).\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OLS Asymptotic Assumptions and Properties</span>"
    ]
  },
  {
    "objectID": "TS04_highly-persistent.html",
    "href": "TS04_highly-persistent.html",
    "title": "12  Highly Persistent TS",
    "section": "",
    "text": "Provided the time series we use are weakly dependent, usual OLS inference procedures are valid under assumptions weaker than the classical linear model assumptions. Unfortunately, many economic time series cannot be characterized by weak dependence.\nUsing time series with strong dependence in regression analysis poses no problem, if the CLM assumptions in finite samples hold. But the usual inference procedures are very susceptible to violation of these assumptions when the data are not weakly dependent, because then we cannot appeal to the law of large numbers and the central limit theorem. In this section, we provide some examples of highly persistent (or strongly dependent) time series and show how they can be transformed for use in regression analysis.\nWhen the time series are highly persistent (they have unit roots), we must exercise extreme caution in using them directly in regression models. An alternative to using the levels is to use the first differences of the variables. For most highly persistent economic time series, the first difference is weakly dependent. Using first differences changes the nature of the model, but this method is often as informative as a model in levels. When data are highly persistent, we usually have more faith in first-difference results.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Highly Persistent TS</span>"
    ]
  },
  {
    "objectID": "TS04_TimeSeries_regression.html",
    "href": "TS04_TimeSeries_regression.html",
    "title": "13  Time Series Regression",
    "section": "",
    "text": "13.1 Finite Distributed Lags\nGiven the dynamic relationship of time series data, there are three different ways of modeling these relationships.\nStationarity\nAn assumption that we will maintain throughout this exercise is that variables in our equation are stationary, which means that a variable is one that is not explosive, nor trending, and nor wandering aimlessly without returning to its mean. A stationary variable simply means a variable whose mean, variance and other statistical properties remain constant over time.\nThe first dynamic relationship we consider is the first model that we introduced, which took the form of\n\\(y_t = f(x_t, x_{t-1}, x_{t-2}, \\ldots)\\),\nwith the additional assumption that the relationship is linear, and after \\(q\\) time periods, changes in \\(x\\) no longer have an impact on \\(y\\). Under these conditions, we have the multiple regression model:\n\\[\ny_t = \\alpha + \\beta_0 x_t + \\beta_1 x_{t-1} + \\beta_2 x_{t-2} + \\ldots + \\beta_q x_{t-q} + e_t\n\\tag{13.1}\\]\nThe above model can be treated in the same way as a multiple regression model. Instead of having a number of different explanatory variables, we have a number of different lags of the same explanatory variable. This equation can be very useful in two ways:\nThe effect of a one-unit change in \\(x_t\\) is distributed over the current and next \\(q\\) periods.\nIt is called a finite distributed lag model of order \\(q\\) because it is assumed that after a finite number of periods \\(q\\),\nchanges in \\(x\\) no longer have an impact on \\(y\\). The coefficient \\(\\beta_s\\) is called a distributed-lag weight or an \\(s\\)-period delayed multiplier.\nThe coefficient \\(\\beta_0\\) is called the impact multiplier.\nAdding up a portion of the coefficients gives you the interim multipliers.\nFor example, the interim multiplier for two periods would be \\((\\beta_0 + \\beta_1 + \\beta_2)\\).\nThe total multiplier is the final effect on \\(y\\) on the sustained increase after \\(q\\) or more periods have elapsed and is given by the equation:\n\\[\n\\sum_{s=0}^q \\beta_s\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Regression</span>"
    ]
  },
  {
    "objectID": "TS04_TimeSeries_regression.html#finite-distributed-lags",
    "href": "TS04_TimeSeries_regression.html#finite-distributed-lags",
    "title": "13  Time Series Regression",
    "section": "",
    "text": "Forecasting future values of \\(y\\).\nTo introduce notation for future values, suppose our sample period is \\(t = 1, 2, \\ldots, T\\).\nWe use \\(t\\) for the index and \\(T\\) for the sample size to emphasize the time series nature of the data.\nGiven the last observation in our sample is at \\(t = T\\), the first post-sample observation that we wish to forecast is at \\(t = T + 1\\).\nThe equation for this observation can be given by:\n\\[\ny_{T+1} = \\alpha + \\beta_0 x_{T+1} + \\beta_1 x_T + \\beta_2 x_{T-1} + \\ldots + \\beta_q x_{T-q+1} + e_{T+1}\n\\]\nStrategic Analysis.\nFor example, to use an economic example, understanding the effects of the change in interest rate on unemployment and inflation, or the effect of advertising on sales on a firm’s products.\nThe coefficient \\(\\beta_s\\) gives the change in \\(E(y_t)\\) when \\(x_{t-s}\\) changes by one unit, but \\(x\\) is held constant in other periods.\nAlternatively, if we look forward rather than backward, \\(\\beta_s\\) gives the changes in \\(E(y_{t+s})\\) when \\(x_t\\) changes by one unit, but \\(x\\) in other periods is held constant. In terms of derivatives:\n\\[\n\\frac{\\partial E(y_t)}{\\partial x_{t-s}} = \\frac{\\partial E(y_{t+s})}{\\partial x_t} = \\beta_s\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Regression</span>"
    ]
  },
  {
    "objectID": "TS04_TimeSeries_regression.html#assumptions",
    "href": "TS04_TimeSeries_regression.html#assumptions",
    "title": "13  Time Series Regression",
    "section": "13.2 Assumptions",
    "text": "13.2 Assumptions\nIn distributed lag models, both \\(y\\) and \\(x\\) are typically random. That is, we do not know their values prior to sampling.\nWe do not “set” output growth, for example, and then observe the resulting level of unemployment.\nTo accommodate for this stochastic process, we assume that the \\(x\\)’s are random and that the error term \\(e_t\\) is\nindependent of all \\(x\\)’s in the sample – past, current, and future.\nThis assumption, in conjunction with the other multiple regression assumptions, is sufficient for the least squares estimator to be unbiased and to be BLUE, conditional on the \\(x\\)’s in the sample.\nThe assumptions of the distributed lag model are:\n\n\\(y_t = \\alpha + \\beta_0 x_t + \\beta_1 x_{t-1} + \\beta_2 x_{t-2} + \\ldots + \\beta_q x_{t-q} + e_t\\)\n\\(y\\) and \\(x\\) are stationary random variables, and \\(e_t\\) is independent of current, past, and future values of \\(x\\)\n\\(E(e_t) = 0\\)\n\\(\\text{var}(e_t) = \\sigma^2\\)\n\\(\\text{cov}(e_t, e_s) = 0 \\quad t \\ne s\\)\n\\(e_t \\sim N(0, \\sigma^2)\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Regression</span>"
    ]
  },
  {
    "objectID": "TS04_TimeSeries_regression.html#application-okuns-law",
    "href": "TS04_TimeSeries_regression.html#application-okuns-law",
    "title": "13  Time Series Regression",
    "section": "13.3 Application: Okun’s Law",
    "text": "13.3 Application: Okun’s Law\nWe will apply the finite distributed lag model to Okun’s Law. Arther Melvin Okun was an economist who posited that higher output growth reduces unemployment. Mathematically, Okun’s Law can be expressed as\n\\[\nU_{t}-U_{t-1}=-\\gamma(G_{t}-G_{N})\n\\] where \\(U_{t}\\) is the unemployment rate in period \\(t\\), \\(G_{t}\\) is the growth rate of output in period \\(t\\), and \\(G_{N}\\) is the “normal” growth rate, which we assume constant over time.\nWe can rewrite the above equation in more familiar notation of the multiple regression model by denoting the change in unemployment as \\(DU_{t}=\\Delta U_{t}=U_{t}-U_{t-1}.\\) We then set \\(\\gamma = \\beta_{0}\\) and \\(G_{N} = \\alpha.\\) Including an error term to our equation yields\n\\[\nDU_{t}=\\alpha + \\beta_{0}G_{t} + \\varepsilon_{t}.\n\\]\nAcknowledging the changes in output are likely to have a distributed-lag effect on unemployment – not all of the effect will take place instantaneously. We can then further expand our equation to\n\\[\nDU_{t} = \\alpha + \\beta_{0}G_{t} + \\beta_{1}G_{t-1} + \\beta_{2}G_{t-2} + \\ldots + \\beta_{q}G_{t-q} + \\varepsilon_{t}\n\\]\n\n13.3.1 Empirical Result\n\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/okun.csv\"\nokun2 &lt;- read_csv(f_name)\nokun2 &lt;- okun2 %&gt;% \n    mutate(dunemp = c(NA, diff(unemp))\n           )\n# convert data to a time series object\nokun2.zoo &lt;- okun2 %&gt;% \n    xts(x = .[,-1], order.by = as.yearqtr(.[[1]])) %&gt;% \n    as.zoo()\nokun2\n\n# A tibble: 310 × 5\n   date       unemp      GDP       gGDP     dunemp\n   &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 1948-01-01   3.3 2239.682  1.506038  NA        \n 2 1948-04-01   3.3 2276.69   1.652377   0        \n 3 1948-07-01   3.3 2289.77   0.5745183  0        \n 4 1948-10-01   3.5 2292.364  0.1132865  0.2000000\n 5 1949-01-01   4.1 2260.807 -1.376614   0.6000000\n 6 1949-04-01   5.3 2253.128 -0.3396575  1.2      \n 7 1949-07-01   6.1 2276.424  1.033940   0.8      \n 8 1949-10-01   6.4 2257.352 -0.8378053  0.3000000\n 9 1950-01-01   5.8 2346.104  3.931686  -0.6000000\n10 1950-04-01   5.1 2417.682  3.050930  -0.7      \n# ℹ 300 more rows\n\n\nBefore we proceed, let’s take a preliminary look at growth and unemployment during this time period.\n\nggplot(okun2 %&gt;% gather(\"key\", \"value\", gGDP, dunemp),\n       aes(x = date, y = value, color = key)) +\n    geom_line() + \n    labs(y=\"%\") +\n    scale_color_manual(\n        values = c(gGDP = \"blue\", dunemp = \"red\"),\n        labels = c(gGDP = \"Growth in Real GDP\", dunemp = \"∆ Unemployment Rate\") ) +\n    theme_bw(base_size = 14) +\n    theme(legend.position = c(.2, .9),\n          legend.title = element_blank(),\n          axis.title.x = element_blank(),\n          )\n\n\n\n\nChange in unemployment (red) and growth rate of GDP (blue).\n\n\n\n\nImplement Finite Distributed Lag models (13.1) for \\(q=1\\) and \\(2\\).\n\nlibrary(dynlm)\nokun.lag1 &lt;- dynlm(d(unemp, 1) ~ L(gGDP, 0:1), data = okun2.zoo)\nokun.lag2 &lt;- dynlm(d(unemp, 1) ~ L(gGDP, 0:2), data = okun2.zoo)\n\nModel estimates for lag length at \\(q=2\\)\n\\[\nDU_{t} = \\alpha + \\beta_{0}G_{t} + \\beta_{1}G_{t-1} + \\beta_{2}G_{t-2} + \\varepsilon_{t}.\n\\tag{13.2}\\]\n\nsummary(okun.lag2)\n\n\nTime series regression with \"zoo\" data:\nStart = 1948 Q3, End = 2025 Q1\n\nCall:\ndynlm(formula = d(unemp, 1) ~ L(gGDP, 0:2), data = okun2.zoo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6733 -0.2155 -0.0214  0.1718  4.9551 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.40698    0.03699  11.003  &lt; 2e-16 ***\nL(gGDP, 0:2)1 -0.44244    0.02286 -19.351  &lt; 2e-16 ***\nL(gGDP, 0:2)2 -0.09429    0.02291  -4.116 4.97e-05 ***\nL(gGDP, 0:2)3  0.01078    0.02285   0.472    0.637    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4422 on 303 degrees of freedom\n  (0 observations deleted due to missingness)\nMultiple R-squared:  0.5816,    Adjusted R-squared:  0.5774 \nF-statistic: 140.4 on 3 and 303 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\nDU_{t} = \\alpha + \\beta_{0}G_{t} + \\beta_{1}G_{t-1} + \\varepsilon_{t}.\n\\tag{13.3}\\]\nModel estimates for lag length at \\(q=1\\)\n\nsummary(okun.lag1)\n\n\nTime series regression with \"zoo\" data:\nStart = 1948 Q2, End = 2025 Q1\n\nCall:\ndynlm(formula = d(unemp, 1) ~ L(gGDP, 0:1), data = okun2.zoo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6950 -0.2147 -0.0223  0.1782  4.9746 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.41373    0.03423  12.088  &lt; 2e-16 ***\nL(gGDP, 0:1)1 -0.44041    0.02270 -19.398  &lt; 2e-16 ***\nL(gGDP, 0:1)2 -0.09228    0.02271  -4.063 6.16e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4416 on 305 degrees of freedom\n  (0 observations deleted due to missingness)\nMultiple R-squared:  0.5798,    Adjusted R-squared:  0.577 \nF-statistic: 210.4 on 2 and 305 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\n\\begin{aligned}\n\\widehat{DU}_{t}\n&= 0.414 - 0.440\\,G_{t} - 0.092\\,G_{t-1} \\\\\n&\\phantom{=}\\;\\; (.034)\\quad (.023) \\quad\\quad (.023) \\\\\nn &= 308, R^2=.580, \\bar{R^2} = .577.\n\\end{aligned}\n\\]\nA 1 percentage point increase in the growth rate of GDP leads to a fall in the change of unemployment rate by 0.44 percentage points in the current quarter, a fall of 0.09 percentage points in the next quarter. The delayed effects stop beyond the first lag.\n\n\n13.3.2 ARDL\nA more general model is\n\\[\ny_{t} = \\alpha + \\rho\\, y_{t-1} + \\gamma_{0}z_{t} + \\gamma_{1}z_{t-1} + v_{t},\n\\]\nwhich is an autoregressive distributed lag model (ARDL). It is called an autoregressive distributed lag model because the dependent variable is regressed on its lagged value (the autoregressive component), and it also includes explanatory variables and their lagged values (the distributed lag component).\nThis equation can be estimated by linear least squares providing that the \\(v_t\\) satisfy the usual assumptions required for least squares estimation, namely that they have zero mean and constant variance, and are not autocorrelated.\nThe presence of the lagged dependent variable \\(y_{t−1}\\) means that a large sample is required for the desirable properties of the least squares estimator to hold, but the least squares procedure is still valid providing that \\(v_t\\) is uncorrelated with current and past values of the right-hand side variables.\nIt is crucial that \\(v_t\\) be serial uncorrelated. If they are serial correlated, e.g., \\(v_t=u_t-\\rho u_t\\), the least square estimator will be biased, even in large sample sizes!\nImplement an ARDL model.\n\\[\nDU_{t} = \\alpha + \\rho\\, DU_{t-1} + \\gamma_{0}G_{t} + \\gamma_{1}G_{t-1} + v_{t}.\n\\tag{13.4}\\]\nThe ARDL model (13.4) assumes the effects of growth in output persists, while FDL(\\(q=1\\)) model (13.3) assumes the change in employment responds to changes in output growth only over two periods (current and the last periods).\n\nokun.ardl &lt;- dynlm(d(unemp, 1) ~ L(d(unemp, 1), 1) + L(gGDP, 0:1), data = okun2.zoo)\nsummary(okun.ardl)\n\n\nTime series regression with \"zoo\" data:\nStart = 1948 Q3, End = 2025 Q1\n\nCall:\ndynlm(formula = d(unemp, 1) ~ L(d(unemp, 1), 1) + L(gGDP, 0:1), \n    data = okun2.zoo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4662 -0.2198 -0.0218  0.1686  4.9875 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.42871    0.03769  11.374  &lt; 2e-16 ***\nL(d(unemp, 1), 1) -0.05640    0.05874  -0.960 0.337736    \nL(gGDP, 0:1)1     -0.43423    0.02390 -18.165  &lt; 2e-16 ***\nL(gGDP, 0:1)2     -0.11951    0.03575  -3.343 0.000932 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4416 on 303 degrees of freedom\n  (0 observations deleted due to missingness)\nMultiple R-squared:  0.5825,    Adjusted R-squared:  0.5784 \nF-statistic: 140.9 on 3 and 303 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\n\\begin{aligned}\n\\widehat{DU}_{t}\n&= 0.429 - 0.056 \\, DU_{t-1} - 0.434\\,G_{t} - 0.120\\,G_{t-1} \\\\\n&\\phantom{=}\\;\\; (.038)\\quad (.059) \\qquad\\quad\\quad (.024) \\quad\\quad\\; (.036)\\\\\nn &= 307, R^2=.583, \\bar{R^2} = .579.\n\\end{aligned}\n\\]\nA 1% increase in the change of unemployment rate from the previous quarter leads to a 0.056% decrease in the change of unemployment rate in the current quarter. This suggests a mild mean-reverting behavior: unemployment shocks tend to partially reverse in the following quarter, but the effect is small and statistically insignificant.\nA 1% increase in the growth rate of GDP leads to a fall in the change of unemployment rate of 0.43% in the current quarter, a fall of 0.12% in the next quarter.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Regression</span>"
    ]
  },
  {
    "objectID": "TS04_TimeSeries_regression.html#references",
    "href": "TS04_TimeSeries_regression.html#references",
    "title": "13  Time Series Regression",
    "section": "References",
    "text": "References\n\nCzar Yobero, Time Series Regression with Stationary Variables: An Introduction to the ARDL Model, https://rpubs.com/cyobero/ardl\nOkun’s Law data source: FRED database, https://fred.stlouisfed.org/series/GDPC1",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Regression</span>"
    ]
  },
  {
    "objectID": "PA01_Pooled_regression.html",
    "href": "PA01_Pooled_regression.html",
    "title": "14  Pooled Regression",
    "section": "",
    "text": "14.1 Notation\nThis chapter focuses on panel data regression models whose observations are pairs \\((y_{it}, \\bx_{it})\\) where \\(y_{it}\\) is the dependent variable and \\(\\bx_{it}\\) is a \\(K\\)-vector of regressors — observations on individual \\(i\\) for time period \\(t\\).\nIt will be useful to cluster the observations at the level of the individual. We write \\(\\by_i = (y_{i1}, y_{i2}, \\ldots, y_{iT_i})'\\) as the \\(T_i \\times 1\\) stacked observations on \\(y_{it}\\) for \\(t \\in S_i\\), stacked in chronological order. Similarly, \\(\\bX_i = (\\bx_{i1}, \\bx_{i2}, \\ldots, \\bx_{iT_i})'\\) is the \\(T_i \\times k\\) matrix of stacked \\(\\bx_{it}'\\).\nWhen we assume a balanced panel, that is, \\(T_i = T,\\) for \\(i=1,\\ldots,N.\\) We use this assumption for simplicity in notations throughout the chapter.\nFor the full sample:",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pooled Regression</span>"
    ]
  },
  {
    "objectID": "PA01_Pooled_regression.html#notation",
    "href": "PA01_Pooled_regression.html#notation",
    "title": "14  Pooled Regression",
    "section": "",
    "text": "\\(\\by = (\\by_1', \\by_2', \\dots, \\by_N')'\\) is the \\(n \\times 1\\) stacked vector of stacked \\(\\by_i,\\) and\n\\(\\bX = (\\bX_1', \\bX_2', \\dots, \\bX_N')'\\) likewise.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pooled Regression</span>"
    ]
  },
  {
    "objectID": "PA01_Pooled_regression.html#model-setup",
    "href": "PA01_Pooled_regression.html#model-setup",
    "title": "14  Pooled Regression",
    "section": "14.2 Model Setup",
    "text": "14.2 Model Setup\nThe simplest model in panel regression is the pooled regression. At the level of the observation, the model is:\n\\[\n\\begin{split}\ny_{it} &= \\bx_{it}' \\bbeta + \\varepsilon_{it}, \\\\\n\\mathbb{E}[\\bx_{it} \\varepsilon_{it}] &= 0\n\\end{split}\n\\]\nAt the individual level:\n\\[\n\\begin{split}\n\\by_i &= \\bX_i \\bbeta + \\bvarepsilon_i, \\\\\n\\mathbb{E}[X_i' \\bvarepsilon_i] &= 0 .\n\\end{split}\n\\]\nwhere\n\\[\n\\underset{(T\\times 1)}{\\by_i} = \\begin{bmatrix}\ny_{i1}  \\\\\ny_{i2}  \\\\\n\\vdots \\\\\ny_{iT}  \\\\\n\\end{bmatrix} , \\quad\n\\underset{(T\\times K)}{\\bX_i} = \\begin{bmatrix}\n\\bx_{i1}'  \\\\\n\\bx_{i2}'  \\\\\n\\vdots \\\\\n\\bx_{iT}'  \\\\\n\\end{bmatrix} , \\quad\n\\underset{(N\\times 1)}{\\bvarepsilon_i} = \\begin{bmatrix}\n\\varepsilon_{i1}  \\\\\n\\varepsilon_{i2}  \\\\\n\\vdots \\\\\n\\varepsilon_{iT}  \\\\\n\\end{bmatrix}\n\\]\nFor the full sample:\n\\[\n\\by = \\bX\\beta + \\bvarepsilon\n\\] where\n\\[\n\\underset{(NT\\times 1)}{\\by} = \\begin{bmatrix}\n\\by_{1}  \\\\\n\\by_{2}  \\\\\n\\vdots \\\\\n\\by_{N}  \n\\end{bmatrix} ,\n\\underset{(NT\\times 1)}{\\bvarepsilon} = \\begin{bmatrix}\n\\bvarepsilon_{1}  \\\\\n\\bvarepsilon_{2}  \\\\\n\\vdots \\\\\n\\bvarepsilon_{N}  \n\\end{bmatrix} ,\n\\underset{(NT\\times K)}{\\bX} = \\begin{bmatrix}\n\\bX_{1}  \\\\\n\\bX_{2}  \\\\\n\\vdots \\\\\n\\bX_{N}  \n\\end{bmatrix}\n\\]\nThe least-squares estimator of \\(\\beta\\) is given by:\n\\[\n\\begin{split}\n\\hat{\\beta}_{\\text{pool}}\n&= \\left(\\sum_{i=1}^N\\sum_{t=1}^T \\bx_{it}\\bx_{it}' \\right)^{-1} \\left(\\sum_{i=1}^N\\sum_{t=1}^T \\bx_{it}y_{it} \\right) \\\\\n&= \\left( \\sum_{i=1}^N X_i' X_i \\right)^{-1} \\left( \\sum_{i=1}^N X_i' y_i \\right)  \\\\\n&= (X' X)^{-1} X' y\n\\end{split}\n\\] \\(\\hat{\\beta}_{\\text{pool}}\\) is called the pooled regression estimator.\nThe residuals for individual \\(i\\):\n\\[\n\\widehat{\\bvarepsilon}_i = \\by_i - \\bX_i \\hat{\\bbeta}_{\\text{pool}}\n\\]\nThe pooled regression model is appropriate when:\n\\[\n\\mathbb{E}[\\varepsilon_{it} \\mid \\bX_i] = 0\n\\tag{14.1}\\]\nThis strict mean independence implies that neither lagged nor future values of \\(\\bx_{it}\\) help predict \\(\\varepsilon_{it}\\). It excludes lagged dependent variables (such as \\(y_{it-1}\\)) from \\(\\bx_{it}\\), otherwise \\(\\varepsilon_{it}\\) would be predictable given \\(\\bx_{it}.\\)",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pooled Regression</span>"
    ]
  },
  {
    "objectID": "PA01_Pooled_regression.html#statistical-properties",
    "href": "PA01_Pooled_regression.html#statistical-properties",
    "title": "14  Pooled Regression",
    "section": "14.3 Statistical Properties",
    "text": "14.3 Statistical Properties\nThe estimator can be rewritten as:\n\\[\n\\begin{split}\n\\hat{\\bbeta}_{\\text{pool}}\n&= \\left( \\sum_{i=1}^N \\bX_i' \\bX_i \\right)^{-1} \\left( \\sum_{i=1}^N \\bX_i' \\left(\\bX_i\\bbeta + \\bvarepsilon_i\\right) \\right)  \\\\\n&= \\bbeta + \\left( \\sum_{i=1}^N \\bX_i' \\bX_i \\right)^{-1} \\left( \\sum_{i=1}^N \\bX_i' \\bvarepsilon_i \\right)\n\\end{split}\n\\]\nGiven (18.4), \\(\\hat{\\bbeta}_{\\text{pool}}\\) is unbiased.\n\nIf \\(\\varepsilon_{it}\\) is homoskedastic and serially uncorrelated: use classical variance estimator.\nIf \\(\\varepsilon_{it}\\) is heteroskedastic: use heteroskedasticity-robust estimator.\nIf \\(\\varepsilon_{it}\\) is serially correlated: use cluster-robust covariance matrix:\n\n\\[\n\\widehat{\\bV}_{\\text{pool}} = (\\bX' \\bX)^{-1} \\left( \\sum_{i=1}^N \\bX_i' \\widehat{\\bvarepsilon}_i \\widehat{\\bvarepsilon}_i' \\bX_i \\right) (\\bX' \\bX)^{-1}\n\\]\nWith Stata’s degrees-of-freedom adjustment:\n\\[\n\\widehat{\\bV}_{\\text{pool}} = \\left( \\frac{n - 1}{n - k} \\right) \\left( \\frac{N}{N - 1} \\right)  (\\bX' \\bX)^{-1} \\left( \\sum_{i=1}^N \\bX_i' \\widehat{\\bvarepsilon}_i \\widehat{\\bvarepsilon}_i' \\bX_i \\right) (\\bX' \\bX)^{-1}\n\\]\nWhen strict mean independence (18.4) fails, however, the pooled least-squares estimator \\(\\hat{\\bbeta}_{\\text{pool}}\\) is not necessarily consistent for \\(\\bbeta\\). Since strict mean independence is a strong and typically undesirable restriction, it is typically preferred to adopt one of the alternative estimation approaches such as Fixed Effects or Random Effects models.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pooled Regression</span>"
    ]
  },
  {
    "objectID": "PA02_Panel_regression.html",
    "href": "PA02_Panel_regression.html",
    "title": "15  Panel Regressions",
    "section": "",
    "text": "15.1 Pooled model\nExample: Data from Grunfeld (1958).\nData structure:\nData handling: Select subset of three firms for illustration and declare individuals (“firm”) and time identifier (“year”).\nPooled model:\n\\[\ninvest_{it} = \\beta_0 + \\beta_1\\, value_{it} + \\beta_2\\, capital_{it} + e_{it}.\n\\]\ngr_pool &lt;- plm(invest ~ value + capital, data = pgr, model = \"pooling\")\nsummary(gr_pool)\n\nPooling Model\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"pooling\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-281.940  -94.923   37.000   80.991  291.322 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept) -101.603040   24.401927 -4.1637 0.0001071 ***\nvalue          0.105016    0.010613  9.8952 5.526e-14 ***\ncapital        0.318719    0.040930  7.7870 1.553e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    5644500\nResidual Sum of Squares: 748140\nR-Squared:      0.86746\nAdj. R-Squared: 0.86281\nF-statistic: 186.524 on 2 and 57 DF, p-value: &lt; 2.22e-16",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Panel Regressions</span>"
    ]
  },
  {
    "objectID": "PA02_Panel_regression.html#fixed-effects",
    "href": "PA02_Panel_regression.html#fixed-effects",
    "title": "15  Panel Regressions",
    "section": "15.2 Fixed Effects",
    "text": "15.2 Fixed Effects\nIndividual fixed effects \\[\n\\begin{aligned}\ninvest_{it}\n&= \\beta_1\\, value_{it} + \\beta_2\\, capital_{it} + \\alpha_i + e_{it} \\\\\n&= \\bx'_{it}\\bbeta  + \\alpha_i + e_{it} .\n\\end{aligned}\n\\]\nAssumptions\n\\[\n\\begin{aligned}\n\\E[e_{it}] &= 0 \\\\\n\\E[\\bx_{it}e_{it}] &= 0 \\\\\n\\E[\\alpha_{it}e_{it}] &= 0 \\\\\n\\end{aligned}\n\\]\n\ngr_fe &lt;- plm(invest ~ value + capital, data = pgr, effect = \"individual\", model = \"within\")\nsummary(gr_fe)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, effect = \"individual\", \n    model = \"within\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-167.3305  -26.1407    2.0878   26.8442  201.6813 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(&gt;|t|)    \nvalue   0.104914   0.016331  6.4242 3.296e-08 ***\ncapital 0.345298   0.024392 14.1564 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1888900\nResidual Sum of Squares: 243980\nR-Squared:      0.87084\nAdj. R-Squared: 0.86144\nF-statistic: 185.407 on 2 and 55 DF, p-value: &lt; 2.22e-16\n\n\nQ: Are the fixed effects really needed?\nA: Compare fixed effects and pooled OLS fits via pFtest().\n\npFtest(gr_fe, gr_pool)\n\n\n    F test for individual effects\n\ndata:  invest ~ value + capital\nF = 56.825, df1 = 2, df2 = 55, p-value = 4.148e-14\nalternative hypothesis: significant effects\n\n\nThis indicates substantial inter-firm variation.\nTwo-way fixed effects \\[\ninvest_{it} = \\beta_1\\, value_{it} + \\beta_2\\, capital_{it} + \\alpha_i + \\gamma_t + e_{it}.\n\\]\n\ngr_fe2 &lt;- plm(invest ~ value + capital, data = pgr, effect = \"twoways\", model = \"within\")\nsummary(gr_fe2)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, effect = \"twoways\", \n    model = \"within\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-153.3683  -29.1392    2.2297   34.7902  125.2116 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(&gt;|t|)    \nvalue   0.129467   0.022436  5.7706 1.408e-06 ***\ncapital 0.418383   0.035293 11.8546 5.471e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    956890\nResidual Sum of Squares: 137520\nR-Squared:      0.85628\nAdj. R-Squared: 0.76446\nF-statistic: 107.246 on 2 and 36 DF, p-value: 6.8414e-16",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Panel Regressions</span>"
    ]
  },
  {
    "objectID": "PA02_Panel_regression.html#random-effects",
    "href": "PA02_Panel_regression.html#random-effects",
    "title": "15  Panel Regressions",
    "section": "15.3 Random Effects",
    "text": "15.3 Random Effects\nModel specification same as fixed effects but with different assumptions.\n\\[\n\\begin{aligned}\ninvest_{it}\n&= \\beta_1\\, value_{it} + \\beta_2\\, capital_{it} + \\alpha_i + e_{it} \\\\\n&= \\bx'_{it}\\bbeta  + \\alpha_i + e_{it} .\n\\end{aligned}\n\\]\nAssumptions\n\\[\n\\begin{aligned}\n\\E[e_{it}] &= 0 \\\\\n\\E[\\bx_{it}e_{it}] &= 0 \\\\\n\\E[\\alpha_{it}e_{it}] &= 0 \\\\\n\\E[\\bx_{it}\\alpha_{i}] &= 0 \\Leftarrow \\text{New assumption}\\\\\n\\end{aligned}\n\\]\n\\(\\E[\\bx_{it}u_{it}] = 0\\) imposes uncorrelation between the covariates (\\(\\bx_{it}\\)) and the fixed effects (\\(u_{i}\\)).\nBy contrast, in the fixed effects model, there is no such constraint, we can think of \\(u_i\\) as random, but potentially correlated with \\(\\bx_{it}.\\)\n\nIn the panel data literature, approaches that do not restrict the dependence between the unobserved and the observed components are called “fixed effects.”\n\nRandom effects models is estimated by generalized least squares (GLS) by specify model = \"random\" in plm() call.\n\n# Using Wallace-Hussain for Grunfeld data.\ngr_re &lt;- plm(invest ~ value + capital, data = pgr, model = \"random\", random.method = \"walhus\")\nsummary(gr_re)\n\nOneway (individual) effect Random Effect Model \n   (Wallace-Hussain's transformation)\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"random\", \n    random.method = \"walhus\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nEffects:\n                  var std.dev share\nidiosyncratic 4389.31   66.25 0.352\nindividual    8079.74   89.89 0.648\ntheta: 0.8374\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-187.3987  -32.9206    6.9595   31.4322  210.2006 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) -109.976572   61.701384 -1.7824   0.07468 .  \nvalue          0.104280    0.014996  6.9539 3.553e-12 ***\ncapital        0.344784    0.024520 14.0613 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1988300\nResidual Sum of Squares: 257520\nR-Squared:      0.87048\nAdj. R-Squared: 0.86594\nChisq: 383.089 on 2 DF, p-value: &lt; 2.22e-16\n\n\nRecall: Random-effects estimator is essentially FGLS estimator, utilizing OLS after “quasi-demeaning” all variables. Precise form of quasi-demeaning depends on random.method selected. Four methods available: Swamy-Arora (default), Amemiya, Wallace-Hussain, and Nerlove.\nComparison of regression coefficients shows that fixed- and random-effects methods yield rather similar results for these data.\nQ: Are the random effects really needed?\nA: Use Lagrange multiplier test. Several versions available in plmtest().\n\nplmtest(gr_pool)\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  invest ~ value + capital\nnormal = 15.47, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\nTest also suggests that some form of parameter heterogeneity must be taken into account.\nRandom-effects methods more efficient than fixed-effects estimator under more restrictive assumptions, namely exogeneity of the individual effects.\nUse Hausman test to test for endogeneity.\n\\[\nH_0: \\text{The individual effects are uncorrelated with other regressors.}\n\\]\n\nphtest(gr_re, gr_fe)\n\n\n    Hausman Test\n\ndata:  invest ~ value + capital\nchisq = 0.04038, df = 2, p-value = 0.98\nalternative hypothesis: one model is inconsistent\n\n\nIn line with estimates presented above, endogeneity does not appear to be a problem here. We can apply the Random effects model here.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Panel Regressions</span>"
    ]
  },
  {
    "objectID": "PA02_Panel_regression.html#dynamic-linear-models",
    "href": "PA02_Panel_regression.html#dynamic-linear-models",
    "title": "15  Panel Regressions",
    "section": "15.4 Dynamic Linear Models",
    "text": "15.4 Dynamic Linear Models\nArellano and Bond (1991) employ an unbalanced panel of \\(n=140\\) firms located in the UK. The dataset spans \\(T=9\\) time periods and is available from R package plm. Arellano and Bond (1991) investigate employment equations and consider the dynamic specification:\n\\[\n\\begin{split}\nemp_{i,t}\n&= \\alpha_1 emp_{i,t-1} + \\alpha_2 emp_{i,t-2} + \\bbeta'(L) \\bx_{it} + \\lambda_t + \\eta_i + \\varepsilon_{i,t} \\\\\n&= \\alpha_1 emp_{i,t-1} + \\alpha_2 emp_{i,t-2}  \\\\\n&\\phantom{=}\\quad  + \\beta_1 wage_{i,t} + \\beta_2 wage_{i,t-1} \\\\\n&\\phantom{=}\\quad  + \\beta_3 capital_{i,t} + \\beta_4 capital_{i,t-1} + \\beta_5 capital_{i,t-2} \\\\\n&\\phantom{=}\\quad + \\beta_6 output_{i,t} + \\beta_7 output_{i,t-1} + \\beta_8 output_{i,t-2}  \\\\\n&\\phantom{=}\\quad + \\gamma_3 d_3 + \\dots + \\gamma_T d_T + \\eta_i + \\varepsilon_{i,t},\n\\end{split}\n\\tag{15.1}\\]\nwhere \\(i = 1,...,n\\) denotes the firm, and \\(t = 3,...,T\\) is the time series dimension. The vector \\(\\bx_{it}\\) contains a set of explanatory variables and \\(\\bbeta(L)\\) is a vector of polynomials in the lag operator.\nThe natural logarithm of employment (\\(emp\\)) is explained by its first two lags and the further covariates\n\nnatural logarithm of wage (\\(wage\\)) and its first order of lag,\nthe natural logarithm of capital (\\(capital\\)) and its first and second lags,\nthe natural logarithm of output (\\(output\\)), and its first and second lags.\nVariables \\(d_3, \\ldots, d_T\\) are time dummies with corresponding coefficients \\(\\gamma_3, \\ldots, \\gamma_T\\);\nunobserved individual-specific effects are represented by \\(\\eta_i\\), and\n\\(\\varepsilon_{i,t}\\) is an error term.\n\nThe goal of the empirical analysis is to estimate the lag parameters \\(\\alpha_1\\) and \\(\\alpha_2\\) and the coefficients \\(\\beta_j\\) of the \\(j=1,\\ldots,8\\) further covariates while controlling for (unobserved) time effects and accounting for unobserved individual-specific heterogeneity.\n\nLet’s first run a simplified regression with pgmm.\n\ndata(\"EmplUK\", package = \"plm\")\nform &lt;- log(emp) ~ log(wage) + log(capital) + log(output)\n\nArellano-Bond estimator is provided by pgmm(). Dynamic formula derived from static formula via list of lags.\nGitHub repo for pgmm: https://github.com/cran/plm/blob/59318399c6eb7bcaeb1d0560f1ce08882f0f55c1/R/est_gmm.R#L159\n\n# Arellano and Bond (1991), table 4, col. b \nempl_ab &lt;- pgmm(\n    dynformula(form, list(2, 1, 0, 1)),\n    data = EmplUK, index = c(\"firm\", \"year\"),\n    effect = \"twoways\", model = \"twosteps\",\n    transform = \"d\",\n    gmm.inst = ~ log(emp), lag.gmm = list(c(2, 3)) \n    )\n\nDynamic model with\n\n\\(p=2\\) lagged endogenous terms\n\\(\\log(wage)\\) and \\(\\log(output)\\) occur up to lag 1\n\\(\\log(capital)\\) contemporaneous term only\ntime- and firm-specific effects,\ninstruments are lagged terms of the dependent variable (all lags beyond lag 1 are to be used).\ntransform = \"d\" uses the “difference GMM” model (see Arellano and Bond 1991) or “ld” for the “system GMM” model (see Blundell and Bond 1998).\n\n\nsummary(empl_ab)\n\nTwoways effects Two-steps model Difference GMM \n\nCall:\npgmm(formula = dynformula(form, list(2, 1, 0, 1)), data = EmplUK, \n    effect = \"twoways\", model = \"twosteps\", transformation = \"d\", \n    index = c(\"firm\", \"year\"), gmm.inst = ~log(emp), lag.gmm = list(c(2, \n        3)))\n\nUnbalanced Panel: n = 140, T = 7-9, N = 1031\n\nNumber of Observations Used: 611\nResiduals:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.587961 -0.021925  0.000000  0.002027  0.036812  0.709073 \n\nCoefficients:\n                          Estimate Std. Error z-value  Pr(&gt;|z|)    \nlag(log(emp), c(1, 2))1  0.0168324  0.2749274  0.0612   0.95118    \nlag(log(emp), c(1, 2))2  0.0076269  0.0639007  0.1194   0.90499    \nlog(wage)               -0.3238139  0.1634338 -1.9813   0.04756 *  \nlag(log(wage), 1)       -0.0113247  0.1193372 -0.0949   0.92440    \nlog(capital)             0.3934478  0.0587112  6.7014 2.064e-11 ***\nlog(output)              0.4032315  0.1791580  2.2507   0.02440 *  \nlag(log(output), 1)     -0.0454226  0.1805358 -0.2516   0.80135    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSargan test: chisq(10) = 13.44187 (p-value = 0.2)\nAutocorrelation test (1): normal = 0.1873592 (p-value = 0.85138)\nAutocorrelation test (2): normal = -0.5052488 (p-value = 0.61338)\nWald test for coefficients: chisq(7) = 100.2117 (p-value = &lt; 2.22e-16)\nWald test for time dummies: chisq(6) = 10.3887 (p-value = 0.10921)\n\n\nInterpretation: Autoregressive dynamics important for these data.\nDiagnostics:\n\nSargan test: Assesses the validity of the instruments. A p-value of 0.2 indicates the instruments are valid.\nAutocorrelation test (1): Tests for first-order serial correlation. A p-value &gt; 0.05 indicates that first-order serial correlation is not present.\nThe autocorrelation test is based on Arellano and Bond Serial Correlation Test. The null hypothesis is that there is no serial correlation of a particular order. The test statistic is computed as proposed by Arellano and Bond (1991) and Arellano (2003).\n\np-value &lt; 0.05 indicates the presence of serial correlation.\n\nWald Tests: Assesses the joint significance of all the coefficients or time dummies in the model. The p-value &lt; 0.05 confirms the coefficients and time dummies significantly affect the dependent variable.\n\nNote: Due to constructing lags and taking first differences, three cross sections are lost. Hence, estimation period is 1979–1984 and only 611 observations effectively available for estimation.\npgmm.W: a list of instruments matrices for every individual\n\n# print the instrument matrix for firm 1\nempl_ab$W[[1]]\n\n                                                                       \n1979 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n1980 0 0 1.617604 1.722767 0.000000 0.000000 0.000000 0.000000 0.000000\n1981 0 0 0.000000 0.000000 1.722767 1.612433 0.000000 0.000000 0.000000\n1982 0 0 0.000000 0.000000 0.000000 0.000000 1.612433 1.550749 0.000000\n1983 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.550749\n1984 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n                                  log(capital)  log(output) lag(log(output), 1)\n1979 0.000000 0 0  0  0  0  0 0 0   0.00000000  0.000000000         0.000000000\n1980 0.000000 0 0 -1  1  0  0 0 0  -0.09278789  0.009410626         0.022861846\n1981 0.000000 0 0  0 -1  1  0 0 0  -0.19533733 -0.009914737         0.009410626\n1982 0.000000 0 0  0  0 -1  1 0 0  -0.18255803 -0.009517010        -0.009914737\n1983 1.409278 0 0  0  0  0 -1 1 0  -0.07587391  0.014246777        -0.009517010\n1984 0.000000 0 0  0  0  0  0 0 0   0.00000000  0.000000000         0.000000000\n       log(wage) lag(log(wage), 1)\n1979  0.00000000        0.00000000\n1980  0.07242480        0.04278076\n1981  0.03458784        0.07242480\n1982  0.03967898        0.03458784\n1983 -0.07611583        0.03967898\n1984  0.00000000        0.00000000\n\n\n\nDynamic models using pdynmc.\n\n# compute logarithms of variables\nEmplUK_log &lt;- EmplUK\nEmplUK_log[,c(4:7)] &lt;- log(EmplUK_log[,c(4:7)])\nhead(EmplUK_log)\n\n  firm year sector      emp     wage    capital   output\n1    1 1977      7 1.617604 2.576543 -0.5286502 4.561294\n2    1 1978      7 1.722767 2.509746 -0.4591824 4.578384\n3    1 1979      7 1.612433 2.552526 -0.3899363 4.601245\n4    1 1980      7 1.550749 2.624951 -0.4827242 4.610656\n5    1 1981      7 1.409278 2.659539 -0.6780615 4.600741\n6    1 1982      7 1.152469 2.699218 -0.8606196 4.591224\n\n## data structure check for unbalancedness\nlibrary(pdynmc)\ndata.info(EmplUK_log, i.name = \"firm\", t.name = \"year\")\n\nUnbalanced panel data set with 1031 rows and the following time period frequencies: \n\n\n1976 1977 1978 1979 1980 1981 1982 1983 1984 \n  80  138  140  140  140  140  140   78   35 \n\nstrucUPD.plot(EmplUK_log, i.name = \"firm\", t.name = \"year\")\n\n\n\n\n\n\n\n# Run pdynmc\nm1 &lt;- pdynmc(\n    dat = EmplUK_log, varname.i = \"firm\", varname.t = \"year\",\n    use.mc.diff = TRUE, use.mc.lev = FALSE, use.mc.nonlin = FALSE,\n    include.y = TRUE, varname.y = \"emp\", lagTerms.y = 2,\n    fur.con = TRUE, fur.con.diff = TRUE, fur.con.lev = FALSE,\n    varname.reg.fur = c(\"wage\", \"capital\", \"output\"), lagTerms.reg.fur = c(1,2,2),\n    include.dum = TRUE, dum.diff = TRUE, dum.lev = FALSE, varname.dum = \"year\",\n    w.mat = \"iid.err\", std.err = \"corrected\",\n    estimation = \"onestep\", opt.meth = \"none\" \n    )\n\n\nsummary(m1)\n\n\nDynamic linear panel estimation (onestep)\nGMM estimation steps: 1\n\nCoefficients:\n            Estimate Std.Err.rob z-value.rob Pr(&gt;|z.rob|)    \nL1.emp      0.686226    0.144594       4.746      &lt; 2e-16 ***\nL2.emp     -0.085358    0.056016      -1.524      0.12751    \nL0.wage    -0.607821    0.178205      -3.411      0.00065 ***\nL1.wage     0.392623    0.167993       2.337      0.01944 *  \nL0.capital  0.356846    0.059020       6.046      &lt; 2e-16 ***\nL1.capital -0.058001    0.073180      -0.793      0.42778    \nL2.capital -0.019948    0.032713      -0.610      0.54186    \nL0.output   0.608506    0.172531       3.527      0.00042 ***\nL1.output  -0.711164    0.231716      -3.069      0.00215 ** \nL2.output   0.105798    0.141202       0.749      0.45386    \n1979        0.009554    0.010290       0.929      0.35289    \n1980        0.022015    0.017710       1.243      0.21387    \n1981       -0.011775    0.029508      -0.399      0.68989    \n1982       -0.027059    0.029275      -0.924      0.35549    \n1983       -0.021321    0.030460      -0.700      0.48393    \n1976       -0.007703    0.031411      -0.245      0.80646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n 41 total instruments are employed to estimate 16 parameters\n 27 linear (DIF) \n 8 further controls (DIF) \n 6 time dummies (DIF) \n \nJ-Test (overid restrictions):  48.75 with 25 DF, pvalue: 0.003\nF-Statistic (slope coeff):  528.06 with 10 DF, pvalue: &lt;0.001\nF-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204\n\nmtest.fct(m1)\n\n\n    Arellano and Bond (1991) serial correlation test of degree 2\n\ndata:  1step GMM Estimation\nnormal = -0.51603, p-value = 0.6058\nalternative hypothesis: serial correlation of order 2 in the error terms\n\njtest.fct(m1)\n\n\n    J-Test of Hansen\n\ndata:  1step GMM Estimation\nchisq = 48.75, df = 25, p-value = 0.00303\nalternative hypothesis: overidentifying restrictions invalid\n\nwald.fct(m1, param = \"all\")\n\n\n    Wald test\n\ndata:  1step GMM Estimation\nchisq = 2234.2, df = 16, p-value &lt; 2.2e-16\nalternative hypothesis: at least one time dummy and/or slope coefficient is not equal to zero\n\n\nmtest.fct(m1) is used to test second order serial correlation.\njtest.fct(m1) is used to test Hansen \\(J\\)-test of overidentifying restrictions.\nwald.fct(m1, param = \"all\") is used to test the null hypothesis that the population parameters of all coefficients included in the model are jointly zero.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Panel Regressions</span>"
    ]
  },
  {
    "objectID": "PA02_Panel_regression.html#references",
    "href": "PA02_Panel_regression.html#references",
    "title": "15  Panel Regressions",
    "section": "References",
    "text": "References\n\nGuillermo Corredor, Dynamic AR(1) Panel Estimation in R, https://bookdown.org/gcorredor/dynamic_ar1_panel/dynamic_ar1_panel.html\nKleiber, C. and Zeileis A. (2017), Chap 3 Linear Regression, Applied Econometrics with R.\nArellano M, Bond S (1998). “Dynamic panel data estimation using DPD98 for GAUSS: a guide for users.” unpublished.\nArellano M, Bond S (2012). “Panel data estimation using DPD for Ox.” unpublished, https://www.doornik.com/download/oxmetrics7/Ox_Packages/dpd.pdf.\nBlundell R, Bond S (1998). “Initital Conditions and Moment Restrictions in Dynamic Panel Data Models.” Journal of Econometrics, 87, 115–143.\nRoodman D (2009). “How to do xtabond2: An introduction to difference and system GMM in Stata.” The Stata Journal, 9, 86-136. https://www.stata-journal.com/article.html?article=st0159.\n\n\n\n\n\nArellano, Manuel. 2003. Panel Data Econometrics. Panel Data Econometrics. Oxford University Press. https://doi.org/10.1093/0199245282.001.0001.\n\n\nArellano, Manuel, and Stephen Bond. 1991. “Some Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations.” Review of Economic Studies 58: 277–97.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Panel Regressions</span>"
    ]
  },
  {
    "objectID": "PA03_Fixed-Effects.html",
    "href": "PA03_Fixed-Effects.html",
    "title": "16  Fixed Effects Model",
    "section": "",
    "text": "16.1 Within Estimator\nConsider the following individual fixed effects model (at the level of the observation):\n\\[\n\\begin{aligned}\ny_{it} &= \\alpha_i + \\bbeta'\\bx_{it} + u_{it}, \\quad t=1,2,\\ldots,T, \\\\\n\\text{or}\\quad y_{it} &= \\alpha_i + \\beta_1 x_{it1} + \\beta_2 x_{it2} + \\cdots + \\beta_K x_{itK} + u_{it}.\n\\end{aligned}\n\\tag{16.1}\\] There are \\(K\\) regressors in \\(\\bx_{it},\\) NOT including an intercept.\nWe assume strict exogeneity:\n\\[\n\\E[\\bu_i|\\bX_i]=0.\n\\]\nThroughout, we will assume that the number \\(N\\) of cross-sectional units is potentially large, while the number \\(T\\) of data points per unit is fixed. This setup is more reflective of typical micropanels.\nAlternative notations:\nAt the level of the individual, the model is written as:\n\\[\n\\by_i = \\bX_i \\bbeta + \\bold{1} \\alpha_i + \\bvarepsilon_i  ,\n\\] where \\(\\bold{1}\\) is a \\(T\\times 1\\) column of ones.\nEquation for the full sample:\n\\[\n\\begin{bmatrix}\n\\by_{1}  \\\\\n\\by_{2}  \\\\\n\\vdots \\\\\n\\by_{N}  \n\\end{bmatrix}  =\n\\begin{bmatrix}\n\\bX_{1}  \\\\\n\\bX_{2}  \\\\\n\\vdots \\\\\n\\bX_{N}  \n\\end{bmatrix} \\bbeta + \\begin{bmatrix}\n\\bold{1} & 0  & \\cdots & 0 \\\\\n0 & \\bold{1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0  & \\cdots & \\bold{1} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\alpha_{1}  \\\\\n\\alpha_{2}  \\\\\n\\vdots \\\\\n\\alpha_{N}\n\\end{bmatrix}\n+ \\begin{bmatrix}\n\\bvarepsilon_{1}  \\\\\n\\bvarepsilon_{2}  \\\\\n\\vdots \\\\\n\\bvarepsilon_{M}  \n\\end{bmatrix}\n\\] or\n\\[\n\\begin{split}\n\\by &= \\begin{bmatrix} \\bX & \\bd_1 & \\bd_2 & \\cdots & \\bd_n \\end{bmatrix}\n\\begin{bmatrix}\n\\bbeta \\\\\n\\balpha\n\\end{bmatrix} + \\bvarepsilon \\\\\n&= \\bX\\bbeta + \\bD\\balpha + \\bvarepsilon  ,\n\\end{split}\n\\] where \\(\\bd_i\\) is a dummy variable indicating the \\(i\\)-th unit, and\n\\[\n\\underset{(NT\\times N)}{\\bD} = \\begin{bmatrix}\\bd_1 & \\bd_2 & \\cdots & \\bd_n \\end{bmatrix} .\n\\]\nWithin (or “fixed effects”) estimation of the random intercept model has two steps:",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fixed Effects Model</span>"
    ]
  },
  {
    "objectID": "PA03_Fixed-Effects.html#within-estimator",
    "href": "PA03_Fixed-Effects.html#within-estimator",
    "title": "16  Fixed Effects Model",
    "section": "",
    "text": "Apply the within transformation to each unit.\nApply OLS to the resulting pooled data.\n\n\n16.1.1 Step 1: Within Transformation\nTo perform the within transformation, we first average the equations for unit \\(i\\) across \\(t\\). Label the average of \\(y_{it}\\) across \\(t\\) for unit \\(i\\) as\n\\[\ny_{i\\cdot} = \\frac{1}{T} \\sum_{t=1}^{T} y_{it}.\n\\]\nThe averaged outcome \\(y_{i\\cdot}\\) satisfies the averaged (or “cross-sectional”) equation\n\\[\ny_{i\\cdot} = \\alpha_i + \\bbeta' \\bx_{i\\cdot} + u_{i\\cdot},\n\\tag{16.2}\\] where \\(\\bx_{i\\cdot}\\) and \\(u_{i\\cdot}\\) are defined analogously to \\(y_{i\\cdot}\\).\nDefine the within-transformed equation by subtracting this averaged equation (16.2) from the original equation (16.1) for each \\(t\\):\n\\[\ny_{it} - y_{i\\cdot} = (\\alpha_i - \\alpha_i) + \\bbeta'(\\bx_{it} - \\bx_{i\\cdot}) + (u_{it} - u_{i\\cdot})\n\\] or \\[\n\\tilde{y}_{it} = \\bbeta' \\tilde{\\bx}_{it} + \\tilde{u}_{it}.\n\\tag{16.3}\\]\nwhere \\(\\tilde{y}_{it} = y_{it} - y_{i\\cdot}\\) is the deviations from the group means, and similarly for \\(\\tilde{\\bx}_{it}\\) and \\(\\tilde{u}_{it}.\\)\nUnder Equation 16.1, the within transformation eliminates the individual random intercepts \\(\\alpha_i\\). All average differences in \\(y\\)’s or \\(\\bx\\)’s between individuals have been wiped out. Equation 16.3 now looks like a regular homogeneous regression.\n\n\n16.1.2 Step 2: OLS on the Within-Transformed Equation\nThe within (fixed effects) estimator is obtained by simply pooling the data across \\(i\\) and \\(t\\) in Equation 16.3 and applying OLS to it. Specifically, the estimator is given by\n\\[\n\\hat{\\bbeta}^W = \\left(\\sum_{i=1}^{N} \\tilde{\\bX}_i' \\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^{N} \\tilde{\\bX}_i \\tilde{\\mathbf{y}}_i.\n\\tag{16.4}\\]\n\n\n16.1.3 Properties\nThe within estimator enjoys several desirable properties if the random intercept model reflects the underlying causal model. Most of these properties can be derived from its sampling error representation \\[\n\\hat{\\bbeta}^W = \\bbeta + \\left(\\sum_{i=1}^{N} \\tilde{\\bX}_i' \\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^{N} \\tilde{\\bX}_i \\tilde{\\bu}_i.\n\\tag{16.5}\\]\n\n\\(\\hat{\\bbeta}^W\\) is unbiased for \\(\\bbeta\\). To show this, it is sufficient to notice that strict exogeneity of \\(\\bu_i\\) with respect to \\(\\bX_i\\) implies strict exogeneity of \\(\\tilde{\\bu}_i\\) with respect to \\(\\tilde{\\bX}_i\\): \\[\n\\E[\\tilde{\\bu}_i|\\tilde{\\bX}_i] =\n\\E[\\E[\\tilde{\\bu}_i|\\bX_i]|\\tilde{\\bX}_i] = 0.\n\\] It follows that the mean of the second term in Equation 16.5 is 0, and so \\[\n\\E[\\hat{\\bbeta}^W] = \\bbeta.\n\\]\n\\(\\hat{\\bbeta}^W\\) is consistent for \\(\\bbeta\\) and asymptotically normal, provided a standard rank condition holds for \\(\\tilde{\\bX}_i\\): \\[\n\\hat{\\bbeta}^W \\xrightarrow{p} \\bbeta, \\quad \\sqrt{N}(\\hat{\\bbeta}^W - \\bbeta) \\Rightarrow N(0, \\Sigma).  \n\\tag{16.6}\\]\n\nSince \\(\\bbeta\\) is the average coefficient vector in this homogeneous model, we conclude that the within estimator consistently estimates average coefficients under the random intercept model (16.1).",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fixed Effects Model</span>"
    ]
  },
  {
    "objectID": "PA03_Fixed-Effects.html#between-groups-estimator",
    "href": "PA03_Fixed-Effects.html#between-groups-estimator",
    "title": "16  Fixed Effects Model",
    "section": "16.2 Between-groups Estimator",
    "text": "16.2 Between-groups Estimator\nThe Between-groups estimator (or between estimator) is obtained by regressing the time averaged variables on each other (16.2) (where we include an intercept, \\(\\beta_0\\)) using OLS regression. \\[\ny_{i\\cdot} = \\alpha_i + \\bbeta' \\bx_{i\\cdot} + u_{i\\cdot},\n\\] This is a cross-sectional regression, with a sample size being the number of entities.\nNote that the between estimator is biased when \\(\\alpha_i\\) is correlated with \\(\\bx_{i\\cdot}.\\) If we think \\(\\alpha_i\\) is uncorrelated with \\(\\bx_{i\\cdot},\\) it is better to use the random effects estimator.\nIt can be suitable for research questions that specifically address variation between different entities rather than changes within entities over time. One caveat is that the Between-groups estimator overlooks important information about how variables change over time.\nTo obtain unbiased estimates, it is crucial to ensure the assumption of zero correlation between the error term and averaged independent variables.\n\nlibrary(plm)\nlibrary(AER) \ndata(Grunfeld) \n\n# Estimate Between model\nmodel_between &lt;- plm(invest ~ value + capital, \n                      data = Grunfeld,\n                      index = c(\"firm\", \"year\"),\n                      model = \"between\")\n# Estimate Within model\nmodel_within &lt;- plm(invest ~ value + capital, \n                      data = Grunfeld,\n                      index = c(\"firm\", \"year\"),\n                      model = \"within\")\n\n\nlibrary(stargazer)\nstargazer(model_between, model_within, \n          column.labels = c(\"Between\", \"Within\"),\n          type=\"html\", digits=2, \n          notes = \"&lt;span&gt;&#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&#42;&#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&#42;&#42;&#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.\",\n          notes.append = F)\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ninvest\n\n\n\n\n\n\nBetween\n\n\nWithin\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nvalue\n\n\n0.13***\n\n\n0.11***\n\n\n\n\n\n\n(0.03)\n\n\n(0.01)\n\n\n\n\n\n\n\n\n\n\n\n\ncapital\n\n\n0.03\n\n\n0.31***\n\n\n\n\n\n\n(0.17)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-7.38\n\n\n\n\n\n\n\n\n(40.44)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n11\n\n\n220\n\n\n\n\nR2\n\n\n0.86\n\n\n0.77\n\n\n\n\nAdjusted R2\n\n\n0.83\n\n\n0.75\n\n\n\n\nF Statistic\n\n\n25.50*** (df = 2; 8)\n\n\n340.08*** (df = 2; 207)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fixed Effects Model</span>"
    ]
  },
  {
    "objectID": "PA03_Fixed-Effects.html#references",
    "href": "PA03_Fixed-Effects.html#references",
    "title": "16  Fixed Effects Model",
    "section": "References",
    "text": "References\n\nVladislav Morozov, Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-within-estimator.html#recall-within-estimator-in-the-random-intercept-model\nVladislav Morozov, GitHub course repository, https://github.com/vladislav-morozov/econometrics-heterogeneity/blob/main/src/linear/linear-within-estimator.qmd",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fixed Effects Model</span>"
    ]
  },
  {
    "objectID": "PA04_Random-Effects.html",
    "href": "PA04_Random-Effects.html",
    "title": "17  Random Effects Model",
    "section": "",
    "text": "We begin with the unobserved effects model where we explicitly include an intercept so that we can make the assumption that the unobserved random heterogeneity specific to the \\(i\\)th observation, \\(\\alpha_i\\), has zero mean. \\(\\alpha_i\\) is constant through time. \\(\\beta_0\\) is the mean of the unobserved heterogeneity.\n\\[\n\\begin{aligned}\ny_{it} &= a_i + \\bbeta'\\bx_{it} + u_{it}, \\quad t=1,2,\\ldots,T, \\\\\n\\text{or}\\quad y_{it} &= a_i + \\beta_0 + \\beta_1 x_{it1} + \\beta_2 x_{it2} + \\cdots + \\beta_K x_{itK} + u_{it}.\n\\end{aligned}\n\\]\nHere the random effects model assumes \\(\\alpha_i\\) is uncorrelated with each explanatory variable in all time periods.\n\\[\n\\cov(x_{itj}, a_i) = 0, \\quad t=1,2,\\ldots,T;\\, j=1,2,\\ldots,K.\n\\]\nIn this case, the fixed effects or first differencing model which eliminates \\(\\alpha_i\\) results in inefficient estimators.\nIf we define the composite error term \\(v_{it}\\) as\n\\[\nv_{it} = a_i + u_{it},\n\\tag{17.1}\\]\nEquation 17.1 is often called an “error components model.”\nThen we have\n\\[\ny_{it} = \\beta_0 + \\beta_1 x_{it1} + \\cdots + \\beta_k x_{itk} + v_{it}.\n\\tag{17.2}\\] Because \\(a_i\\) is in the composite error in each time period, the \\(v_{it}\\) are serially correlated across time.\nAssumptions of random effects model:\n\n\\(\\E[a_i\\mid \\bX] = \\E[u_{it}\\mid \\bX] = 0\\)\n\\(\\E[a_i\\mid \\bX] = \\sigma_a^2\\) and \\(\\E[u_{it}\\mid \\bX] = \\sigma_u^2\\)\n\\(\\E[u_{it}a_j\\mid \\bX] = 0\\) for all \\(i,t\\), and \\(j\\)\n\\(\\E[u_{it}u_{js}\\mid \\bX] = 0\\) if \\(i\\ne j\\) or \\(t\\ne s\\)\n\\(\\E[a_ia_j\\mid \\bX] = 0\\) for \\(i\\ne j\\)\n\nWe view the data structure as blocks of \\(T\\) observations for group \\(i.\\)\nLet\n\\[\n\\bv_i = \\begin{bmatrix}\nv_{i1}, v_{i2}, \\cdots , v_{iT}\n\\end{bmatrix}'.\n\\] Let’s have a look at the variance-covariance matrix of \\(\\bv_i.\\) We have\n\\[\n\\begin{aligned}\n\\E[v_{it}^2\\mid \\bX] &= \\E[a_i^2 + u_{it}^2 + 2a_iu_{it} \\mid \\bx] \\\\\n&= \\sigma_a^2 + \\sigma_u^2 .\n\\end{aligned}\n\\] For \\(t\\ne s\\), \\[\n\\begin{aligned}\n\\E[v_{it}v_{is}\\mid \\bX]\n&= \\E[(a_i+u_{it})(a_i+u_{is})] \\\\\n&= \\E[a_i^2 + a_iu_{it} + a_iu_{is} + u_{it}u_{is} \\mid \\bx] \\\\\n&= \\sigma_a^2 ,\n\\end{aligned}\n\\]\nFor \\(i\\ne j\\) and all \\(t\\) and \\(s\\), since observations \\(i\\) and \\(j\\) are independent, we have: \\[\n\\E[v_{it}v_{js}\\mid \\bX] = 0.\n\\]\nLet \\(\\Sigma = \\E[\\bv_i\\bv_i'\\mid \\bX]\\) be the \\(T\\times T\\) covariance matrix for observation \\(i\\), then\n\\[\n\\begin{aligned}\n\\underset{(T\\times T)}{\\Sigma} &= \\E[\\bv_i\\bv_i'\\mid \\bX] \\\\\n&= \\begin{bmatrix}\n\\sigma_a^2 + \\sigma_u^2 & \\sigma_a^2 & \\cdots & \\sigma_a^2 \\\\\n\\sigma_a^2  & \\sigma_a^2 + \\sigma_u^2 & \\cdots & \\sigma_a^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_a^2  & \\sigma_a^2  & \\cdots & \\sigma_a^2 + \\sigma_u^2\\\\\n\\end{bmatrix} \\\\\n&= \\sigma_a^2 \\bI_T + \\sigma_u^2\\bi_T\\bi_T'\n\\end{aligned}\n\\] where \\(\\bI_T\\) is an \\(T\\times T\\) identity matrix and \\(\\bi_T\\) is a \\(T\\times 1\\) column vector of 1’s.\nThe disturbance covariance matrix for the full \\(NT\\) observations is\n\\[\n\\underset{(NT\\times NT)}{\\Omega} = \\begin{bmatrix}\n\\Sigma & 0 & \\cdots & 0 \\\\\n0 & \\Sigma & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\Sigma \\\\\n\\end{bmatrix} = \\bI_N \\otimes \\Sigma\n\\]\nNote that the errors are serially correlated under the random effects assumptions: \\[\n\\text{Corr}(v_{it}, v_{is}) = \\frac{\\sigma_a^2}{\\sigma_a^2 + \\sigma_u^2}, \\quad t \\neq s,\n\\]\nwhere \\(\\sigma_a^2 = \\text{Var}(a_i)\\) and \\(\\sigma_u^2 = \\text{Var}(u_{it})\\).\nThis (necessarily) positive serial correlation in the error term can be substantial, and, because the usual pooled OLS standard errors ignore this correlation, they will be incorrect, as will the usual test statistics.\nGeneralized least squares (GLS) can be used to estimate models with autoregressive serial correlation. For the procedure to have good properties, we should have large \\(N\\) and relatively small \\(T\\). We assume that we have a balanced panel, although the method can be extended to unbalanced panels.\n\\[\n\\begin{split}\n\\hat{\\bbeta}_{\\text{GLS}} &= \\left(\\bX'\\bOmega^{-1}\\bX \\right)^{-1} \\bX'\\bOmega^{-1}\\by \\\\\n&= \\left(\\sum_{i=1}^N \\bX_i'\\bSigma^{-1}\\bX_i \\right)^{-1} \\left( \\sum_{i=1}^N \\bX_i'\\bSigma^{-1}\\by_i \\right)\n\\end{split}\n\\]\nDeriving the GLS transformation that eliminates serial correlation in the errors requires sophisticated matrix algebra (see, for example, Wooldridge (2010, Chapter 10)). But the transformation itself is simple. Define \\[\n\\theta = 1 - \\left[ \\frac{\\sigma_u^2}{\\sigma_u^2 + T \\sigma_a^2} \\right]^{1/2} ,\n\\]\nwhich is between zero and one. As \\(\\frac{\\sigma_u^2}{\\sigma_u^2 + T \\sigma_a^2}\\) approaches zero, \\(\\theta\\to1.\\)\nThen, the transformed equation turns out to be \\[\n\\begin{aligned}\ny_{it} - \\theta \\overline{y}_i &= \\beta_0 (1 - \\theta) + \\beta_1 (x_{it1} - \\theta \\overline{x}_{i1}) + \\cdots \\\\\n&\\phantom{=}\\quad + \\beta_K (x_{itK} - \\theta \\overline{x}_{iK}) + (v_{it} - \\theta \\overline{v}_i),\n\\end{aligned}\n\\tag{17.3}\\]\nwhere the overbar \\(\\overline{y}_i=T^{-1}\\sum_{t=1}^Ty_{it}\\) is the time averages of \\(y\\) for individual \\(i\\), and similarly for \\(\\overline{x}_{ij},\\) \\(j=1,\\ldots,K.\\)\nThis is a very interesting equation, as it involves quasi-demeaned data on each variable. The fixed effects estimator subtracts the time averages from the corresponding variable. The random effects transformation subtracts a fraction of that time average, where the fraction depends on \\(\\sigma_u^2,\\) \\(\\sigma_a^2\\) and the number of time periods, \\(T\\). This transformation is known as “theta-differencing.”\nThe GLS estimator is simply the pooled OLS estimator of the transformed model (17.3). The errors in (17.3) are serially uncorrelated.\nEquation 17.3 can be written more compactly as follows:\n\\[\ny_{it}^\\ast = \\bbeta' \\bx_{it}^\\ast + u_{it}^\\ast,\n\\tag{17.4}\\] where \\[\ny_{it}^\\ast = y_{it} - \\theta \\overline{y}_i ,\n\\] and\n\\[\n\\begin{split}\n\\theta &= 1 - \\left[ \\frac{\\sigma_u^2}{\\sigma_u^2 + T \\sigma_a^2} \\right]^{1/2} , \\\\\n\\overline{y}_i &= T^{-1}\\sum_{t=1}^Ty_{it}  .\n\\end{split}\n\\]\n\\(\\hat{\\bbeta}_{\\text{GLS}}\\) can be obtained by using OLS on the transformed model (17.4).\nThe transformation in Equation 17.3 allows for explanatory variables that are constant over time, and this is one advantage of random effects (RE) over either fixed effects or first differencing. This is possible because RE assumes that the unobserved effect is uncorrelated with all explanatory variables, whether the explanatory variables are fixed over time or not. Thus, in a wage equation, we can include a variable such as education even if it does not change over time. But we are assuming that education is uncorrelated with \\(a_i\\), which contains ability and family background. In many applications, the whole reason for using panel data is to allow the unobserved effect to be correlated with the explanatory variables.\nThe parameter \\(\\theta\\) is never known in practice, but it can always be estimated. There are different ways to do this, which may be based on pooled OLS or fixed effects, for example. Generally, \\(\\hat{\\theta}\\) takes the form \\[\n\\hat{\\theta} = 1 - \\left[ \\frac{\\hat{\\sigma}_u^2}{\\hat{\\sigma}_a^2 + T \\hat{\\sigma}_u^2} \\right]^{1/2},\n\\] where \\(\\hat{\\sigma}_a^2\\) is a consistent estimator of \\(\\sigma_a^2\\) and \\(\\hat{\\sigma}_u^2\\) is a consistent estimator of \\(\\sigma_u^2\\).\nThese estimators can be based on the pooled OLS or the fixed effects residuals (Within Groups or Between Groups estimators). \\[\n\\hat{\\sigma}_a^2 = \\left[ \\frac{NT(T - 1)}{2} - (K + 1) \\right]^{-1} \\sum_{i=1}^{N} \\sum_{t=1}^{T-1} \\sum_{s=t+1}^{T} \\hat{v}_{it} \\hat{v}_{is},\n\\] where the \\(\\hat{v}_{it}\\) are the residuals from estimating Equation 17.2 by pooled OLS.\nGiven this, we can estimate \\(\\sigma_u^2\\) by using \\[\n\\hat{\\sigma}_u^2 = \\hat{\\sigma}_v^2 - \\hat{\\sigma}_a^2,\n\\] where \\(\\hat{\\sigma}_v^2\\) is the square of the usual standard error of the regression from pooled OLS. [See Wooldridge (2010, Chapter 10) for additional discussion of these estimators.]\nThe feasible GLS estimator that uses \\(\\hat{\\theta}\\) in place of \\(\\theta\\) is called the random effects estimator.\nUnder the random effects assumptions, the estimator is consistent (not unbiased) and asymptotically normally distributed as \\(N\\) gets large with fixed \\(T\\).\nThe properties of the random effects (RE) estimator with small \\(N\\) and large \\(T\\) are largely unknown, although it has certainly been used in such situations.\n\nComparing RE estimates and FE estimates: When \\(\\theta=1\\), RE becomes the FE model. Recall that \\[\n\\theta = 1 - \\left[ \\frac{\\sigma_u^2}{\\sigma_a^2 + T \\sigma_u^2} \\right]^{1/2},\n\\] There are three scenarios where \\(\\theta\\to1\\):\n\n\\(T\\) is big \\(\\rightarrow\\) Lots of variation across time for each individual \\(\\rightarrow\\) more like fixed effects\n\\(\\sigma_a^2\\) is big \\(\\rightarrow\\) Lots of variation in the fixed effects \\(\\rightarrow\\) more like fixed effects\n\\(\\sigma_u^2\\) is small relative to \\(\\sigma_a^2\\) \\(\\rightarrow\\) idiosyncratic variation is small, more variation from fixed effects \\(\\rightarrow\\) more like fixed effects\n\nComparing RE estimates and Pooled OLS: When \\(\\theta=0\\), RE becomes the Pooled OLS. This happens when the unobserved effect, \\(a_i,\\) is relatively unimportant (because it has small variance relative to \\(\\sigma^2_u\\)).\n\nSummary of RE:\n\nRandom effects estimators are a weighted average of the between estimator (variation between individuals in a cross section) and the within/fixed estimator (variation within individuals over time)\nRandom effects estimators will be consistent and unbiased if fixed effects are not correlated with \\(\\bx\\)’s. Fixed effects estimators will always be consistent and unbiased (under usual GM assumptions)\nRandom effects estimators will be more efficient (have smaller standard errors) than fixed effects estimators because they use more of the variation in \\(X\\) (specifically, they use the cross sectional/between variation)",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Random Effects Model</span>"
    ]
  },
  {
    "objectID": "PA05_dynamic_models.html",
    "href": "PA05_dynamic_models.html",
    "title": "18  Dynamic Models",
    "section": "",
    "text": "18.1 Exogeneity and Dynamic Models",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dynamic Models</span>"
    ]
  },
  {
    "objectID": "PA05_dynamic_models.html#exogeneity-and-dynamic-models",
    "href": "PA05_dynamic_models.html#exogeneity-and-dynamic-models",
    "title": "18  Dynamic Models",
    "section": "",
    "text": "18.1.1 Introduction\nThroughout the previous section, we assumed that the idiosyncratic term \\(u_{it}\\) satisfied strict exogeneity with respect to the data: \\[\n    \\E[u_{it}|\\bX_i] = 0.\n\\]\nIt implies that for all pairs of indices \\(s\\) and \\(t,\\) it holds that \\[\n    \\E[u_{it}\\bx_{is}] =0.\n\\tag{18.1}\\]\n\nHowever, strict exogeneity is not possible in the presence of lagged dependent variables.\nTherefore, sequential exogeneity requires only (18.1) holds for \\(t\\ge s.\\) That is,\n\\[\n  \\E[u_{it}\\bx_{is}] = 0 \\text{ for } t\\ge s.\n   \\tag{18.2}\\]\nIn plain language, the current and past values of \\(\\bx\\) are uncorrelated with the current error term, although its future values may not be.\nIn this case, the regressor \\(\\bx\\) is called “predetermined” as its value is determined prior to the current period.\nExamples of predetermined variables:\n\nLagged dependent variables: Variables that represent past values of the dependent variable.\nExogenous variables: Variables that are determined outside the model and are not influenced by the endogenous variables within the current period.\n\nFor \\(t&lt;s\\), Equation 18.1 means that past shocks are uncorrelated with future values of \\(\\bx\\). In other words, one cannot predict future \\(\\bx\\)’s from past shocks.\nThis requirement might fail if \\(\\bx\\) is dynamic, e.g., covaraites including lagged dependent variables, and its evolution is affected by \\(u_{it}\\). That is, \\[\n  \\E[u_{it}\\bx_{is}] \\ne 0 \\text{ for } t&lt;s.\n  \\] This means, \\(\\bx_{is}\\) can respond dynamically to past values of \\(y_{it}.\\)\nIn this section, we will discuss this challenge and some traditional approaches to dealing with it with short panel data.\n\n\n\n18.1.2 No Strict Exogeneity In the Presence of Lagged Dependent Variables\nConsider the AR(1) model, \\[\ny_t = \\beta_0 + \\beta_1 y_{t-1} + u_t,\n\\tag{18.3}\\] where the error \\(u_t\\) has a zero expected value, given all past values of \\(y\\): \\[\n\\mathbb{E}(u_t \\mid y_{t-1}, y_{t-2}, \\ldots) = 0.\n\\tag{18.4}\\]\nConventions for \\(t\\):\n\nThe first observation is \\(y_1\\), and \\(t=2, \\ldots, T\\), so that the first available equation is\n\\[\n  y_2 = \\beta_0 + \\beta_1 y_{1} + u_2 .\n  \\]\nand there are (\\(T-1\\)) equations in levels. We will mostly use this convention.\nThe first observation is \\(y_0\\), and \\(t=1, \\ldots, T\\), so that the first available equation is\n\\[\n  y_1 = \\beta_0 + \\beta_1 y_{0} + u_1 .\n  \\]\nand there are (\\(T\\)) equations in levels.\n\nCombined, these two equations imply that \\[\n\\mathbb{E}(y_t \\mid y_{t-1}, y_{t-2}, \\ldots) = \\mathbb{E}(y_t \\mid y_{t-1}) = \\beta_0 + \\beta_1 y_{t-1}.\n\\]\nThis result is very important. First, it means that, once \\(y\\) lagged one period has been controlled for, no further lags of \\(y\\) affect the expected value of \\(y_t\\). (This is where the name “first order” originates.) Second, the relationship is assumed to be linear.\nBecause \\(x_t\\) contains only \\(y_{t-1}\\), Equation 18.4 implies that the contemporaneous exogeneity Assumption holds. By contrast, the strict exogeneity assumption needed for unbiasedness, which does not hold.\nIn fact, because \\(u_t\\) is uncorrelated with \\(y_{t-1}\\) under Equation 18.4, \\(u_t\\) and \\(y_t\\) must be correlated.\n\\[\n\\begin{aligned}\n\\text{Cov}(y_t, u_t) &= \\text{Cov}(\\beta_0 + \\beta_1 y_{t-1} + u_t, u_t) \\\\\n&= \\text{Var}(u_t) &gt; 0.\n\\end{aligned}\n\\]\nTherefore, a model with a lagged dependent variable cannot satisfy the strict exogeneity assumption.\nFor the weak dependence condition to hold, we must assume that \\(|\\beta_1| &lt; 1\\). If this condition holds, then the OLS estimator from the regression of \\(y_t\\) on \\(y_{t-1}\\) produces consistent estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\nUnfortunately, \\(\\hat{\\beta}_1^{OLS}\\) is biased, and this bias can be large if the sample size is small or if \\(\\beta_1\\) is near 1. (For \\(\\beta_1\\) near 1, \\(\\hat{\\beta}_1^{OLS}\\) can have a severe downward bias.) In moderate to large samples, \\(\\hat{\\beta}_1^{OLS}\\) should be a good estimator of \\(\\beta_1\\).\nEquation 18.4 is called the sequential exogeneity assumption. Or more generally, using \\(\\bx\\) to represent the vector of covariates, the sequential exogeneity is:\n\\[\n\\E(u_t \\mid \\bx_t, \\bx_{t-1}, \\ldots,) = \\E(u_t) = 0, t=1,2,\\ldots.\n\\tag{18.5}\\]\nWe can see that sequential exogeneity implies contemporaneous exogeneity.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dynamic Models</span>"
    ]
  },
  {
    "objectID": "PA05_dynamic_models.html#dynamic-panel-representation",
    "href": "PA05_dynamic_models.html#dynamic-panel-representation",
    "title": "18  Dynamic Models",
    "section": "18.2 Dynamic Panel Representation",
    "text": "18.2 Dynamic Panel Representation\nDynamic models account for temporal dependencies by including the lagged dependent variable, often providing more accurate results than static panel models.\nA Nickell bias arises from including the lagged dependent variable as an explanatory variable, making standard panel data estimators (FE, RE, FD) inconsistent, especially in analyses with short T and large N. System-GMM estimation addresses this by instrumenting the endogenous variables with their lagged values.\nMore generally, consider a panel data linear model with homogeneous coefficients, a random intercept, and a dynamic process in the \\(K\\)-th order for the outcome: \\[\ny_{it} = \\alpha_i + \\sum_{k=1}^K \\lambda_k y_{it-k} + \\bbeta'\\bx_{it} + u_{it}.  \n\\tag{18.6}\\]\nNotice that the RHS now includes the lagged dependent variables \\(y_{i,t-k},\\) \\(k=1,\\ldots,K.\\) \\(\\lambda_k\\) are the autoregressive coefficients, \\(\\bx_{it}\\) is a vector of regressors. \\(\\alpha_i\\) is an individual-effect, and \\(u_{it}\\) is an idiosyncratic error.\nIt is conventional to assume that \\(\\alpha_i\\) and the error \\(u_{it}\\) are mutually independent\n\\[\n\\cov(\\alpha_i, u_{it}) = 0,\n\\]\nand \\(u_{it}\\) are serially uncorrelated:\n\\[\n\\E[u_{it}u_{is}] = 0 \\quad \\text{for } t \\ne s.\n\\]\nAdding dynamics to a model makes major changes in the interpretation of the equation. The entire history of the LHS variable enters into the equation. Any measured influence is conditioned on this history. Any impact of \\(\\bx_{it}\\) represents the effects of new information.\nIn Equation 18.6, a sequential exogeneity assumption takes form \\[\n     \\E[u_{it}\\mid \\curl{y_{is-1}, \\bx_{is}}_{s\\leq t}] =0.\n\\]\nFor the sake of simplicity, we will discuss a simple version of model (18.6) without extra covariates \\(\\bx_{it}\\) and with only one lag of \\(y_{it}\\): \\[\n\\color{#008B45} {y_{it} = \\alpha_i + \\lambda y_{it-1} + u_{it}, }\n\\tag{18.7}\\] where \\(i=1,\\ldots,N,\\) and \\(t=2,\\ldots,T.\\) And \\(u_{it}\\) satisfies sequential exogeneity in the form \\(\\E[u_{it}|y_{is}, s&lt;t] = 0.\\)\nA popular class of estimators that are consistent as \\(N\\to\\infty\\) with \\(T\\) fixed first transform the model to eliminate the individual effects \\(\\alpha_i\\), and then apply instrumental variables.\nLet us difference Equation 18.7 across \\(t\\) to eliminate the random intercept \\(\\alpha_i\\). The differenced equation takes form\n\\[\n\\color{#008B45} {\\Delta y_{it} = \\lambda \\Delta y_{it-1} + \\Delta u_{it}} ,\n\\tag{18.8}\\]\nfor \\(t=3,\\ldots,T,\\) where \\(\\Delta y_{it} = y_{it} - y_{it-1}.\\) Equation 18.8 seems like a simple regression equation. It is tempting to just apply OLS and regress \\(\\Delta y_{it}\\) on \\(\\Delta y_{it-1}\\). But it turns out that Equation 18.8 has an endogeneity problem. To prove this, we can show that \\(\\E[\\Delta y_{it-1} \\Delta u_{it}] \\ne 0.\\)\nPlug in \\(\\Delta y_{it-1} = y_{it-1}-y_{it-2}\\) and \\(\\Delta u_{it} = u_{it}-u_{it-1}.\\)\n\\[\n\\begin{aligned}\n\\E[\\Delta y_{it-1} \\Delta u_{it}]\n&= \\E[(y_{it-1}-y_{it-2})(u_{it}-u_{it-1})] \\\\\n&= \\underbrace{\\E[y_{it-1}u_{it}]}_{0} - \\underbrace{\\E[y_{it-2}u_{it}]}_0 - \\E[y_{it-1}u_{it-1}] + \\underbrace{\\E[y_{it-2}u_{it-1}]}_0 \\\\\n&= -\\E[y_{it-1}u_{it-1}] \\\\\n&= -\\E[(\\alpha_i + \\lambda y_{it-2} + u_{it-1}) u_{it-1}] \\\\\n&= -\\E[u_{it-1}^2] \\\\\n&= -\\sigma_u^2 \\ne 0\n\\end{aligned}\n\\]\nHence we conclude that\n\\[\n\\E[\\Delta y_{it-1} \\Delta u_{it}] \\ne 0,\n\\] and that \\(\\Delta y_{it-1}\\) is an endogeneous regressor in the differenced Equation 18.8. Hence, we see that the first difference estimator is biased.\nNote that the bias is large when \\(T\\) is relatively small, which is common for micro data. When \\(T\\) is large the Nickell’s bias is relatively small.\nThe dynamic panel data (DPD) models are designed to account for this endogeneity.\n\n18.2.1 IV Estimator of Dynamic Panel Models\nGiven the endogeneity issue in first-differencing, instrumental variable methods offer a potential solution. We only need to find suitable instruments – variables \\(z_{it}\\) which satisfy relevance and exogeneity conditions: \\[\n\\begin{aligned}\n    \\E[z_{it}\\Delta y_{it-1}] \\neq 0,\\\\\n    \\E[z_{it}\\Delta u_{it}] = 0.\n\\end{aligned}\n\\]\nIn most contexts, one has to look for external variables that can serve as instruments.\nHowever, model (18.8) is a very special case where one can use instruments internal to the model!\n\n18.2.1.1 The Anderson-Hsiao Estimator\nIn particular, consider using twice lagged level of dependent variable \\(y_{it-2}\\) as an instrument for \\(\\Delta y_{it-1}.\\)\n\nRelevance seems to be relatively straightforward, as the target endogenous variable \\(\\Delta y_{it-1} = y_{it-1}-y_{it-2}\\) actually has the instrument inside.\n\\[\n  \\E[y_{it-2}\\Delta y_{it-1}] = \\E[y_{it-2}(y_{it-1}-y_{it-2})] \\ne 0.\n  \\]\n\\(y_{it-2}\\) is correlated with the endogenous variable \\(\\Delta y_{it-1}.\\)\nExogeneity can be justified by appealing to sequential exogeneity:\n\\[  \n  \\E[y_{it-2}\\Delta u_{it}]  = \\E[y_{it-2}u_{it} - y_{it-2}u_{it-1}]=0.\n   \\tag{18.9}\\]\n\\(y_{it-2}\\) is not correlated with the error term \\(\\Delta u_{it}.\\) The orthogonality condition (18.9) ensures the validity of the instrument.\n\nHence \\(y_{it-2}\\) is a valid instrument for \\(\\Delta y_{it-1}.\\)\nThe resulting IV estimator (\\(\\hat{\\bbeta}_{\\mathrm{iv}} = (\\bZ'\\bX)^{-1}(\\bZ'\\by)\\)) is known as the Anderson and Hsiao (1982) estimator and given by \\[\n\\begin{split}\n\\hat{\\lambda}_{AH}^{\\mathrm{iv}}\n&= (\\bZ'\\Delta\\by_{-1})^{-1} (\\bZ'\\Delta\\by)  \\\\\n&= \\dfrac{ \\sum_{i=1}^N \\sum_{t=3}^T y_{it-2}\\Delta y_{it} }{\\sum_{i=1}^N \\sum_{t=3}^T y_{it-2}\\Delta y_{it-1} }.\n\\end{split}\n\\] where\n\n\\(\\Delta \\by\\) is the stacked \\(N(T-2)\\times 1\\) vector of observations on \\(\\Delta y_{it},\\)\n\\(\\Delta \\by_{-1}\\) is the stacked \\(N(T-2)\\times 1\\) vector of observations on \\(\\Delta y_{i,t-1},\\) and\n\\(\\bZ\\) is the stacked \\(N(T-2)\\times 1\\) vector of observations on \\(y_{i,t-2}.\\)\n\nThe 2SLS estimator (\\(\\widehat{\\bbeta}_{2\\text{sls}} = \\left[\\bX'\\bZ (\\bZ'\\bZ)^{-1} \\bZ'\\bX \\right]^{-1} \\left[\\bX'\\bZ (\\bZ'\\bZ)^{-1} \\bZ'\\by \\right]\\)) is given by\n\\[\n\\begin{split}\n\\hat{\\lambda}_{AH}^{\\text{2sls}}\n&= \\left[\\Delta\\by_{-1}'\\bZ (\\bZ'\\bZ)^{-1} \\bZ'\\Delta\\by_{-1}\\right]^{-1} \\left[\\Delta\\by_{-1}'\\bZ (\\bZ'\\bZ)^{-1} \\bZ'\\Delta\\by \\right]  .\n\\end{split}\n\\]\nOne might use the twice lagged differences \\(\\Delta y_{it-2} = y_{it-2}-y_{it-3}\\) as an instrument for \\(\\Delta y_{it-1}.\\) But\n\nOne further time series observation is lost if \\(\\Delta y_{i,t-2}\\) rather than \\(y_{it-2}\\) is used as the instrument.\nLarger asymptotic variance\n\nThe Anderson-Hsiao (AH) estimator delivers consistent but not eﬃcient estimates of the parameters in the model. This is due to the fact that the IV doesn’t exploit all the available moments conditions.\nWith \\(T=3,\\) we have one instrument \\(y_{i1}\\) for \\(\\Delta y_{i2}\\) in the equation\n\\[\n\\Delta y_{i3} = \\lambda \\Delta y_{i2} + \\Delta u_{i3} , \\quad \\text{for } i=1,\\ldots, N.\n\\] With \\(T&gt;3,\\) further valid instruments become available for the first differenced equations in the later time periods. Efficiency can be improved by exploiting these additional instruments.\nThe IV estimator also ignores the structure of the error component in the transformed model.\n\nThe autocorrelation in the first differenced errors leads to inconsistency of the IV estimates.\nThe IV estimates would be inconsistent when other regressors are correlated with the error term.\n\n\n\n18.2.1.2 The Arellano-Bond Estimator\nThe orthogonality condition (18.9) is one of many implied by the dynamic panel model. Indeed, all lags \\(y_{it-2},\\) \\(y_{it-3},\\) are valid instruments. If \\(T &gt; p + 2\\) (\\(p\\) is the order of autocorrelation) these can be used to potentially improve estimation efficiency. This was first pointed out by Holtz-Eakin, Newey, and Rosen (1988) and further developed by Arellano and Bond (1991).\nArellano and Bond (1991) expanded the idea by using additional lags of the dependent variable as instruments.\nStarting point: the FD estimator\n\\[\n\\Delta y_{it} = \\lambda \\Delta y_{it-1} + \\Delta u_{it},\n\\]\nValid instruments:\n\n\\(t = 2 \\text{ or } t = 1\\): no instruments,\n\\(t = 3\\): the valid instrument for \\(\\Delta y_{i2} = (y_{i2} - y_{i1})\\) is \\(y_{i1}\\), the moment condition is\n\n\\[\n\\E[y_{i1}\\Delta u_{i3}] = 0 \\text{ for }i=1,2,\\ldots,N\n\\]\n\n\\(t = 4\\): the valid instruments for \\(\\Delta y_{i3} = (y_{i3} - y_{i2})\\) are \\(y_{i2}\\) as well as \\(y_{i1}\\), the moment conditions are\n\n\\[\n\\begin{aligned}\n\\E[y_{i1}\\Delta u_{i4}] &= 0 \\\\\n\\E[y_{i2}\\Delta u_{i4}] &= 0\n\\end{aligned}\n\\]\n\n\\(t = 5\\): the valid instruments for \\(\\Delta y_{i4} = (y_{i4} - y_{i3})\\) are \\(y_{i3}\\) as well as \\(y_{i2}\\) and \\(y_{i1}\\), the moment conditions are\n\n\\[\n\\begin{aligned}\n\\E[y_{i1}\\Delta u_{i5}] &= 0 \\\\\n\\E[y_{i2}\\Delta u_{i5}] &= 0 \\\\\n\\E[y_{i3}\\Delta u_{i5}] &= 0 \\\\\n\\end{aligned}\n\\]\n\n\\(t = 6\\): the valid instruments for \\(\\Delta y_{i5} = (y_{i5} - y_{i4})\\) are \\(y_{i4}\\) as well as \\(y_{i3}\\), \\(y_{i2}\\), and \\(y_{i1}\\), the moment conditions are\n\n\\[\n\\begin{aligned}\n\\E[y_{i1}\\Delta u_{i6}] &= 0 \\\\\n\\E[y_{i2}\\Delta u_{i6}] &= 0 \\\\\n\\E[y_{i3}\\Delta u_{i6}] &= 0 \\\\\n\\E[y_{i4}\\Delta u_{i6}] &= 0 \\\\\n\\end{aligned}\n\\]\n\n\\(t = T\\): the valid instruments for \\(\\Delta y_{iT-1} = (y_{iT-1} - y_{iT-2})\\) are \\(y_{iT-2}\\) as well as \\(y_{iT-3}, \\dots, y_{i1}\\), the moment conditions are\n\n\\[\n\\begin{aligned}\n\\E[y_{i1}\\Delta u_{iT}] &= 0 \\\\\n\\E[y_{i2}\\Delta u_{iT}] &= 0 \\\\\n\\E[y_{i3}\\Delta u_{iT}] &= 0 \\\\\n\\vdots \\\\\n\\E[y_{i,T-2}\\Delta u_{iT}] &= 0 \\\\\n\\end{aligned}\n\\]\nAs \\(t\\) increases, the number of instruments available also increases. The total number of instruments will be quadratic in \\(T.\\) One disadvantage of this strategy should be apparent. If \\(T &lt;10\\), that may be a manageable number, but for a longer times eries, it may be necessary to restrict the number of past lags used.\nWe’ll have an instrument matrix with one row for each time period that we are instrumenting. \\(Z_i\\) is the corresponding matrix of instruments for the lagged difference.\n\\[\nZ_i =\n\\begin{bmatrix}\ny_{i,1} & 0 & 0 & 0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & y_{i,1} & y_{i,2} & 0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 0 & y_{i,1} & y_{i,2} & y_{i,3} & \\cdots & 0 & 0  \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\cdots & \\vdots & \\vdots  \\\\\n0 & 0 & 0 & 0 & \\cdots & 0 & y_{i,1} & \\cdots & y_{i,T-2}\n\\end{bmatrix}\n\\]\nThe number of rows of \\(Z_i\\) is \\(T-2.\\) The number of columns of \\(Z_i\\) depends on the length of \\(T,\\) it can grow very quickly with \\(T:\\)\n\\[\n1+ 2 + \\cdots + (T-2) = \\sum_{k=1}^{T-2} k = \\frac{(T-1)(T-2)}{2}\n\\] Rewrite the model in the vector form\n\\[\n\\Delta \\by_{i\\cdot} = \\lambda \\Delta y_{i\\cdot-1} + \\Delta u_{i\\cdot},\n\\] where\n\\[\n\\begin{split}\n\\Delta y_{i\\cdot} =\n\\begin{bmatrix}\n\\Delta y_{i3} \\\\\n\\Delta y_{i4} \\\\\n\\vdots \\\\\n\\Delta y_{iT}\n\\end{bmatrix},\n\\quad\n\\Delta y_{i\\cdot-1} =\n\\begin{bmatrix}\n\\Delta y_{i2} \\\\\n\\Delta y_{i3} \\\\\n\\vdots \\\\\n\\Delta y_{i,T-1}\n\\end{bmatrix},\n\\quad\n\\Delta u_{i\\cdot} & =\n\\begin{bmatrix}\n\\Delta u_{i3} \\\\\n\\Delta u_{i4} \\\\\n\\vdots \\\\\n\\Delta u_{iT}\n\\end{bmatrix}.  \\\\\n& \\phantom{=} (T-2) \\times 1\n\\end{split}\n\\]\nArellano and Bond (1991) suggested using a GMM approach based on all available moment conditions:\n\\[\n\\E[Z_i'\\Delta u_{i\\cdot}] = \\boldsymbol{0}\n\\]\nSample analogue\n\\[\nJ_N(\\lambda) = \\frac{1}{N} \\sum_{i=1}^N \\bZ'\\Delta u_{i\\cdot}\n\\]\n\nAlternative notation: Sometimes you see the moment conditions as \\(\\E[Z_i' u_{i\\cdot}^*] = \\boldsymbol{0}\\), where \\(u_{i\\cdot}^*\\) refers to the first difference transformed errors.\n\nThere are all together \\(\\frac{(T-1)(T-2)}{2}\\) conditions.\nThe one-step Arellano-Bond estimator is a GMM estimator:\n\\[\n\\hat{\\lambda}^{GMM} =  Q_1^{-1} \\left(\\sum_{i=1}^N \\Delta y_{i\\cdot -1}^{\\prime} Z_i\\right) A_1 \\left(\\sum_{i=1}^N Z_i' \\Delta y_i \\right)\n\\] where\n\\[\nQ_1 = \\left(\\sum_{i=1}^N \\Delta y_{i\\cdot -1}^{\\prime} Z_i\\right) A_1 \\left(\\sum_{i=1}^N Z_i'  \\Delta y_{i\\cdot -1} \\right)\n\\] and\n\\[\n\\begin{aligned}\nA_1 &= \\left( \\sum_{i=1}^N Z_i^\\prime H_i Z_i \\right)^{-1} , \\\\\nH_i &= \\E[\\Delta u_i \\Delta u_i^{\\prime}] = \\sigma_u^2\n\\begin{bmatrix}\n2 & -1 & 0 & \\cdots & 0 & 0 \\\\\n-1 & 2 & -1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 2 & -1 \\\\\n0 & 0 & 0 & \\cdots & -1 & 2 \\\\\n\\end{bmatrix} .\n\\end{aligned}\n\\]\n\\(A_1\\) is the optimal weighting matrix: \\(H_i\\) is the covariance matrix of the differenced idiosyncratic errors. Note that \\(H_i\\) implies homoskedesticity while taking into account the dynamic structure of the error term.\nThe two-step estimator is based on one-step estimator:\n\\[\n\\hat{\\lambda}^{GMM} =  Q_2^{-1} \\left(\\sum_{i=1}^N \\Delta y_{i\\cdot -1}^{\\prime} Z_i\\right) A_2 \\left(\\sum_{i=1}^N Z_i' \\Delta y_i \\right)\n\\] where\n\\[\nQ_2 = \\left(\\sum_{i=1}^N \\Delta y_{i\\cdot -1}^{\\prime} Z_i\\right) A_2 \\left(\\sum_{i=1}^N Z_i'  \\Delta y_{i\\cdot -1} \\right)\n\\] and\n\\[\n\\begin{aligned}\nA_1 &= \\left( \\sum_{i=1}^N Z_i^\\prime G_i Z_i \\right)^{-1} , \\\\\nG_i &= \\widehat{\\Delta u_i}\\; \\widehat{\\Delta u_i}^{\\prime}  .\n\\end{aligned}\n\\] That is, the only diﬀerence between one-step and two-step estimator is the weighting matrix \\(A_1\\) and \\(A_2\\). \\(A_2\\) uses the one-step residual from the one-step estimation.\nSimulation studies have suggested very modest eﬃciency gain from using the two-step version, even in the presence of considerable heteroskedasticity. More importantly the dependence of the two-step weight matrix on estimated parameters makes the usual asymptotic distribution approximations less reliable for the two step estimator. Simulation studies have shown that the asymptotic standard errors tend to be much too small for the two-step estimator. These are the reasons that much applied work has focused on the results of one-step estimator.\nThe Arellano-Bond (AB) estimator is suitable for situations with:\n\n“small \\(T\\), large \\(N\\)” panels: few time periods and many individual units\na linear functional relationship\none left-hand variable that is dynamic, depending on its own past realizations\nright-hand variables that are not strictly exogenous: correlated with past and possibly current realizations of the error\nfixed individual effects, implying unobserved heterogeneity\nheteroskedasticity and autocorrelation within individual units’ errors, but not across them\n\nThe AB estimator is usually called difference GMM.\nA potential weakness in the Arellano-Bond DPD estimator was revealed in later work by Arellano and Bover (1995) and Blundell and Bond (1998).\n\nThe lagged levels are often rather poor instruments for first differenced variables, especially if the variables are close to a random walk, i.e., \\(\\lambda\\) is close to unity.\nFor long panel (large \\(T\\)) the number of instruments increases dramatically, i.e., \\(\\frac{(T-1)(T-2)}{2}.\\) This leads to potential loss of efficiency.\nBy using the lag limits options, you may specify, for instance, that only lags 2–5 are to be used in constructing the GMM instruments.\nConsistency of the GMM estimator bases on the assumption that the trans- formed error term is not serially correlated for lags greater or equal to two, i.e., \\(\\E[\\Delta u_{it} \\Delta u_{it-k}] = 0\\) for \\(k\\ge 2.\\) It’s crucial to test whether the second-order autocorrelation is zero for all periods in the sample.\nIf a significant AR(2) statistic is encountered, the second lags of endogenous variables will not be appropriate instruments for their current values.\nWhen \\(T &gt; 3\\) and the model is over-identified, a Sargan–Hansen test can be used to test the overidentifying restrictions.\n\nTheir modification of the estimator includes lagged levels as well as lagged differences. The expanded estimator is commonly termed System GMM. The cost of the System GMM estimator involves a set of additional restrictions on the initial conditions of the process generating \\(y.\\)",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dynamic Models</span>"
    ]
  },
  {
    "objectID": "PA05_dynamic_models.html#references",
    "href": "PA05_dynamic_models.html#references",
    "title": "18  Dynamic Models",
    "section": "References",
    "text": "References\n\nChapter 12, “Serial Correlation and Heteroskedasticity in Time Series Regressions.” Introductory Econometrics: A Modern Approach, 7e by Jeffrey M. Wooldridge\nVladislav Morozov, “Interlude: Standard Dynamic Panel Estimators.” Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-dynamic-panel-iv.html#extra-endogeneity-and-the-within-estimator\n\n\n\n\n\nAnderson, T. W., and Cheng Hsiao. 1982. “Formulation and estimation of dynamic models using panel data.” Journal of Econometrics 18 (1): 47–82. https://doi.org/10.1016/0304-4076(82)90095-1.\n\n\nArellano, Manuel, and Stephen Bond. 1991. “Some Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations.” Review of Economic Studies 58: 277–97.\n\n\nArellano, Manuel, and Olympia Bover. 1995. “Another Look at the Instrumental Variable Estimation of Error-Components Models.” Journal of Econometrics 68 (1): 29–51.\n\n\nBlundell, Richard, and Stephen Bond. 1998. “Initial Conditions and Moment Restrictions in Dynamic Panel Data Models.” Journal of Econometrics 87: 115–43.\n\n\nHoltz-Eakin, Douglas, Whitney K. Newey, and Harvey S. Rosen. 1988. “Estimating Vector Autoregressions with Panel Data.” Econometrica 56 (6): 1371–95.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dynamic Models</span>"
    ]
  },
  {
    "objectID": "PA06_dynamic_models_with_pdynmc.html",
    "href": "PA06_dynamic_models_with_pdynmc.html",
    "title": "19  Dynamic Model Applications with pdynmc",
    "section": "",
    "text": "19.1 No time dummies\nModel Setup\n\\[\n\\begin{split}\nemp_{i,t}\n&= \\alpha_1 emp_{i,t-1} + \\alpha_2 emp_{i,t-2} + \\bbeta'(L) \\bx_{it} + \\lambda_t + \\eta_i + \\varepsilon_{i,t} \\\\\n&= \\alpha_1 emp_{i,t-1} + \\alpha_2 emp_{i,t-2}  \\\\\n&\\phantom{=}\\quad  + \\beta_1 wage_{i,t} + \\beta_2 wage_{i,t-1} \\\\\n&\\phantom{=}\\quad  + \\beta_3 capital_{i,t} + \\beta_4 capital_{i,t-1} + \\beta_5 capital_{i,t-2} \\\\\n&\\phantom{=}\\quad + \\beta_6 output_{i,t} + \\beta_7 output_{i,t-1} + \\beta_8 output_{i,t-2}  \\\\\n&\\phantom{=}\\quad + \\gamma_3 d_3 + \\dots + \\gamma_T d_T + \\eta_i + \\varepsilon_{i,t},\n\\end{split}\n\\]",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dynamic Model Applications with `pdynmc`</span>"
    ]
  },
  {
    "objectID": "PA06_dynamic_models_with_pdynmc.html#no-time-dummies",
    "href": "PA06_dynamic_models_with_pdynmc.html#no-time-dummies",
    "title": "19  Dynamic Model Applications with pdynmc",
    "section": "",
    "text": "library(pdynmc)\ndata(\"EmplUK\", package = \"plm\")\n# Run pdynmc\nm0 &lt;- pdynmc(\n    dat = EmplUK_log, varname.i = \"firm\", varname.t = \"year\",\n    use.mc.diff = TRUE, use.mc.lev = FALSE, use.mc.nonlin = FALSE,\n    include.y = TRUE, varname.y = \"emp\", lagTerms.y = 2,\n    fur.con = TRUE, fur.con.diff = TRUE, fur.con.lev = FALSE,\n    varname.reg.fur = c(\"wage\", \"capital\", \"output\"), lagTerms.reg.fur = c(1,2,2),\n    include.dum = FALSE,\n    w.mat = \"iid.err\", std.err = \"corrected\",\n    estimation = \"onestep\", opt.meth = \"none\" \n)\nsummary(m0)\nmtest.fct(m0)\njtest.fct(m0)\nwald.fct(m0, param = \"all\")\n&gt; summary(m0)\n\nDynamic linear panel estimation (onestep)\nGMM estimation steps: 1\n\nCoefficients:\n           Estimate Std.Err.rob z-value.rob Pr(&gt;|z.rob|)    \nL1.emp      0.72011     0.14893       4.835      &lt; 2e-16 ***\nL2.emp     -0.09164     0.05816      -1.576      0.11503    \nL0.wage    -0.61195     0.17805      -3.437      0.00059 ***\nL1.wage     0.38730     0.18285       2.118      0.03418 *  \nL0.capital  0.36127     0.05858       6.167      &lt; 2e-16 ***\nL1.capital -0.06120     0.07179      -0.852      0.39421    \nL2.capital -0.02891     0.03515      -0.822      0.41108    \nL0.output   0.65801     0.11697       5.625      &lt; 2e-16 ***\nL1.output  -0.53246     0.21663      -2.458      0.01397 *  \nL2.output   0.01351     0.14734       0.092      0.92670    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n 35 total instruments are employed to estimate 10 parameters\n 27 linear (DIF) \n 8 further controls (DIF) \n no time dummies \n \nJ-Test (overid restrictions):  42.68 with 25 DF, pvalue: 0.0152\nF-Statistic (slope coeff):  2025.03 with 10 DF, pvalue: &lt;0.001\nF-Statistic (time dummies):  no time dummies included in estimation\nWarning message:\nIn jtest.fct(object) :\n  Hansen J-Test statistic is inconsistent when error terms are non-spherical.\n&gt; mtest.fct(m0)\n\n    Arellano and Bond (1991) serial correlation test of degree 2\n\ndata:  1step GMM Estimation\nnormal = -0.50782, p-value = 0.6116\nalternative hypothesis: serial correlation of order 2 in the error terms\n&gt; wald.fct(m0, param = \"all\")\n\n    Wald test\n\ndata:  1step GMM Estimation\nchisq = 2025, df = 10, p-value &lt; 2.2e-16\nalternative hypothesis: at least one time dummy and/or slope coefficient is not equal to zero",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dynamic Model Applications with `pdynmc`</span>"
    ]
  },
  {
    "objectID": "PA06_dynamic_models_with_pdynmc.html#with-time-dummies",
    "href": "PA06_dynamic_models_with_pdynmc.html#with-time-dummies",
    "title": "19  Dynamic Model Applications with pdynmc",
    "section": "19.2 With time dummies",
    "text": "19.2 With time dummies\nm2 &lt;- pdynmc(\n    dat = EmplUK_log, varname.i = \"firm\", varname.t = \"year\",\n    use.mc.diff = TRUE, use.mc.lev = FALSE, use.mc.nonlin = FALSE,\n    include.y = TRUE, varname.y = \"emp\", lagTerms.y = 2,\n    fur.con = TRUE, fur.con.diff = TRUE, fur.con.lev = FALSE,\n    varname.reg.fur = c(\"wage\", \"capital\", \"output\"), lagTerms.reg.fur = c(1,2,2),\n    include.dum = TRUE, dum.diff = TRUE, dum.lev = FALSE, varname.dum = \"year\",\n    w.mat = \"iid.err\", std.err = \"corrected\", \n    estimation = \"twostep\", opt.meth = \"none\"\n    )\n\nsummary(m2)\nmtest.fct(m2)\njtest.fct(m2)\nwald.fct(m2, param = \"all\")\n&gt; summary(m2)\n\nDynamic linear panel estimation (twostep)\nGMM estimation steps: 2\n\nCoefficients:\n           Estimate Std.Err.rob z-value.rob Pr(&gt;|z.rob|)    \nL1.emp      0.62871     0.19341       3.251      0.00115 ** \nL2.emp     -0.06519     0.04505      -1.447      0.14790    \nL0.wage    -0.52576     0.15461      -3.401      0.00067 ***\nL1.wage     0.31129     0.20300       1.533      0.12528    \nL0.capital  0.27836     0.07280       3.824      0.00013 ***\nL1.capital  0.01410     0.09246       0.152      0.87919    \nL2.capital -0.04025     0.04327      -0.930      0.35237    \nL0.output   0.59192     0.17309       3.420      0.00063 ***\nL1.output  -0.56599     0.26110      -2.168      0.03016 *  \nL2.output   0.10054     0.16110       0.624      0.53263    \n1979        0.01122     0.01168       0.960      0.33706    \n1980        0.02307     0.02006       1.150      0.25014    \n1981       -0.02136     0.03324      -0.642      0.52087    \n1982       -0.03112     0.03397      -0.916      0.35967    \n1983       -0.01799     0.03693      -0.487      0.62626    \n1976       -0.02337     0.03661      -0.638      0.52347    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n 41 total instruments are employed to estimate 16 parameters\n 27 linear (DIF) \n 8 further controls (DIF) \n 6 time dummies (DIF) \n \nJ-Test (overid restrictions):  31.38 with 25 DF, pvalue: 0.1767\nF-Statistic (slope coeff):  269.16 with 10 DF, pvalue: &lt;0.001\nF-Statistic (time dummies):  15.43 with 6 DF, pvalue: 0.0172",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dynamic Model Applications with `pdynmc`</span>"
    ]
  },
  {
    "objectID": "PA06_dynamic_models_with_pdynmc.html#references",
    "href": "PA06_dynamic_models_with_pdynmc.html#references",
    "title": "19  Dynamic Model Applications with pdynmc",
    "section": "References",
    "text": "References\n\nGuillermo Corredor, Dynamic AR(1) Panel Estimation in R, https://bookdown.org/gcorredor/dynamic_ar1_panel/dynamic_ar1_panel.html\nMaria R. Koldasheva and Nikolai A. Popov, Dynamic Models, https://rpubs.com/Nick_Popov/thesis_dynamic_models\nPackage pdynmc resources:\n\nVignettes: https://cran.r-project.org/web/packages/pdynmc/vignettes/pdynmc-introLong.pdf\nReference: https://cran.r-project.org/web/packages/pdynmc/pdynmc.pdf\nGitHub repo: https://github.com/vincentarelbundock/modelsummary/blob/main/R/modelsummary.R",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dynamic Model Applications with `pdynmc`</span>"
    ]
  },
  {
    "objectID": "PA07_Hetero_lm.html",
    "href": "PA07_Hetero_lm.html",
    "title": "20  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "20.1 Linearity and Heterogeneity",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA07_Hetero_lm.html#linearity-and-heterogeneity",
    "href": "PA07_Hetero_lm.html#linearity-and-heterogeneity",
    "title": "20  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "20.1.1 Models with Homogeneous Slopes\nWe begin our journey where standard textbooks and first-year foundational courses in econometrics leave off. The “standard” linear models considered in such courses often assume homogeneity in individual responses to covariates (e.g., Hansen (2022)). A common cross-sectional specification is:\n\\[\ny_i = \\bbeta'\\bx_i + u_{i},\n\\tag{20.1}\\] where \\(i=1, \\dots, N\\) indexes cross-sectional units.\nIn panel data, models often include unit-specific \\((i)\\) and time-specific \\((t)\\) intercepts while maintaining a common slope vector \\(\\bbeta\\):\n\\[\ny_{it} = \\alpha_i + \\delta_t +  \\bbeta'\\bx_{it} + u_{it}.\n\\tag{20.2}\\]\n\n\n20.1.2 Heterogeneity in Slopes.\nHowever, modern economic theory rarely supports the assumption of homogeneous slopes \\(\\bbeta.\\) Theoretical models recognize that observationally identical individuals, firms, and countries can respond differently to the same stimulus. In a linear model, this requires us to consider more flexible models with heterogeneous coefficients:\n\nCross-sectional model (20.1) generalizes to\n\\[\ny_i = \\bbeta_{i}'\\bx + u_i.\n\\tag{20.3}\\]\nPanel data model (20.2) generalizes to\n\\[\ny_{it}  = \\bbeta_{it}'\\bx_{it} + u_{it}.\n\\tag{20.4}\\]\n\nSuch models are worth studying, as they naturally arise in a variety of contexts:\n\nStructural models with parametric restrictions: Certain parametric restrictions yield linear relationships in coefficients. An example is given by firm-level Cobb-Douglas production functions where firm-specific productivity differences induce heterogeneous coefficients (Combes et al. (2012); Sury (2011)).\nBinary covariates and interaction terms: if all covariates are binary and all interactions are included, a linear model encodes all treatment effects without loss of generality (see, e.g., Wooldridge (2005)).\nLog-linearized models: Nonlinear models may be approximated by linear models around a steady-state. For example, Heckman and Vytlacil (1998) demonstrate how the nonlinear Card (2001) education model simplifies to a heterogeneous linear specification after linearization.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA07_Hetero_lm.html#references",
    "href": "PA07_Hetero_lm.html#references",
    "title": "20  Linear Models with Heterogeneous Coefficients",
    "section": "References",
    "text": "References\n\nVladislav Morozov, Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-introduction.html\nVladislav Morozov, GitHub course repository, https://github.com/vladislav-morozov/econometrics-heterogeneity/tree/main/src/linear\n\n\n\n\n\nCard, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga, and Sébastien Roux. 2012. “The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random coefficient model.” Journal of Human Resources 33 (4): 974–87.\n\n\nSury, Tavneet. 2011. “Selection and Comparative Advantage in Technology Adoption.” Econometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  }
]