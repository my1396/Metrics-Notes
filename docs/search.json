[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metrics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability_theory.html",
    "href": "probability_theory.html",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "1.1 Notations\n\\(\\Omega\\): A sample space, a set of possible outcomes of a random experiment.\n\\(X\\): A random variable, a function from the sample space to the real numbers: \\(X: \\Omega \\to \\R\\).\nStochastic Process\nA stochastic process is a family of random variables, \\(\\{X(t): t\\in T\\},\\) where \\(t\\) usually denotes time. That is, at every time \\(t\\) in the set \\(T\\), a random number \\(X(t)\\) is observed.\nThe state space, \\(S\\), is the set of real values that \\(X(t)\\) can take.\nYou can think of “conditioning” as “changing the sample space.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "probability_theory.html#notations",
    "href": "probability_theory.html#notations",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "Discrete-time process: \\(T=\\{0,1,2,3\\}\\), the discrete process is \\(\\{X(0), X(1), X(2), \\dots\\}\\)\nContinuous-time process: \\(T=[0, \\infty]\\) or \\(T=[0, K]\\) for some \\(K\\).\n\n\n\n\nFrom unconditional to conditional\n\\[\n  \\P (B) = \\P(B\\mid \\Omega)\n  \\]\n\\(\\Omega\\) denotes the sample space, \\(\\P (B) = \\P(B\\mid \\Omega)\\) just means that we are looking for the probability of the event \\(B\\), out of all possible outcomes in the set \\(\\Omega.\\)\nPartition Theorem\n\\[\n  \\P(A) = \\sum_{i=1}^m \\P(A\\cap B_i) = \\sum_{i=1}^m \\P(A\\mid B_i) \\P(B_i)\n  \\]\nwhere \\(B_i, i=1,\\dots,m,\\) are a partition of \\(\\Omega.\\) The intuition behind the Partition Theorem is that the whole is the sum of its parts.\n\nA partition of \\(\\Omega\\) is a collection of mutually exclusive events whose union is \\(\\Omega.\\)\nThat is, sets \\(B_1, B_2, \\dots, B_m\\) form a partition of \\(\\Omega\\) if\n\\[\n  \\begin{split}\n  B_i \\cap B_j &= \\emptyset \\;\\text{ for all $i, j$ with $i\\ne j,$} \\\\\n  \\text{and }  \\bigcup_{i=1}^m B_i &= B_1 \\cup B_2 \\cup \\dots \\cup B_m = \\Omega.\n  \\end{split}\n  \\]\nBayes’ Theorem\nBayes’ Theorem allows us to invert a conditional statement, i.e., the express \\(\\P(B\\mid A)\\) in terms of \\(\\P(A\\mid B).\\)\nFor any events \\(A\\) and \\(B\\):\n\\[\n  \\P(B\\mid A) = \\frac{\\P(A\\cap B)}{\\P(A)} = \\frac{\\P(A\\mid B)\\P(B)}{\\P(A)}\n  \\]\nGeneralized Bayes’ Theorem\nFor any partition member \\(B_j\\),\n\\[\n  \\P(B_j\\mid A) = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\P(A)} = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\sum_{i=1}^m\\P(A\\mid B_i)\\P(B_i)}\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html",
    "href": "conditional_expectation.html",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "2.1 Generalized Adam’s Law\nIdentities for conditional expectations\n\\[\n  \\E\\left[ \\E[Y \\mid g(X)] \\mid f(g(X)) \\right] = \\E[Y\\mid f(g(X))]\n\\]\nShow that the following identify is a special case of the Generalized Adam’s Law:\n\\[\n\\E[\\E[Y\\mid X,Z] \\mid Z] = \\E[Y\\mid Z]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#generalized-adams-law",
    "href": "conditional_expectation.html#generalized-adams-law",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "Proof. If we take \\(f(g(x, z)) = z\\) and \\(g(x, z) = (x, z)\\) in the generalized Adam’s Law, we get the result.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#projection-interpretation",
    "href": "conditional_expectation.html#projection-interpretation",
    "title": "2  Conditional Expectation",
    "section": "2.2 Projection interpretation",
    "text": "2.2 Projection interpretation\nConditional expectation gives the best prediction\n\nTheorem 2.1 (Conditional expectation minimizes MSE) Suppose we have random element \\(X\\in \\Xcal\\) and random variable \\(Y\\in\\R.\\) Let \\(g(x)=\\E[Y\\mid X=x].\\) Then\n\\[\ng(x) = \\underset{f}{\\arg\\min}\\, \\E(Y-f(X))^2\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\E(Y-f(X))^2 &= \\E\\left[(Y-\\E[Y\\mid X]) + (\\E[Y\\mid X]-f(X)) \\right]^2 \\quad (\\text{plus and minus } \\E[Y\\mid X]) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2 \\\\\n& \\phantom{=}\\; + 2\\E[(Y-\\E[Y\\mid X])(\\underbrace{\\E[Y\\mid X]-f(X)}_{h(X)})] \\quad \\Bigl(\\E\\bigl[ \\bigl(Y-\\E[Y\\vert X]\\bigr) h(X) \\bigr] = 0\\Bigr) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2\n\\end{split}\n\\]\nThe first term is independent of \\(f\\), and the second term is minimized by taking \\(f(x)=\\E[Y\\mid X].\\)\n\n□\n\n\nIf we think of \\(\\E[Y\\mid X]\\) as a prediction/projection for \\(Y\\) given \\(X\\), then \\((Y-\\E[Y\\mid X])\\) is the residual of that prediction.\nIt’s helpful to think of decomposing \\(Y\\) as\n\\[\nY = \\underbrace{\\E[Y\\mid X]}_\\text{best prediction for $Y$ given $X$} + \\underbrace{(Y-E[Y\\mid X])}_\\text{residual}\n\\]\nNote that the two terms on the RHS are uncorrelated, by the projection interpretation.\nSince variance is additive for uncorrelated random variables (i.e., if \\(X\\) and \\(Y\\) are uncorrelated, then \\(\\var(X+Y)=\\var(X)+\\var(Y)\\)), we get the following theorem\n\nTheorem 2.2 (Variance decomposition with projection) For any random variable \\(X\\in \\Xcal\\) and random variable \\(Y\\in \\R,\\) we have\n\\[\n\\var(Y) = \\var(\\E[Y\\mid X]) + \\var(Y-\\E[Y\\mid X])\n\\]\n\nTheorem 2.1 tells us that \\(\\E[Y\\mid X]\\) is the best approximation of \\(Y\\) we can get from \\(X.\\) We can also think of \\(\\E[Y\\mid X]\\) as a “less random” version of \\(Y,\\) since \\(\\var(\\E[Y\\vert X]) \\le \\var(Y).\\)\nWe can say that \\(\\E[Y\\mid X]\\) only keeps the randomness in \\(Y\\) that is predictable from \\(X.\\) \\(\\E[Y\\mid X]\\) is a deterministic function of \\(X,\\) so there’s no other source of randomness in \\(\\E[Y\\mid X].\\)\n\nTheorem 2.3 (Projection interpretation) For any \\(h:\\Xcal \\to \\R,\\)\n\\[\n\\E[(Y-\\E[Y\\mid X])h(X)]=0\n\\]\n\nTheorem 2.3 says that the residual of \\(\\E[Y\\mid X]\\) is “orthogonal” to every random variable of the form \\(h(X).\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#keeping-just-what-is-needed",
    "href": "conditional_expectation.html#keeping-just-what-is-needed",
    "title": "2  Conditional Expectation",
    "section": "2.3 Keeping just what is needed",
    "text": "2.3 Keeping just what is needed\n\nTheorem 2.4 For any random variables \\(X, Y\\in \\R,\\)\n\\[\n\\E[XY] = \\E[X\\E[Y\\mid X]]\n\\]\n\nOne way to think about this is that for the purposes of computing \\(\\E [XY],\\) we only care about the randomness in \\(Y\\) that is predictable from \\(X\\).\n\nProof. \n\\[\n\\begin{split}\n\\E[XY] &= \\E[\\E[XY\\mid X]] \\quad (\\text{LIE}) \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Taking out what is known})\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof1). We can show this using the projection interpretation:\n\\[\n\\begin{split}\n\\E[XY] &= \\E\\left[ X \\left(\\E[Y\\mid X] + \\underbrace{Y-\\E[Y\\mid X]}_\\text{residuals uncorrelated with $X$} \\right)\\right] \\\\[1em]\n&= \\E[X\\E[Y\\mid X]] + \\E[X(Y-\\E[Y\\mid X])] \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Projection interpretation, } \\E[X(Y-\\E[Y\\mid X])]=0)\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof2). \n\\[\n\\begin{split}\n\\E[X\\E[Y\\mid X]] &= \\sum_x x\\E[Y\\mid X=x] \\P(X=x) \\\\\n&= \\sum_x\\sum_y xy\\P(Y=y\\mid X=x)\\P(X=x) \\\\\n&= \\sum_x\\sum_y xy \\P(Y=y, X=x)\n\\end{split}\n\\]\n\n□\n\n\nA more general case of \\(\\E[XY] = \\E[X\\E[Y\\mid X]]\\) is",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#references",
    "href": "conditional_expectation.html#references",
    "title": "2  Conditional Expectation",
    "section": "References",
    "text": "References\n\nDavid S. Rosenberg. Conditional Expectations: Review and Lots of Examples, https://davidrosenberg.github.io/ttml2021fall/background/conditional-expectation-notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "measure_theory.html",
    "href": "measure_theory.html",
    "title": "3  Measure Theory",
    "section": "",
    "text": "3.1 Definitions\nWe denote the collection of subsets, or power set, of a set \\(X\\) by \\(\\Pcal(X).\\)\nThe Cartesian product, or product, of sets \\(X, Y\\) is the collection of all ordered pairs\n\\[\nX\\times Y = \\{(x,y): x\\in X, y\\in Y\\}.\n\\]\nA topological space is a set equipped with a collection of open subsets that satisfies appropriate conditions.\nThe complement of an open set in \\(X\\) is called a closed set, and \\(\\Tcal\\) is called a topology on \\(X.\\)\nA \\(\\sigma\\)-algebra on a set \\(X\\) is a collection of subsets of a set \\(X\\) that contains \\(\\emptyset\\) and \\(X\\), and is closed under complements, finite unions, countable unions, and countable intersections.\nA measurable space \\((X, \\Acal)\\) is an non-empty set \\(X\\) equipped with a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X.\\)\nDifference between a measurable space and \\(\\sigma\\)-algebra:\nA measure \\(\\mu\\) is a countably additive, non-negative, extended real-valued function defined on a \\(\\sigma\\)-algebra.\nA measure space \\((X, \\Acal, \\mu)\\) consist of a set \\(X\\), a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X\\), and a measure \\(\\mu\\) defined on \\(\\Acal.\\) When \\(\\Acal\\) and \\(\\mu\\) are clear from the context, we will refer to the measure space \\(X\\).\nAn abstract probability space \\((\\Omega, \\Fcal, \\P)\\)\nA random variable is any function \\(X: \\Omega \\to \\Xcal.\\) We say that \\(X\\) has distribution \\(P,\\) and write \\(X\\sim P\\), if\n\\[\n\\P(X\\in B) = \\P(\\{\\omega: X(\\omega)\\in B\\}) = \\P(B)\n\\]\nWe say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue measure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any function \\(f\\).\nThe expectation of a random variable is defined as an integral with respect to \\(\\P\\):\n\\[\n\\E[X] = \\int X(\\omega)\\, \\mathrm d \\P(\\omega),\n\\]\nand\n\\[\n\\E[f(X,Y)] = \\int f(X(\\omega), Y(\\omega))\\, \\mathrm d \\P(\\omega).\n\\]\nA measure \\(\\mu\\) on a measurable space \\((X,\\Acal)\\) is a function\n\\[\n\\mu: \\Acal \\to [0, \\infty]\n\\]\nsuch that\n\\[\n\\mu \\left( \\bigcup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nA measure \\(\\mu\\) on a set \\(X\\) is\nReferences:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "measure_theory.html#definitions",
    "href": "measure_theory.html#definitions",
    "title": "3  Measure Theory",
    "section": "",
    "text": "Definition 3.1 (Topological Space) A topological space \\((X, \\Tcal)\\) is a set \\(X\\) and a collection \\(\\Tcal \\subset \\Pcal(X)\\) of subsets of \\(X,\\) called open sets, such that\n\n\\(\\emptyset, X \\in \\Tcal;\\)\nIf \\(\\{U_\\alpha \\in \\Tcal: \\alpha \\in I \\}\\) is an arbitrary collection of open sets, then their union\n\\[\n\\bigcup_{\\alpha\\in I} U_\\alpha \\in \\Tcal\n\\]\nis open;\nIf \\(\\{U_i \\in \\Tcal: i=1,2,\\dots,N \\}\\) is a finite collection of open sets,Then their intersection\n\\[\n\\bigcap_{i=1}^N U_i \\in \\Tcal\n\\]\nis open.\n\n\n\n\n\nDefinition 3.2 A \\(\\sigma\\)-algebra on a set \\(X\\) is a collection \\(\\Acal\\) of subsets of a set \\(X\\) such that:\n\n\\(\\emptyset, X \\in \\Acal;\\)\nIf \\(A\\in\\Acal\\) then \\(A^c\\in\\Acal;\\)\nIf \\(A_i\\in\\Acal\\) then \\[\n\\bigcup_{i=1}^\\infty A_i \\in \\Acal, \\quad \\bigcap_{i=1}^\\infty A_i \\in \\Acal.\n\\]\n\n\n\nExample 3.1 If \\(X\\) is a set, then \\(\\{\\emptyset,X\\}\\) and \\(\\Pcal(X)\\) are \\(\\sigma\\)-algebras on \\(X\\); they are the smallest and largest \\(\\sigma\\)-algebras on \\(X\\), respectively.\n\n\n\n\nThe complement of a measurable set is measurable, but the complement of an open set is not, in general, open, excluding special cases such as the discrete topology \\(\\Tcal = \\Pcal (X)\\)\nCountable intersections and unions of measurable sets are measurable, but only finite intersections of open sets are open while arbitrary (even uncountable) unions of open sets are open.\n\n\n\n\n\n\\(\\omega\\in \\Omega\\) is called an outcome;\n\\(A\\in \\Fcal\\) is called an event;\n\\(\\P(A)\\) is called the probability of \\(A.\\)\n\\(\\P(\\Omega)=1\\) the sum of probability of all possible outcomes is 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu(\\emptyset)=0;\\)\nIf \\(\\{A_i\\in \\Acal: i\\in \\N\\}\\) is a countable disjoint collection of sets in \\(\\Acal,\\) then\n\n\n\n\nfinite if \\(\\mu(X)&lt;\\infty,\\) and\n\\(\\sigma\\)-finite if \\(X=\\bigcup_{n=1}^\\infty A_n\\) is a countable union of measurable sets \\(A_n\\) with finite measure, \\(\\mu(A_n)&lt;\\infty.\\)\n\n\n\nJ. K. Hunter (2011). Measure Theory. Department of Mathematics, University of California at Davis. https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "AR1.html",
    "href": "AR1.html",
    "title": "4  AR(1)",
    "section": "",
    "text": "4.1 AR(1) Visualization\nTime series data often display autocorrelation, or serial correlation of the disturbances across periods.\nIf you plot the residuals and observe that the effect of a given disturbance is carried, at least in part, across periods, then it is a strong signal of serial correlation. It’s like the disturbances exhibiting a sort of “memory” over time.\nThe first-order autoregressive process, denoted AR(1), is \\[\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\] where \\(w_t\\) is a strictly stationary and ergodic white noise process with 0 mean and variance \\(\\sigma^2_w\\).\nFigure 4.1: White noise process with \\(\\sigma=20\\).\nTo illustrate the behavior of the AR(1) process, Figure 4.2 plots two simulated AR(1) processes. Each is generated using the white noise process et displayed in Figure 4.1.\nThe plot in Figure 4.2(a) sets \\(\\rho=0.5\\) and the plot in Figure 4.2(b) sets \\(\\rho=0.95\\).\nRemarks\nFigure 4.2: Simulated AR(1) processes with positive \\(\\rho\\). (a) \\(\\rho=0.5\\), (b) \\(\\rho=0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nWe have seen the cases when \\(\\rho\\) is positive, now let’s consider when \\(\\rho\\) is negative. Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.5\\), and Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.95\\,.\\)\nWe see that the sample path is very choppy when \\(\\rho\\) is negative. The different patterns for positive and negative \\(\\rho\\)’s are due to their autocorrelation functions (ACFs).\nFigure 4.3: Simulated AR(1) processes with negtive \\(\\rho\\). (a) \\(\\rho=-0.5\\), (b) \\(\\rho=-0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nPossible causes of serial correlation: Incomplete or flawed model specification. Relevant factors omitted from the time series regression are correlated across periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "AR1.html#ar1-visualization",
    "href": "AR1.html#ar1-visualization",
    "title": "4  AR(1)",
    "section": "",
    "text": "Figure 4.2(b) is more smooth than Figure 4.2(a).\nThe smoothing increases with \\(\\rho\\).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "AR1.html#mathematical-representation",
    "href": "AR1.html#mathematical-representation",
    "title": "4  AR(1)",
    "section": "4.2 Mathematical Representation",
    "text": "4.2 Mathematical Representation\nLet’s formulate an AR(1) model as follows:\n\\[\n\\begin{align}\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\end{align}\n\\tag{4.1}\\]\nwhere \\(w_t\\) is a white noise series with mean zero and variance \\(\\sigma^2_w\\). We also assume \\(|\\rho|&lt;1\\).\nWe can represent the AR(1) model as a linear combination of the innovations \\(w_t\\).\nBy iterating backwards \\(k\\) times, we get\n\\[\n\\begin{aligned}\n\\varepsilon_t &= \\rho \\,\\varepsilon_{t-1} + w_t \\\\\n&= \\rho\\, (\\rho \\, \\varepsilon_{t-2} + w_{t-1}) + w_t \\\\\n&= \\rho^2 \\varepsilon_{t-2} + \\rho w_{t-1} + w_t \\\\\n&\\quad \\vdots \\\\\n&= \\rho^k \\varepsilon_{t-k} + \\sum_{j=0}^{k-1} \\rho^j \\,w_{t-j} \\,.\n\\end{aligned}\n\\] This suggests that, by continuing to iterate backward, and provided that \\(|\\rho|&lt;1\\) and \\(\\sup_t \\text{Var}(\\varepsilon_t)&lt;\\infty\\), we can represent \\(\\varepsilon_t\\) as a linear process given by\n\\[\n\\color{#EE0000FF}{\\varepsilon_t = \\sum_{j=0}^\\infty \\rho^j \\,w_{t-j}} \\,.\n\\]\n\n\n4.2.1 Expectation\n\\(\\varepsilon_t\\) is stationary with mean zero.\n\\[\nE(\\varepsilon_t) = \\sum_{j=0}^\\infty \\rho^j \\, E(w_{t-j})\n\\]\n\n\n\n4.2.2 Autocovariance\nThe autocovariance function of the AR(1) process is \\[\n\\begin{aligned}\n\\gamma (h) &= \\text{Cov}(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E\\left[\\left(\\sum_{j=0}^\\infty \\rho^j \\,w_{t+h-j}\\right)  \\left(\\sum_{k=0}^\\infty \\rho^k \\,w_{t-k}\\right) \\right] \\\\\n&= \\sum_{l=0}^{\\infty} \\rho^{h+l} \\rho^l \\sigma_w^2 \\\\\n&= \\sigma_w^2 \\cdot \\rho^{h} \\cdot \\sum_{l=0}^{\\infty} \\rho^{2l}  \\\\\n&= \\frac{\\sigma_w^2 \\cdot \\rho^{h} }{1-\\rho^2}, \\quad h&gt;0 \\,.\n\\end{aligned}\n\\] When \\(h=0\\), \\[\n\\gamma(0) = \\frac{\\sigma_w^2}{1-\\rho^2}\n\\] is the variance of the process \\(\\text{Var}(\\varepsilon_t)\\).\nNote that\n\n\\(\\gamma(0) \\ge |\\gamma (h)|\\) for all \\(h\\). Maximum value at 0 lag.\n\\(\\gamma (h)\\) is symmetric, i.e., \\(\\gamma (-h) = \\gamma (h)\\)\n\n\n\n\n4.2.3 Autocorrelation\nThe autocorrelation function (ACF) is given by\n\\[\n\\rho(h) = \\frac{\\gamma (h)}{\\gamma (0)} = \\rho^h,\n\\] which is simply the correlation between \\(\\varepsilon_{t+h}\\) and \\(\\varepsilon_{t}\\,.\\)\nNote that \\(\\rho(h)\\) satisfies the recursion \\[\n\\rho(h) = \\rho\\cdot \\rho(h-1) \\,.\n\\]\n\nFor \\(\\rho &gt;0\\), \\(\\rho(h)=\\rho^h&gt;0\\) observations close together are positively correlated with each other. The larger the \\(\\rho\\), the larger the correlation.\nFor \\(\\rho &lt;0\\), the sign of the ACF \\(\\rho(h)=\\rho^h\\) depends on the time interval.\n\nWhen \\(h\\) is even, \\(\\rho(h)\\) is positive;\nwhen \\(h\\) is odd, \\(\\rho(h)\\) is negative.\n\nThis result means that observations at contiguous time points are negatively correlated, but observations two time points apart are positively correlated.\n\nFor example, if an observation, \\(\\varepsilon_t\\), is positive, the next observation, \\(\\varepsilon_{t+1}\\), is typically negative, and the next observation, \\(\\varepsilon_{t+2}\\), is typically positive. Thus, in this case, the sample path is very choppy.\n\n\nAnother interpretation of \\(\\rho(h)\\) is the optimal weight for scaling \\(\\varepsilon_t\\) into \\(\\varepsilon_{t+h}\\), i.e., the weight, \\(a\\), that minimizes \\(E[(\\varepsilon_{t+h} - a\\,\\varepsilon_{t})^2]\\,.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "AR_example.html",
    "href": "AR_example.html",
    "title": "5  AR – Example",
    "section": "",
    "text": "References\nThis script provides an example of autocorrelated residuals.\nDataset description\nUS Macroeconomics Data Set, Quarterly, 1950I to 2000IV, 204 Quarterly Observations\nSource: Department of Commerce, BEA website and www.economagic.com\nEmpirical model\n\\[\n\\Delta I_t =  \\beta_1 + \\beta_2 u_t + \\varepsilon_t\n\\] where\nWe remove the first two quarters due to missing value in the first observation and the change in the rate of inflation.\nRegression result for OLS.\nAutocorrelated residuals\nPlot the residuals.\nFigure 5.1 shows striking negative autocorrelation.\nNow we test the serial correlation of the residuals. \\[\n\\varepsilon_t = \\phi\\varepsilon_{t-1} + e_t\n\\]\nThe regression of the least squares residuals on their past values gives a slope of -0.4263 with a highly significant \\(t\\) ratio of -6.7078. We thus conclude that the residuals in this models are highly negatively autocorrelated.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "AR_example.html#references",
    "href": "AR_example.html#references",
    "title": "5  AR – Example",
    "section": "",
    "text": "Ex. 12.3, Chap 12 Serial Correlation, Econometric Analysis, Greene 5th Edition, pp 251.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "lag_poly.html",
    "href": "lag_poly.html",
    "title": "6  Lag polynomials",
    "section": "",
    "text": "6.1 Product of Filters\nA \\(p\\)-th degree lag polynomial is given by:\n\\[\n\\alpha(L) = \\alpha_0 + \\alpha_1L + \\cdots + \\alpha_pL^p,\n\\] where \\(L\\) is the lag operator, defined by the relation \\(L^jx_t=x_{t-j}.\\)\nWe define a filter given by \\(\\alpha(L)\\) to an input process \\(\\{x_t\\}\\), we get a weighted average of the current and \\(p\\) most recent values of the process:\n\\[\n\\begin{aligned}\n\\alpha(L)x_t &= \\alpha_0x_t + \\alpha_1Lx_t + \\alpha_2L^2x_t + \\cdots + \\alpha_pL^px_t \\\\\n&= \\alpha_0x_t + \\alpha_1x_{t-1}+ \\alpha_2x_{t-2} + \\cdots + \\alpha_px_{t-p} \\\\\n&= \\sum_{j=0}^p \\alpha_jx_{t-j}\n\\end{aligned}\n\\]\nLet \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}\\) be two arbitrary sequences of real numbers and define the sequence \\(\\{\\delta_j\\}\\) by the relation\n\\[\n\\begin{gathered}\n\\delta_0 = \\alpha_0\\beta_0, \\\\\n\\delta_1 = \\alpha_0\\beta_1 + \\alpha_1\\beta_0, \\\\\n\\delta_2 = \\alpha_0\\beta_2 + \\alpha_1\\beta_1 + \\alpha_2\\beta_0, \\\\\n\\vdots \\\\\n\\delta_j = \\alpha_0\\beta_j + \\alpha_1\\beta_{j-1} + \\alpha_2\\beta_{j-2} + \\cdots + \\alpha_{j-1}\\beta_{1} + \\alpha_{j}\\beta_{0}, \\\\\n\\vdots \\\\\n\\end{gathered}\n\\tag{6.1}\\]\nThe sequence \\(\\{\\delta_j\\}\\) created from this convoluted formula is called the convolution of \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}.\\)\nFor example, for \\(\\alpha(L)=1+\\alpha_1L\\) and \\(\\beta(L)=1+\\beta_1L\\), we have\n\\[\n\\delta(L)=(1+\\alpha_1L)(1+\\beta_1L) = 1+ (\\alpha_1+\\beta_1)L + \\alpha_1\\beta_1L^2.\n\\]\nFilters are commutative:\n\\[\n\\alpha(L)\\beta(L) = \\beta(L)\\alpha(L)\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "lag_poly.html#inverses",
    "href": "lag_poly.html#inverses",
    "title": "6  Lag polynomials",
    "section": "6.2 Inverses",
    "text": "6.2 Inverses\nThe inverse of \\(\\alpha(L)\\) is denoted as \\(\\alpha(L)^{-1}\\) or \\(1/\\alpha(L)\\):\n\\[\n\\alpha(L)\\alpha(L)^{-1}=1\n\\] Define a \\(p\\)-th degree lag polynomial \\(\\phi(L)\\)\n\\[\n\\phi(L) = 1-\\phi_1L-\\phi_2L^2-\\cdots-\\phi_pL^p.\n\\tag{6.2}\\]\nEquation 6.2 is often used to construct AR processes.\nNow let’s calculate its inverse, \\(\\psi(L) = \\phi(L)^{-1}.\\)\n\\[\n\\psi(L) = \\psi_0 + \\psi_1L + \\psi_2L^2 + \\cdots\n\\]\nBy the convolution formula (6.1), we have\n\\[\n\\begin{aligned}\n\\text{constant}:&\\quad  \\psi_0 =1 \\\\\nL: &\\quad  \\psi_1-\\psi_0\\phi_1 = 0 \\Longrightarrow \\psi_1 = \\phi_1 \\\\\nL^2: &\\quad  \\psi_2-\\psi_1\\phi_1-\\psi_0\\phi_2 = 0 \\Longrightarrow \\psi_2 = \\phi_1^2 + \\phi_2 \\\\\n\\vdots \\\\\nL^p: &\\quad  \\psi_p - \\psi_{p-1}\\phi_1 - \\psi_{p-2}\\phi_2 - \\cdots - \\psi_{1}\\phi_{p-1} - \\psi_{0}\\phi_p = 0 \\\\\nL^{p+1}: &\\quad  \\psi_{p+1} - \\psi_{p}\\phi_1 - \\psi_{p-1}\\phi_2 - \\cdots - \\psi_{2}\\phi_{p-1} - \\psi_{1}\\phi_p = 0 \\\\\n\\vdots\n\\end{aligned}\n\\]\n\nExample 6.1 Consider a 1st degree lag polynomial \\(\\phi(L)=1-\\phi L\\), its inverse \\(\\psi(L)\\) can be calculated as\n\\[\n\\begin{aligned}\n\\text{constant}: &\\quad \\psi_0 =1  \\\\\nL: &\\quad \\psi_1 - \\psi_0\\phi = 0 \\Longrightarrow \\psi_1 = \\phi \\\\\nL^2: &\\quad \\psi_2 - \\psi_1\\phi = 0 \\Longrightarrow \\psi_2 = \\phi^2 \\\\\nL^3: &\\quad \\psi_3 - \\psi_2\\phi = 0 \\Longrightarrow \\psi_3 = \\phi^4 \\\\\n\\vdots\n\\end{aligned}\n\\] \\(\\psi(L) = 1 + \\phi L + \\phi^2 L^2 + \\phi^3 L^3 + \\cdots.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "lag_poly.html#stability-condition",
    "href": "lag_poly.html#stability-condition",
    "title": "6  Lag polynomials",
    "section": "6.3 Stability Condition",
    "text": "6.3 Stability Condition\nThe solution sequence \\(\\{\\psi_j\\}\\) eventually starts declining at a geometric rate if the stability condition holds. The condition states:\nAll the roots of the \\(p\\)-th degree polynomial equation in \\(z\\)\n\\[\n\\phi(z) = 0 \\text{ where } \\phi(z) \\equiv 1-\\phi_1z-\\phi_2z^2-\\cdots-\\phi_pz^p\n\\] are greater than 1 in absolute value (lie outside the unit circle).\nEquivalently, we can consider the roots of the reciprocal polynomial defined as (basically this means inverting the order of the coefficients)\n\\[\n\\phi^*(z) = z^p\\phi(z^{-1}) = z^p - \\phi_1z^{p-1} - \\dots - \\phi_p.\n\\] The stability condition can be stated as:\nAll the roots of\n\\[\n\\phi^*(z) \\equiv z^p - \\phi_1z^{p-1} - \\dots - \\phi_p =0\n\\] are less than 1 in the absolute value (i.e., lie inside the unit circle).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "AR-MA-representation.html",
    "href": "AR-MA-representation.html",
    "title": "7  AR(1) and Its MA representation",
    "section": "",
    "text": "7.1 Case 1: \\(\\abs{\\phi}&lt;1\\)\nA first-order autoregressive process (AR(1)) satisfies the following stochastic difference equation:\n\\[\n\\begin{align}\ny_t &= c + \\phi y_{t-1} + \\varepsilon_t,  \\quad \\text{or} \\\\\ny_t - \\phi y_{t-1} &= c + \\varepsilon_t,  \\quad \\text{or} \\\\\n(1-\\phi L) y_t &= c + \\varepsilon_t,\n\\end{align}\n\\tag{7.1}\\] where \\(\\{\\varepsilon_t\\}\\) is white noise.\nIf \\(\\phi\\ne 1,\\) let \\(\\mu\\equiv c/(1-\\phi)\\) and rewrite the equation as\n\\[\n\\begin{align}\n(y_t-\\mu) - \\phi(y_{t-1}-\\mu) &= \\varepsilon_t \\quad \\text{or} \\\\\n(1-\\phi L) (y_t-\\mu) &= \\varepsilon_t.\n\\end{align}\n\\tag{7.2}\\]\n\\(\\mu\\) is the mean of \\(y_t\\) if \\(y_t\\) is covariance-stationary. For this reason, we call (7.2) a deviation-from-the-mean form. Note that the moving average is on the successive values of \\(\\{y_t\\},\\) not on \\(\\{\\varepsilon_t\\}.\\) The difference equation is called stochastic because of the presence of the random variable \\(\\varepsilon_t.\\)\nWe seek a covariance-stationary solution \\(\\{y_t\\}\\) to this stochastic difference equation. The solution depends on whether \\(\\abs{\\phi}\\) is less than, equal to, or greater than 1.\nSumming up:\nCase 1: \\(\\abs{\\phi}&lt;1\\)\nThe AR(1) process is stationary and causal, i.e., allows us to write an AR(1) as an MA(\\(\\infty\\)) using past values of \\(\\varepsilon_t.\\)\nCase 2: \\(\\abs{\\phi}&gt;1\\)\nThe AR(1) process is stationary but not causal. \\(y_t\\) is correlated with future values of \\(\\varepsilon_t.\\) This is a feasible representation but it is unnatural.\nCase 3: \\(\\abs{\\phi}=1\\)\nWe have a non-stationary process (called random walk when \\(\\phi = 1\\)) and we say that this process has a unit root.\nThe solution can be obtained easily by the use of the inverse \\((1 - \\phi L)^{-1}\\). Since this filter is absolutely summable when \\(|\\phi| &lt; 1\\), we can apply it to both sides of the AR(1) (7.2) to obtain\n\\[\n(1 - \\phi L)^{-1}(1 - \\phi L)(y_t - \\mu) = (1 - \\phi L)^{-1} \\varepsilon_t.\n\\]\nSo\n\\[\ny_t - \\mu = (1 - \\phi L)^{-1} \\varepsilon_t = (1 + \\phi L + \\phi^2 L^2 + \\cdots)\\varepsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\]\n\\[\n\\text{or} \\quad y_t = \\mu + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{7.3}\\]\nWhat we have shown is that, if \\(\\{y_t\\}\\) is a covariance-stationary solution to the stochastic difference equation (7.1) or (7.2), then \\(y_t\\) has the moving-average representation as in (7.3). Conversely, if \\(y_t\\) has the representation (7.3), then it satisfies the difference equation.\nThe condition \\(\\abs{\\phi}&lt;1,\\) which is the stability condition associated with the first-degree polynomial equation \\(1-\\phi z=0,\\) is called the stationary condition in the context of autoregressive processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "AR-MA-representation.html#case-2-absphi1",
    "href": "AR-MA-representation.html#case-2-absphi1",
    "title": "7  AR(1) and Its MA representation",
    "section": "7.2 Case 2: \\(\\abs{\\phi}>1\\)",
    "text": "7.2 Case 2: \\(\\abs{\\phi}&gt;1\\)\nBy shifting time forward by one period (i.e., by replacing \\(t\\) by \\(t+1\\)), multiplying both sides by \\(\\phi^{-1}\\), and rearranging, the stochastic difference equation (7.2) can be written as\n\\[\ny_t - \\mu = \\phi^{-1}(y_{t+1} - \\mu) - \\phi^{-1} \\varepsilon_{t+1}.\n\\] Keep this substitution,\n\\[\ny_{t+1} - \\mu = \\phi^{-1}(y_{t+2} - \\mu) - \\phi^{-1} \\varepsilon_{t+2}\n\\] then\n\\[\ny_t - \\mu = \\phi^{-2}(y_{t+2} - \\mu) - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.\n\\]\nSubstituting \\((y_{t+2} - \\mu)\\) for the corresponding next period equation:\n\\[\n\\begin{aligned}\ny_{t+2} - \\mu &= \\phi^{-1}(y_{t+3} - \\mu) - \\phi^{-1} \\varepsilon_{t+3}, \\\\\ny_t - \\mu &= \\phi^{-3}(y_{t+3} - \\mu) - \\phi^{-3}\\varepsilon_{t+3} - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.  \\\\\n\\end{aligned}\n\\]\nThen likewise for \\((y_{t+3} - \\mu)\\) and so on. Iterating \\(k\\) times, we get the following representation:\n\\[\n\\begin{aligned}\ny_t - \\mu &= \\phi^{-k}(y_{t+k} - \\mu) - \\phi^{-k}\\varepsilon_{t+k}  - \\phi^{-k+1}\\varepsilon_{t+k-1} - \\cdots - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1} \\\\\n&= \\phi^{-k}(y_{t+k} - \\mu) - \\sum_{j=1}^k \\phi^{-j}\\varepsilon_{t+j}.\n\\end{aligned}\n\\] As \\(k\\to\\infty\\), \\(\\phi^{-k}\\to 0\\), we have\n\\[\ny_t = \\mu - \\sum_{j=1}^{\\infty} \\phi^{-j} \\varepsilon_{t+j}.\n\\]\nThat is, the current value of \\(y\\) is a moving average of future values of \\(\\varepsilon\\). The infinite sum is well defined because the sequence \\(\\{\\phi^{-j}\\}\\) is absolutely summable if \\(|\\phi| &gt; 1\\). This is a feasible representation but unnatural. Moreover, it is unstable as the initial condition should now be set into the future as for example \\(y_T=0\\) as \\(T\\to\\infty\\) which is not likely to be the case.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "AR-MA-representation.html#case-3-absphi1",
    "href": "AR-MA-representation.html#case-3-absphi1",
    "title": "7  AR(1) and Its MA representation",
    "section": "7.3 Case 3: \\(\\abs{\\phi}=1\\)",
    "text": "7.3 Case 3: \\(\\abs{\\phi}=1\\)\nThe stochastic difference equation has no covariance-stationary solution. For example, if \\(\\phi = 1\\), the stochastic difference equation becomes\n\\[\n\\begin{aligned}\ny_t &= c + y_{t-1} + \\varepsilon_t \\\\\n    &= c + (c + y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\quad \\text{(since } y_{t-1} = c + y_{t-2} + \\varepsilon_{t-1}) \\\\\n    &= c + (c + (c + y_{t-3} + \\varepsilon_{t-2}) + \\varepsilon_{t-1}) + \\varepsilon_t, \\quad \\text{etc.}\n\\end{aligned}\n\\]\nRepeating this type of successive substitution \\(j\\) times, we obtain\n\\[\ny_t - y_{t-j} = c \\cdot j + (\\varepsilon_t + \\varepsilon_{t-1}+ \\varepsilon_{t-2} + \\cdots + + \\varepsilon_{t-j+1})\n\\] If \\(\\{\\varepsilon_t\\}\\) is independent white noise, \\(\\{y_t - y_{t-j}\\}\\) is a random walk with drift \\(c.\\)\nUnit root processes are non-stationary not because of the presence of a linear deterministic trend but because they are driven by a stochastic trend which makes their variance time dependent and are called Difference Stationary processes as opposed to Trend Stationary processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "AR-MA-representation.html#references",
    "href": "AR-MA-representation.html#references",
    "title": "7  AR(1) and Its MA representation",
    "section": "References",
    "text": "References\n\n§6.2, F. Hayashi (2021), Econometrics, Princeton University Press, IBSN: 9780691010182.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "01_Hetero_lm.html",
    "href": "01_Hetero_lm.html",
    "title": "8  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "8.1 Linearity and Heterogeneity",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "01_Hetero_lm.html#linearity-and-heterogeneity",
    "href": "01_Hetero_lm.html#linearity-and-heterogeneity",
    "title": "8  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "8.1.1 Models with Homogeneous Slopes\nWe begin our journey where standard textbooks and first-year foundational courses in econometrics leave off. The “standard” linear models considered in such courses often assume homogeneity in individual responses to covariates (e.g., Hansen (2022)). A common cross-sectional specification is:\n\\[\ny_i = \\bbeta'\\bx_i + u_{i},\n\\tag{8.1}\\] where \\(i=1, \\dots, N\\) indexes cross-sectional units.\nIn panel data, models often include unit-specific \\((i)\\) and time-specific \\((t)\\) intercepts while maintaining a common slope vector \\(\\bbeta\\):\n\\[\ny_{it} = \\alpha_i + \\delta_t +  \\bbeta'\\bx_{it} + u_{it}.\n\\tag{8.2}\\]\n\n\n8.1.2 Heterogeneity in Slopes.\nHowever, modern economic theory rarely supports the assumption of homogeneous slopes \\(\\bbeta.\\) Theoretical models recognize that observationally identical individuals, firms, and countries can respond differently to the same stimulus. In a linear model, this requires us to consider more flexible models with heterogeneous coefficients:\n\nCross-sectional model (8.1) generalizes to\n\\[\ny_i = \\bbeta_{i}'\\bx + u_i.\n\\tag{8.3}\\]\nPanel data model (8.2) generalizes to\n\\[\ny_{it}  = \\bbeta_{it}'\\bx_{it} + u_{it}.\n\\tag{8.4}\\]\n\nSuch models are worth studying, as they naturally arise in a variety of contexts:\n\nStructural models with parametric restrictions: Certain parametric restrictions yield linear relationships in coefficients. An example is given by firm-level Cobb-Douglas production functions where firm-specific productivity differences induce heterogeneous coefficients (Combes et al. (2012); Sury (2011)).\nBinary covariates and interaction terms: if all covariates are binary and all interactions are included, a linear model encodes all treatment effects without loss of generality (see, e.g., Wooldridge (2005)).\nLog-linearized models: Nonlinear models may be approximated by linear models around a steady-state. For example, Heckman and Vytlacil (1998) demonstrate how the nonlinear Card (2001) education model simplifies to a heterogeneous linear specification after linearization.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "01_Hetero_lm.html#references",
    "href": "01_Hetero_lm.html#references",
    "title": "8  Linear Models with Heterogeneous Coefficients",
    "section": "References",
    "text": "References\n\nVladislav Morozov, Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-introduction.html\nVladislav Morozov, GitHub course repository, https://github.com/vladislav-morozov/econometrics-heterogeneity/blob/fix/linear/src/linear/linear-introduction.qmd\n\n\n\n\n\nCard, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga, and Sébastien Roux. 2012. “The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random coefficient model.” Journal of Human Resources 33 (4): 974–87.\n\n\nSury, Tavneet. 2011. “Selection and Comparative Advantage in Technology Adoption.” Econometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  }
]