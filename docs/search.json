[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metrics Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "probability_theory.html",
    "href": "probability_theory.html",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "1.1 Notations\n\\(\\Omega\\): A sample space, a set of possible outcomes of a random experiment.\n\\(X\\): A random variable, a function from the sample space to the real numbers: \\(X: \\Omega \\to \\R\\).\nStochastic Process\nA stochastic process is a family of random variables, \\(\\{X(t): t\\in T\\},\\) where \\(t\\) usually denotes time. That is, at every time \\(t\\) in the set \\(T\\), a random number \\(X(t)\\) is observed.\nThe state space, \\(S\\), is the set of real values that \\(X(t)\\) can take.\nYou can think of “conditioning” as “changing the sample space.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "probability_theory.html#notations",
    "href": "probability_theory.html#notations",
    "title": "1  Probability Refresher",
    "section": "",
    "text": "Discrete-time process: \\(T=\\{0,1,2,3\\}\\), the discrete process is \\(\\{X(0), X(1), X(2), \\dots\\}\\)\nContinuous-time process: \\(T=[0, \\infty]\\) or \\(T=[0, K]\\) for some \\(K\\).\n\n\n\n\nFrom unconditional to conditional\n\\[\n  \\P (B) = \\P(B\\mid \\Omega)\n  \\]\n\\(\\Omega\\) denotes the sample space, \\(\\P (B) = \\P(B\\mid \\Omega)\\) just means that we are looking for the probability of the event \\(B\\), out of all possible outcomes in the set \\(\\Omega.\\)\nPartition Theorem\n\\[\n  \\P(A) = \\sum_{i=1}^m \\P(A\\cap B_i) = \\sum_{i=1}^m \\P(A\\mid B_i) \\P(B_i)\n  \\]\nwhere \\(B_i, i=1,\\dots,m,\\) are a partition of \\(\\Omega.\\) The intuition behind the Partition Theorem is that the whole is the sum of its parts.\n\nA partition of \\(\\Omega\\) is a collection of mutually exclusive events whose union is \\(\\Omega.\\)\nThat is, sets \\(B_1, B_2, \\dots, B_m\\) form a partition of \\(\\Omega\\) if\n\\[\n  \\begin{split}\n  B_i \\cap B_j &= \\emptyset \\;\\text{ for all $i, j$ with $i\\ne j,$} \\\\\n  \\text{and }  \\bigcup_{i=1}^m B_i &= B_1 \\cup B_2 \\cup \\dots \\cup B_m = \\Omega.\n  \\end{split}\n  \\]\nBayes’ Theorem\nBayes’ Theorem allows us to invert a conditional statement, i.e., the express \\(\\P(B\\mid A)\\) in terms of \\(\\P(A\\mid B).\\)\nFor any events \\(A\\) and \\(B\\):\n\\[\n  \\P(B\\mid A) = \\frac{\\P(A\\cap B)}{\\P(A)} = \\frac{\\P(A\\mid B)\\P(B)}{\\P(A)}\n  \\]\nGeneralized Bayes’ Theorem\nFor any partition member \\(B_j\\),\n\\[\n  \\P(B_j\\mid A) = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\P(A)} = \\frac{\\P(A\\mid B_j)\\P(B_j)}{\\sum_{i=1}^m\\P(A\\mid B_i)\\P(B_i)}\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Refresher</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html",
    "href": "conditional_expectation.html",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "2.1 Generalized Adam’s Law\nIdentities for conditional expectations\n\\[\n  \\E\\left[ \\E[Y \\mid g(X)] \\mid f(g(X)) \\right] = \\E[Y\\mid f(g(X))]\n\\]\nShow that the following identify is a special case of the Generalized Adam’s Law:\n\\[\n\\E[\\E[Y\\mid X,Z] \\mid Z] = \\E[Y\\mid Z]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#generalized-adams-law",
    "href": "conditional_expectation.html#generalized-adams-law",
    "title": "2  Conditional Expectation",
    "section": "",
    "text": "Proof. If we take \\(f(g(x, z)) = z\\) and \\(g(x, z) = (x, z)\\) in the generalized Adam’s Law, we get the result.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#projection-interpretation",
    "href": "conditional_expectation.html#projection-interpretation",
    "title": "2  Conditional Expectation",
    "section": "2.2 Projection interpretation",
    "text": "2.2 Projection interpretation\nConditional expectation gives the best prediction\n\nTheorem 2.1 (Conditional expectation minimizes MSE) Suppose we have random element \\(X\\in \\Xcal\\) and random variable \\(Y\\in\\R.\\) Let \\(g(x)=\\E[Y\\mid X=x].\\) Then\n\\[\ng(x) = \\underset{f}{\\arg\\min}\\, \\E(Y-f(X))^2\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\E(Y-f(X))^2 &= \\E\\left[(Y-\\E[Y\\mid X]) + (\\E[Y\\mid X]-f(X)) \\right]^2 \\quad (\\text{plus and minus } \\E[Y\\mid X]) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2 \\\\\n& \\phantom{=}\\; + 2\\E[(Y-\\E[Y\\mid X])(\\underbrace{\\E[Y\\mid X]-f(X)}_{h(X)})] \\quad \\Bigl(\\E\\bigl[ \\bigl(Y-\\E[Y\\vert X]\\bigr) h(X) \\bigr] = 0\\Bigr) \\\\\n&= \\E(Y-\\E[Y\\mid X])^2 + \\E(\\E[Y\\mid X]-f(X))^2\n\\end{split}\n\\]\nThe first term is independent of \\(f\\), and the second term is minimized by taking \\(f(x)=\\E[Y\\mid X].\\)\n\n□\n\n\nIf we think of \\(\\E[Y\\mid X]\\) as a prediction/projection for \\(Y\\) given \\(X\\), then \\((Y-\\E[Y\\mid X])\\) is the residual of that prediction.\nIt’s helpful to think of decomposing \\(Y\\) as\n\\[\nY = \\underbrace{\\E[Y\\mid X]}_\\text{best prediction for $Y$ given $X$} + \\underbrace{(Y-E[Y\\mid X])}_\\text{residual}\n\\]\nNote that the two terms on the RHS are uncorrelated, by the projection interpretation.\nSince variance is additive for uncorrelated random variables (i.e., if \\(X\\) and \\(Y\\) are uncorrelated, then \\(\\var(X+Y)=\\var(X)+\\var(Y)\\)), we get the following theorem\n\nTheorem 2.2 (Variance decomposition with projection) For any random variable \\(X\\in \\Xcal\\) and random variable \\(Y\\in \\R,\\) we have\n\\[\n\\var(Y) = \\var(\\E[Y\\mid X]) + \\var(Y-\\E[Y\\mid X])\n\\]\n\nTheorem 2.1 tells us that \\(\\E[Y\\mid X]\\) is the best approximation of \\(Y\\) we can get from \\(X.\\) We can also think of \\(\\E[Y\\mid X]\\) as a “less random” version of \\(Y,\\) since \\(\\var(\\E[Y\\vert X]) \\le \\var(Y).\\)\nWe can say that \\(\\E[Y\\mid X]\\) only keeps the randomness in \\(Y\\) that is predictable from \\(X.\\) \\(\\E[Y\\mid X]\\) is a deterministic function of \\(X,\\) so there’s no other source of randomness in \\(\\E[Y\\mid X].\\)\n\nTheorem 2.3 (Projection interpretation) For any \\(h:\\Xcal \\to \\R,\\)\n\\[\n\\E[(Y-\\E[Y\\mid X])h(X)]=0\n\\]\n\nTheorem 2.3 says that the residual of \\(\\E[Y\\mid X]\\) is “orthogonal” to every random variable of the form \\(h(X).\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#keeping-just-what-is-needed",
    "href": "conditional_expectation.html#keeping-just-what-is-needed",
    "title": "2  Conditional Expectation",
    "section": "2.3 Keeping just what is needed",
    "text": "2.3 Keeping just what is needed\n\nTheorem 2.4 For any random variables \\(X, Y\\in \\R,\\)\n\\[\n\\E[XY] = \\E[X\\E[Y\\mid X]]\n\\]\n\nOne way to think about this is that for the purposes of computing \\(\\E [XY],\\) we only care about the randomness in \\(Y\\) that is predictable from \\(X\\).\n\nProof. \n\\[\n\\begin{split}\n\\E[XY] &= \\E[\\E[XY\\mid X]] \\quad (\\text{LIE}) \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Taking out what is known})\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof1). We can show this using the projection interpretation:\n\\[\n\\begin{split}\n\\E[XY] &= \\E\\left[ X \\left(\\E[Y\\mid X] + \\underbrace{Y-\\E[Y\\mid X]}_\\text{residuals uncorrelated with $X$} \\right)\\right] \\\\[1em]\n&= \\E[X\\E[Y\\mid X]] + \\E[X(Y-\\E[Y\\mid X])] \\\\\n&= \\E[X\\E[Y\\mid X]] \\quad (\\text{Projection interpretation, } \\E[X(Y-\\E[Y\\mid X])]=0)\n\\end{split}\n\\]\n\n□\n\n\n\nProof (Alternative proof2). \n\\[\n\\begin{split}\n\\E[X\\E[Y\\mid X]] &= \\sum_x x\\E[Y\\mid X=x] \\P(X=x) \\\\\n&= \\sum_x\\sum_y xy\\P(Y=y\\mid X=x)\\P(X=x) \\\\\n&= \\sum_x\\sum_y xy \\P(Y=y, X=x)\n\\end{split}\n\\]\n\n□\n\n\nA more general case of \\(\\E[XY] = \\E[X\\E[Y\\mid X]]\\) is",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "conditional_expectation.html#references",
    "href": "conditional_expectation.html#references",
    "title": "2  Conditional Expectation",
    "section": "References",
    "text": "References\n\nDavid S. Rosenberg. Conditional Expectations: Review and Lots of Examples, https://davidrosenberg.github.io/ttml2021fall/background/conditional-expectation-notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Expectation</span>"
    ]
  },
  {
    "objectID": "measure_theory.html",
    "href": "measure_theory.html",
    "title": "3  Measure Theory",
    "section": "",
    "text": "3.1 Definitions\nWe denote the collection of subsets, or power set, of a set \\(X\\) by \\(\\Pcal(X).\\)\nThe Cartesian product, or product, of sets \\(X, Y\\) is the collection of all ordered pairs\n\\[\nX\\times Y = \\{(x,y): x\\in X, y\\in Y\\}.\n\\]\nA topological space is a set equipped with a collection of open subsets that satisfies appropriate conditions.\nThe complement of an open set in \\(X\\) is called a closed set, and \\(\\Tcal\\) is called a topology on \\(X.\\)\nA \\(\\sigma\\)-algebra on a set \\(X\\) is a collection of subsets of a set \\(X\\) that contains \\(\\emptyset\\) and \\(X\\), and is closed under complements, finite unions, countable unions, and countable intersections.\nA measurable space \\((X, \\Acal)\\) is an non-empty set \\(X\\) equipped with a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X.\\)\nDifference between a measurable space and \\(\\sigma\\)-algebra:\nA measure \\(\\mu\\) is a countably additive, non-negative, extended real-valued function defined on a \\(\\sigma\\)-algebra.\nA measure space \\((X, \\Acal, \\mu)\\) consist of a set \\(X\\), a \\(\\sigma\\)-algebra \\(\\Acal\\) on \\(X\\), and a measure \\(\\mu\\) defined on \\(\\Acal.\\) When \\(\\Acal\\) and \\(\\mu\\) are clear from the context, we will refer to the measure space \\(X\\).\nAn abstract probability space \\((\\Omega, \\Fcal, \\P)\\)\nA random variable is any function \\(X: \\Omega \\to \\Xcal.\\) We say that \\(X\\) has distribution \\(P,\\) and write \\(X\\sim P\\), if\n\\[\n\\P(X\\in B) = \\P(\\{\\omega: X(\\omega)\\in B\\}) = \\P(B)\n\\]\nWe say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue measure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any function \\(f\\).\nThe expectation of a random variable is defined as an integral with respect to \\(\\P\\):\n\\[\n\\E[X] = \\int X(\\omega)\\, \\mathrm d \\P(\\omega),\n\\]\nand\n\\[\n\\E[f(X,Y)] = \\int f(X(\\omega), Y(\\omega))\\, \\mathrm d \\P(\\omega).\n\\]\nA measure \\(\\mu\\) on a measurable space \\((X,\\Acal)\\) is a function\n\\[\n\\mu: \\Acal \\to [0, \\infty]\n\\]\nsuch that\n\\[\n\\mu \\left( \\bigcup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nA measure \\(\\mu\\) on a set \\(X\\) is\nReferences:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "measure_theory.html#definitions",
    "href": "measure_theory.html#definitions",
    "title": "3  Measure Theory",
    "section": "",
    "text": "Definition 3.1 (Topological Space) A topological space \\((X, \\Tcal)\\) is a set \\(X\\) and a collection \\(\\Tcal \\subset \\Pcal(X)\\) of subsets of \\(X,\\) called open sets, such that\n\n\\(\\emptyset, X \\in \\Tcal;\\)\nIf \\(\\{U_\\alpha \\in \\Tcal: \\alpha \\in I \\}\\) is an arbitrary collection of open sets, then their union\n\\[\n\\bigcup_{\\alpha\\in I} U_\\alpha \\in \\Tcal\n\\]\nis open;\nIf \\(\\{U_i \\in \\Tcal: i=1,2,\\dots,N \\}\\) is a finite collection of open sets,Then their intersection\n\\[\n\\bigcap_{i=1}^N U_i \\in \\Tcal\n\\]\nis open.\n\n\n\n\n\nDefinition 3.2 A \\(\\sigma\\)-algebra on a set \\(X\\) is a collection \\(\\Acal\\) of subsets of a set \\(X\\) such that:\n\n\\(\\emptyset, X \\in \\Acal;\\)\nIf \\(A\\in\\Acal\\) then \\(A^c\\in\\Acal;\\)\nIf \\(A_i\\in\\Acal\\) then \\[\n\\bigcup_{i=1}^\\infty A_i \\in \\Acal, \\quad \\bigcap_{i=1}^\\infty A_i \\in \\Acal.\n\\]\n\n\n\nExample 3.1 If \\(X\\) is a set, then \\(\\{\\emptyset,X\\}\\) and \\(\\Pcal(X)\\) are \\(\\sigma\\)-algebras on \\(X\\); they are the smallest and largest \\(\\sigma\\)-algebras on \\(X\\), respectively.\n\n\n\n\nThe complement of a measurable set is measurable, but the complement of an open set is not, in general, open, excluding special cases such as the discrete topology \\(\\Tcal = \\Pcal (X)\\)\nCountable intersections and unions of measurable sets are measurable, but only finite intersections of open sets are open while arbitrary (even uncountable) unions of open sets are open.\n\n\n\n\n\n\\(\\omega\\in \\Omega\\) is called an outcome;\n\\(A\\in \\Fcal\\) is called an event;\n\\(\\P(A)\\) is called the probability of \\(A.\\)\n\\(\\P(\\Omega)=1\\) the sum of probability of all possible outcomes is 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu(\\emptyset)=0;\\)\nIf \\(\\{A_i\\in \\Acal: i\\in \\N\\}\\) is a countable disjoint collection of sets in \\(\\Acal,\\) then\n\n\n\n\nfinite if \\(\\mu(X)&lt;\\infty,\\) and\n\\(\\sigma\\)-finite if \\(X=\\bigcup_{n=1}^\\infty A_n\\) is a countable union of measurable sets \\(A_n\\) with finite measure, \\(\\mu(A_n)&lt;\\infty.\\)\n\n\n\nJ. K. Hunter (2011). Measure Theory. Department of Mathematics, University of California at Davis. https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Measure Theory</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html",
    "href": "TS01_AR1.html",
    "title": "4  AR(1)",
    "section": "",
    "text": "4.1 AR(1) Visualization\nTime series data often display autocorrelation, or serial correlation of the disturbances across periods.\nIf you plot the residuals and observe that the effect of a given disturbance is carried, at least in part, across periods, then it is a strong signal of serial correlation. It’s like the disturbances exhibiting a sort of “memory” over time.\nThe first-order autoregressive process, denoted AR(1), is \\[\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\] where \\(w_t\\) is a strictly stationary and ergodic white noise process with 0 mean and variance \\(\\sigma^2_w\\).\nFigure 4.1: White noise process with \\(\\sigma=20\\).\nTo illustrate the behavior of the AR(1) process, Figure 4.2 plots two simulated AR(1) processes. Each is generated using the white noise process et displayed in Figure 4.1.\nThe plot in Figure 4.2(a) sets \\(\\rho=0.5\\) and the plot in Figure 4.2(b) sets \\(\\rho=0.95\\).\nRemarks\nFigure 4.2: Simulated AR(1) processes with positive \\(\\rho\\). (a) \\(\\rho=0.5\\), (b) \\(\\rho=0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nWe have seen the cases when \\(\\rho\\) is positive, now let’s consider when \\(\\rho\\) is negative. Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.5\\), and Figure 4.3(a) shows an AR(1) process with \\(\\rho=-0.95\\,.\\)\nWe see that the sample path is very choppy when \\(\\rho\\) is negative. The different patterns for positive and negative \\(\\rho\\)’s are due to their autocorrelation functions (ACFs).\nFigure 4.3: Simulated AR(1) processes with negtive \\(\\rho\\). (a) \\(\\rho=-0.5\\), (b) \\(\\rho=-0.95\\). Each is generated useing the white noise process \\(w_t\\) displayed in Figure 4.1.\nPossible causes of serial correlation: Incomplete or flawed model specification. Relevant factors omitted from the time series regression are correlated across periods.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#ar1-visualization",
    "href": "TS01_AR1.html#ar1-visualization",
    "title": "4  AR(1)",
    "section": "",
    "text": "Figure 4.2(b) is more smooth than Figure 4.2(a).\nThe smoothing increases with \\(\\rho\\).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR1.html#mathematical-representation",
    "href": "TS01_AR1.html#mathematical-representation",
    "title": "4  AR(1)",
    "section": "4.2 Mathematical Representation",
    "text": "4.2 Mathematical Representation\nLet’s formulate an AR(1) model as follows:\n\\[\n\\begin{align}\n\\varepsilon_t = \\rho \\varepsilon_{t-1} + w_t\n\\end{align}\n\\tag{4.1}\\]\nwhere \\(w_t\\) is a white noise series with mean zero and variance \\(\\sigma^2_w\\). We also assume \\(|\\rho|&lt;1\\).\nWe can represent the AR(1) model as a linear combination of the innovations \\(w_t\\).\nBy iterating backwards \\(k\\) times, we get\n\\[\n\\begin{aligned}\n\\varepsilon_t &= \\rho \\,\\varepsilon_{t-1} + w_t \\\\\n&= \\rho\\, (\\rho \\, \\varepsilon_{t-2} + w_{t-1}) + w_t \\\\\n&= \\rho^2 \\varepsilon_{t-2} + \\rho w_{t-1} + w_t \\\\\n&\\quad \\vdots \\\\\n&= \\rho^k \\varepsilon_{t-k} + \\sum_{j=0}^{k-1} \\rho^j \\,w_{t-j} \\,.\n\\end{aligned}\n\\] This suggests that, by continuing to iterate backward, and provided that \\(|\\rho|&lt;1\\) and \\(\\sup_t \\text{Var}(\\varepsilon_t)&lt;\\infty\\), we can represent \\(\\varepsilon_t\\) as a linear process given by\n\\[\n\\color{#EE0000FF}{\\varepsilon_t = \\sum_{j=0}^\\infty \\rho^j \\,w_{t-j}} \\,.\n\\]\n\n\n4.2.1 Expectation\n\\(\\varepsilon_t\\) is stationary with mean zero.\n\\[\nE(\\varepsilon_t) = \\sum_{j=0}^\\infty \\rho^j \\, E(w_{t-j})\n\\]\n\n\n\n4.2.2 Autocovariance\nThe autocovariance function of the AR(1) process is \\[\n\\begin{aligned}\n\\gamma (h) &= \\text{Cov}(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E(\\varepsilon_{t+h}, \\varepsilon_t) \\\\\n&= E\\left[\\left(\\sum_{j=0}^\\infty \\rho^j \\,w_{t+h-j}\\right)  \\left(\\sum_{k=0}^\\infty \\rho^k \\,w_{t-k}\\right) \\right] \\\\\n&= \\sum_{l=0}^{\\infty} \\rho^{h+l} \\rho^l \\sigma_w^2 \\\\\n&= \\sigma_w^2 \\cdot \\rho^{h} \\cdot \\sum_{l=0}^{\\infty} \\rho^{2l}  \\\\\n&= \\frac{\\sigma_w^2 \\cdot \\rho^{h} }{1-\\rho^2}, \\quad h&gt;0 \\,.\n\\end{aligned}\n\\] When \\(h=0\\), \\[\n\\gamma(0) = \\frac{\\sigma_w^2}{1-\\rho^2}\n\\] is the variance of the process \\(\\text{Var}(\\varepsilon_t)\\).\nNote that\n\n\\(\\gamma(0) \\ge |\\gamma (h)|\\) for all \\(h\\). Maximum value at 0 lag.\n\\(\\gamma (h)\\) is symmetric, i.e., \\(\\gamma (-h) = \\gamma (h)\\)\n\n\n\n\n4.2.3 Autocorrelation\nThe autocorrelation function (ACF) is given by\n\\[\n\\rho(h) = \\frac{\\gamma (h)}{\\gamma (0)} = \\rho^h,\n\\] which is simply the correlation between \\(\\varepsilon_{t+h}\\) and \\(\\varepsilon_{t}\\,.\\)\nNote that \\(\\rho(h)\\) satisfies the recursion \\[\n\\rho(h) = \\rho\\cdot \\rho(h-1) \\,.\n\\]\n\nFor \\(\\rho &gt;0\\), \\(\\rho(h)=\\rho^h&gt;0\\) observations close together are positively correlated with each other. The larger the \\(\\rho\\), the larger the correlation.\nFor \\(\\rho &lt;0\\), the sign of the ACF \\(\\rho(h)=\\rho^h\\) depends on the time interval.\n\nWhen \\(h\\) is even, \\(\\rho(h)\\) is positive;\nwhen \\(h\\) is odd, \\(\\rho(h)\\) is negative.\n\nThis result means that observations at contiguous time points are negatively correlated, but observations two time points apart are positively correlated.\n\nFor example, if an observation, \\(\\varepsilon_t\\), is positive, the next observation, \\(\\varepsilon_{t+1}\\), is typically negative, and the next observation, \\(\\varepsilon_{t+2}\\), is typically positive. Thus, in this case, the sample path is very choppy.\n\n\nAnother interpretation of \\(\\rho(h)\\) is the optimal weight for scaling \\(\\varepsilon_t\\) into \\(\\varepsilon_{t+h}\\), i.e., the weight, \\(a\\), that minimizes \\(E[(\\varepsilon_{t+h} - a\\,\\varepsilon_{t})^2]\\,.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR(1)</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html",
    "href": "TS01_AR_example.html",
    "title": "5  AR – Example",
    "section": "",
    "text": "References\nThis script provides an example of autocorrelated residuals.\nDataset description\nUS Macroeconomics Data Set, Quarterly, 1950I to 2000IV, 204 Quarterly Observations\nSource: Department of Commerce, BEA website and www.economagic.com\nEmpirical model\n\\[\n\\Delta I_t =  \\beta_1 + \\beta_2 u_t + \\varepsilon_t\n\\] where\nWe remove the first two quarters due to missing value in the first observation and the change in the rate of inflation.\nRegression result for OLS.\nAutocorrelated residuals\nPlot the residuals.\nFigure 5.1 shows striking negative autocorrelation.\nNow we test the serial correlation of the residuals. \\[\n\\varepsilon_t = \\phi\\varepsilon_{t-1} + e_t\n\\]\nThe regression of the least squares residuals on their past values gives a slope of -0.4263 with a highly significant \\(t\\) ratio of -6.7078. We thus conclude that the residuals in this models are highly negatively autocorrelated.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_AR_example.html#references",
    "href": "TS01_AR_example.html#references",
    "title": "5  AR – Example",
    "section": "",
    "text": "Ex. 12.3, Chap 12 Serial Correlation, Econometric Analysis, Greene 5th Edition, pp 251.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AR -- Example</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html",
    "href": "TS01_phillips-curve.html",
    "title": "6  Phillips Curve",
    "section": "",
    "text": "6.1 Static Phillips Curve\nA static Phillips curve is given by:\n\\[\ninf_t = \\beta_0 + \\beta_1\\, unem_t + u_t,\n\\]\nwhere \\(inf_t\\) is the annual inflation rate and \\(unem_t\\) is the unemployment rate.\nThis form of the Phillips curve assumes a constant natural rate of unemployment and constant inflationary expectations, and it can be used to study the contemporaneous tradeoff between inflation and unemployment.\n# data preview\ndata &lt;- read.table(\"https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/TableF5-2.txt\", header = TRUE)\ndata &lt;- data %&gt;% \n    mutate(delta_infl = infl-lag(infl))\ndata %&gt;% \n    head() %&gt;% \n    knitr::kable(digits = 5) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, latex_options=\"scale_down\") %&gt;% \n    scroll_box(width = \"100%\")\n\n\n\n\n\nYear\nqtr\nrealgdp\nrealcons\nrealinvs\nrealgovt\nrealdpi\ncpi_u\nM1\ntbilrate\nunemp\npop\ninfl\nrealint\ndelta_infl\n\n\n\n\n1950\n1\n1610.5\n1058.9\n198.1\n361.0\n1186.1\n70.6\n110.20\n1.12\n6.4\n149.461\n0.0000\n0.0000\n\n\n\n1950\n2\n1658.8\n1075.9\n220.4\n366.4\n1178.1\n71.4\n111.75\n1.17\n5.6\n150.260\n4.5071\n-3.3404\n4.5071\n\n\n1950\n3\n1723.0\n1131.0\n239.7\n359.6\n1196.5\n73.2\n112.95\n1.23\n4.6\n151.064\n9.9590\n-8.7290\n5.4519\n\n\n1950\n4\n1753.9\n1097.6\n271.8\n382.5\n1210.0\n74.9\n113.93\n1.35\n4.2\n151.871\n9.1834\n-7.8301\n-0.7756\n\n\n1951\n1\n1773.5\n1122.8\n242.9\n421.9\n1207.9\n77.3\n115.08\n1.40\n3.5\n152.393\n12.6160\n-11.2160\n3.4326\n\n\n1951\n2\n1803.7\n1091.4\n249.2\n480.1\n1225.8\n77.6\n116.19\n1.53\n3.1\n152.917\n1.5494\n-0.0161\n-11.0666\nlm_phillips_stat &lt;- lm(infl ~ unemp, data=data)\nsummary(lm_phillips_stat)\n\n\nCall:\nlm(formula = infl ~ unemp, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.654 -2.187 -0.590  1.472 12.758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.2198     0.8860   2.506   0.0130 *\nunemp         0.2995     0.1505   1.991   0.0479 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.377 on 202 degrees of freedom\nMultiple R-squared:  0.01924,   Adjusted R-squared:  0.01438 \nF-statistic: 3.962 on 1 and 202 DF,  p-value: 0.04788\nThe simple regression estimates are\n\\[\n\\begin{aligned}\n\\widehat{inf}_t &= 2.22 + 0.30\\, unem_t \\\\\n&\\phantom{={ }} (0.89)\\;\\; (.15)\n\\end{aligned}\n\\] The regression indicates a positive relationship (\\(\\hat{\\beta}_1&gt;0\\)) between inflation and unemployment at 5% significance level.\nThere are some problems with this analysis that we cannot address in detail now. The Classical Linear Model assumptions do not hold. In addition, the static Phillips curve is probably not the best model for determining whether there is a short run tradeoff between inflation and unemployment. Macroeconomists generally prefer the expectations augmented Phillips curve, while we will see shortly.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "href": "TS01_phillips-curve.html#expectations-augmented-phillips-curve",
    "title": "6  Phillips Curve",
    "section": "6.2 Expectations Augmented Phillips Curve",
    "text": "6.2 Expectations Augmented Phillips Curve\nA linear version of the expectations augmented Phillips curve can be written as\n\\[\ninf_t - inf^e_t = \\beta_1 (unem_t - \\mu_0) + e_t,\n\\]\nwhere \\(\\mu_0\\) is the natural rate of unemployment and \\(inf^3_t\\) is the expected rate of inflation formed in year \\(t-1.\\)\nThe difference between actual unemployment and the natural rate is called cyclical unemployment, while the difference between actual and expected inflation is called unanticipated inflation.\nThe error term, \\(e_t\\), is called a supply shock by macroeconomists.\nTo complete this model, we need to make an assumption about inflationary expectations. Under adaptive expectations, the expected value of current inflation depends on recently observed inflation. A particularly simple formulation is that expected inflation this year is last year’s inflation:\n\\[\ninf^e_t = inf_{t-1}\n\\]\nUnder this assumption, we can write\n\\[\ninf_t - inf_{t-1} = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] or\n\\[\n\\Delta  inf_t = \\beta_0 + \\beta_1 \\,unem_t  + e_t,\n\\] where \\(\\Delta  inf_t = inf_t - inf_{t-1}\\) and \\(\\beta_0=-\\beta_1\\mu_0.\\)\n\nlm_phillips_aug &lt;- lm(delta_infl ~ unemp, data)\nsummary(lm_phillips_aug)\n\n\nCall:\nlm(formula = delta_infl ~ unemp, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3030  -1.6596  -0.0261   1.7875   8.5274 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.51777    0.74316   0.697    0.487\nunemp       -0.09077    0.12627  -0.719    0.473\n\nResidual standard error: 2.833 on 201 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.002564,  Adjusted R-squared:  -0.002398 \nF-statistic: 0.5167 on 1 and 201 DF,  p-value: 0.4731\n\n\nThe OLS estimates are\n\\[\n\\begin{aligned}\n\\Delta\\widehat{inf}_t &= 0.52 - 0.09 \\,unem_t \\\\\n&\\phantom{={}} (0.74) \\;\\; (0.13)\n\\end{aligned}\n\\]\n\\(\\hat{\\beta}_1&lt;0\\) indicates a tradeoff between cyclical unemployment and unanticipated inflation. But the effect is statistically insignificant.\nThe natural employment rate is 5.78 percent.\n\\[\n\\mu_0 = -\\frac{\\beta_0}{\\beta_1} = \\frac{0.52}{0.09} \\approx 5.78\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phillips Curve</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html",
    "href": "TS02_lag_poly.html",
    "title": "7  Lag polynomials",
    "section": "",
    "text": "7.1 Product of Filters\nA \\(p\\)-th degree lag polynomial is given by:\n\\[\n\\alpha(L) = \\alpha_0 + \\alpha_1L + \\cdots + \\alpha_pL^p,\n\\] where \\(L\\) is the lag operator, defined by the relation \\(L^jx_t=x_{t-j}.\\)\nWe define a filter given by \\(\\alpha(L)\\) to an input process \\(\\{x_t\\}\\), we get a weighted average of the current and \\(p\\) most recent values of the process:\n\\[\n\\begin{aligned}\n\\alpha(L)x_t &= \\alpha_0x_t + \\alpha_1Lx_t + \\alpha_2L^2x_t + \\cdots + \\alpha_pL^px_t \\\\\n&= \\alpha_0x_t + \\alpha_1x_{t-1}+ \\alpha_2x_{t-2} + \\cdots + \\alpha_px_{t-p} \\\\\n&= \\sum_{j=0}^p \\alpha_jx_{t-j}\n\\end{aligned}\n\\]\nLet \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}\\) be two arbitrary sequences of real numbers and define the sequence \\(\\{\\delta_j\\}\\) by the relation\n\\[\n\\begin{gathered}\n\\delta_0 = \\alpha_0\\beta_0, \\\\\n\\delta_1 = \\alpha_0\\beta_1 + \\alpha_1\\beta_0, \\\\\n\\delta_2 = \\alpha_0\\beta_2 + \\alpha_1\\beta_1 + \\alpha_2\\beta_0, \\\\\n\\vdots \\\\\n\\delta_j = \\alpha_0\\beta_j + \\alpha_1\\beta_{j-1} + \\alpha_2\\beta_{j-2} + \\cdots + \\alpha_{j-1}\\beta_{1} + \\alpha_{j}\\beta_{0}, \\\\\n\\vdots \\\\\n\\end{gathered}\n\\tag{7.1}\\]\nThe sequence \\(\\{\\delta_j\\}\\) created from this convoluted formula is called the convolution of \\(\\{\\alpha_j\\}\\) and \\(\\{\\beta_j\\}.\\)\nFor example, for \\(\\alpha(L)=1+\\alpha_1L\\) and \\(\\beta(L)=1+\\beta_1L\\), we have\n\\[\n\\delta(L)=(1+\\alpha_1L)(1+\\beta_1L) = 1+ (\\alpha_1+\\beta_1)L + \\alpha_1\\beta_1L^2.\n\\]\nFilters are commutative:\n\\[\n\\alpha(L)\\beta(L) = \\beta(L)\\alpha(L)\n\\]",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#inverses",
    "href": "TS02_lag_poly.html#inverses",
    "title": "7  Lag polynomials",
    "section": "7.2 Inverses",
    "text": "7.2 Inverses\nThe inverse of \\(\\alpha(L)\\) is denoted as \\(\\alpha(L)^{-1}\\) or \\(1/\\alpha(L)\\):\n\\[\n\\alpha(L)\\alpha(L)^{-1}=1\n\\] Define a \\(p\\)-th degree lag polynomial \\(\\phi(L)\\)\n\\[\n\\phi(L) = 1-\\phi_1L-\\phi_2L^2-\\cdots-\\phi_pL^p.\n\\tag{7.2}\\]\nEquation 7.2 is often used to construct AR processes.\nNow let’s calculate its inverse, \\(\\psi(L) = \\phi(L)^{-1}.\\)\n\\[\n\\psi(L) = \\psi_0 + \\psi_1L + \\psi_2L^2 + \\cdots\n\\]\nBy the convolution formula (7.1), we have\n\\[\n\\begin{aligned}\n\\text{constant}:&\\quad  \\psi_0 =1 \\\\\nL: &\\quad  \\psi_1-\\psi_0\\phi_1 = 0 \\Longrightarrow \\psi_1 = \\phi_1 \\\\\nL^2: &\\quad  \\psi_2-\\psi_1\\phi_1-\\psi_0\\phi_2 = 0 \\Longrightarrow \\psi_2 = \\phi_1^2 + \\phi_2 \\\\\n\\vdots \\\\\nL^p: &\\quad  \\psi_p - \\psi_{p-1}\\phi_1 - \\psi_{p-2}\\phi_2 - \\cdots - \\psi_{1}\\phi_{p-1} - \\psi_{0}\\phi_p = 0 \\\\\nL^{p+1}: &\\quad  \\psi_{p+1} - \\psi_{p}\\phi_1 - \\psi_{p-1}\\phi_2 - \\cdots - \\psi_{2}\\phi_{p-1} - \\psi_{1}\\phi_p = 0 \\\\\n\\vdots\n\\end{aligned}\n\\]\n\nExample 7.1 Consider a 1st degree lag polynomial \\(\\phi(L)=1-\\phi L\\), its inverse \\(\\psi(L)\\) can be calculated as\n\\[\n\\begin{aligned}\n\\text{constant}: &\\quad \\psi_0 =1  \\\\\nL: &\\quad \\psi_1 - \\psi_0\\phi = 0 \\Longrightarrow \\psi_1 = \\phi \\\\\nL^2: &\\quad \\psi_2 - \\psi_1\\phi = 0 \\Longrightarrow \\psi_2 = \\phi^2 \\\\\nL^3: &\\quad \\psi_3 - \\psi_2\\phi = 0 \\Longrightarrow \\psi_3 = \\phi^4 \\\\\n\\vdots\n\\end{aligned}\n\\] \\(\\psi(L) = 1 + \\phi L + \\phi^2 L^2 + \\phi^3 L^3 + \\cdots.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_lag_poly.html#stability-condition",
    "href": "TS02_lag_poly.html#stability-condition",
    "title": "7  Lag polynomials",
    "section": "7.3 Stability Condition",
    "text": "7.3 Stability Condition\nThe solution sequence \\(\\{\\psi_j\\}\\) eventually starts declining at a geometric rate if the stability condition holds. The condition states:\nAll the roots of the \\(p\\)-th degree polynomial equation in \\(z\\)\n\\[\n\\phi(z) = 0 \\text{ where } \\phi(z) \\equiv 1-\\phi_1z-\\phi_2z^2-\\cdots-\\phi_pz^p\n\\] are greater than 1 in absolute value (lie outside the unit circle).\nEquivalently, we can consider the roots of the reciprocal polynomial defined as (basically this means inverting the order of the coefficients)\n\\[\n\\phi^*(z) = z^p\\phi(z^{-1}) = z^p - \\phi_1z^{p-1} - \\dots - \\phi_p.\n\\] The stability condition can be stated as:\nAll the roots of\n\\[\n\\phi^*(z) \\equiv z^p - \\phi_1z^{p-1} - \\dots - \\phi_p =0\n\\] are less than 1 in the absolute value (i.e., lie inside the unit circle).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lag polynomials</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html",
    "href": "TS02_AR-MA-representation.html",
    "title": "8  AR(1) and Its MA representation",
    "section": "",
    "text": "8.1 Case 1: \\(\\abs{\\phi}&lt;1\\)\nA first-order autoregressive process (AR(1)) satisfies the following stochastic difference equation:\n\\[\n\\begin{align}\ny_t &= c + \\phi y_{t-1} + \\varepsilon_t,  \\quad \\text{or} \\\\\ny_t - \\phi y_{t-1} &= c + \\varepsilon_t,  \\quad \\text{or} \\\\\n(1-\\phi L) y_t &= c + \\varepsilon_t,\n\\end{align}\n\\tag{8.1}\\] where \\(\\{\\varepsilon_t\\}\\) is white noise.\nIf \\(\\phi\\ne 1,\\) let \\(\\mu\\equiv c/(1-\\phi)\\) and rewrite the equation as\n\\[\n\\begin{align}\n(y_t-\\mu) - \\phi(y_{t-1}-\\mu) &= \\varepsilon_t \\quad \\text{or} \\\\\n(1-\\phi L) (y_t-\\mu) &= \\varepsilon_t.\n\\end{align}\n\\tag{8.2}\\]\n\\(\\mu\\) is the mean of \\(y_t\\) if \\(y_t\\) is covariance-stationary. For this reason, we call (8.2) a deviation-from-the-mean form. Note that the moving average is on the successive values of \\(\\{y_t\\},\\) not on \\(\\{\\varepsilon_t\\}.\\) The difference equation is called stochastic because of the presence of the random variable \\(\\varepsilon_t.\\)\nWe seek a covariance-stationary solution \\(\\{y_t\\}\\) to this stochastic difference equation. The solution depends on whether \\(\\abs{\\phi}\\) is less than, equal to, or greater than 1.\nSumming up:\nCase 1: \\(\\abs{\\phi}&lt;1\\)\nThe AR(1) process is stationary and causal, i.e., allows us to write an AR(1) as an MA(\\(\\infty\\)) using past values of \\(\\varepsilon_t.\\)\nCase 2: \\(\\abs{\\phi}&gt;1\\)\nThe AR(1) process is stationary but not causal. \\(y_t\\) is correlated with future values of \\(\\varepsilon_t.\\) This is a feasible representation but it is unnatural.\nCase 3: \\(\\abs{\\phi}=1\\)\nWe have a non-stationary process (called random walk when \\(\\phi = 1\\)) and we say that this process has a unit root.\nThe solution can be obtained easily by the use of the inverse \\((1 - \\phi L)^{-1}\\). Since this filter is absolutely summable when \\(|\\phi| &lt; 1\\), we can apply it to both sides of the AR(1) (8.2) to obtain\n\\[\n(1 - \\phi L)^{-1}(1 - \\phi L)(y_t - \\mu) = (1 - \\phi L)^{-1} \\varepsilon_t.\n\\]\nSo\n\\[\ny_t - \\mu = (1 - \\phi L)^{-1} \\varepsilon_t = (1 + \\phi L + \\phi^2 L^2 + \\cdots)\\varepsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\]\n\\[\n\\text{or} \\quad y_t = \\mu + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{8.3}\\]\nWhat we have shown is that, if \\(\\{y_t\\}\\) is a covariance-stationary solution to the stochastic difference equation (8.1) or (8.2), then \\(y_t\\) has the moving-average representation as in (8.3). Conversely, if \\(y_t\\) has the representation (8.3), then it satisfies the difference equation.\nThe condition \\(\\abs{\\phi}&lt;1,\\) which is the stability condition associated with the first-degree polynomial equation \\(1-\\phi z=0,\\) is called the stationary condition in the context of autoregressive processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-2-absphi1",
    "href": "TS02_AR-MA-representation.html#case-2-absphi1",
    "title": "8  AR(1) and Its MA representation",
    "section": "8.2 Case 2: \\(\\abs{\\phi}>1\\)",
    "text": "8.2 Case 2: \\(\\abs{\\phi}&gt;1\\)\nBy shifting time forward by one period (i.e., by replacing \\(t\\) by \\(t+1\\)), multiplying both sides by \\(\\phi^{-1}\\), and rearranging, the stochastic difference equation (8.2) can be written as\n\\[\ny_t - \\mu = \\phi^{-1}(y_{t+1} - \\mu) - \\phi^{-1} \\varepsilon_{t+1}.\n\\] Keep this substitution,\n\\[\ny_{t+1} - \\mu = \\phi^{-1}(y_{t+2} - \\mu) - \\phi^{-1} \\varepsilon_{t+2}\n\\] then\n\\[\ny_t - \\mu = \\phi^{-2}(y_{t+2} - \\mu) - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.\n\\]\nSubstituting \\((y_{t+2} - \\mu)\\) for the corresponding next period equation:\n\\[\n\\begin{aligned}\ny_{t+2} - \\mu &= \\phi^{-1}(y_{t+3} - \\mu) - \\phi^{-1} \\varepsilon_{t+3}, \\\\\ny_t - \\mu &= \\phi^{-3}(y_{t+3} - \\mu) - \\phi^{-3}\\varepsilon_{t+3} - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1}.  \\\\\n\\end{aligned}\n\\]\nThen likewise for \\((y_{t+3} - \\mu)\\) and so on. Iterating \\(k\\) times, we get the following representation:\n\\[\n\\begin{aligned}\ny_t - \\mu &= \\phi^{-k}(y_{t+k} - \\mu) - \\phi^{-k}\\varepsilon_{t+k}  - \\phi^{-k+1}\\varepsilon_{t+k-1} - \\cdots - \\phi^{-2}\\varepsilon_{t+2} - \\phi^{-1} \\varepsilon_{t+1} \\\\\n&= \\phi^{-k}(y_{t+k} - \\mu) - \\sum_{j=1}^k \\phi^{-j}\\varepsilon_{t+j}.\n\\end{aligned}\n\\] As \\(k\\to\\infty\\), \\(\\phi^{-k}\\to 0\\), we have\n\\[\ny_t = \\mu - \\sum_{j=1}^{\\infty} \\phi^{-j} \\varepsilon_{t+j}.\n\\]\nThat is, the current value of \\(y\\) is a moving average of future values of \\(\\varepsilon\\). The infinite sum is well defined because the sequence \\(\\{\\phi^{-j}\\}\\) is absolutely summable if \\(|\\phi| &gt; 1\\). This is a feasible representation but unnatural. Moreover, it is unstable as the initial condition should now be set into the future as for example \\(y_T=0\\) as \\(T\\to\\infty\\) which is not likely to be the case.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#case-3-absphi1",
    "href": "TS02_AR-MA-representation.html#case-3-absphi1",
    "title": "8  AR(1) and Its MA representation",
    "section": "8.3 Case 3: \\(\\abs{\\phi}=1\\)",
    "text": "8.3 Case 3: \\(\\abs{\\phi}=1\\)\nThe stochastic difference equation has no covariance-stationary solution. For example, if \\(\\phi = 1\\), the stochastic difference equation becomes\n\\[\n\\begin{aligned}\ny_t &= c + y_{t-1} + \\varepsilon_t \\\\\n    &= c + (c + y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\quad \\text{(since } y_{t-1} = c + y_{t-2} + \\varepsilon_{t-1}) \\\\\n    &= c + (c + (c + y_{t-3} + \\varepsilon_{t-2}) + \\varepsilon_{t-1}) + \\varepsilon_t, \\quad \\text{etc.}\n\\end{aligned}\n\\]\nRepeating this type of successive substitution \\(j\\) times, we obtain\n\\[\ny_t - y_{t-j} = c \\cdot j + (\\varepsilon_t + \\varepsilon_{t-1}+ \\varepsilon_{t-2} + \\cdots + + \\varepsilon_{t-j+1})\n\\] If \\(\\{\\varepsilon_t\\}\\) is independent white noise, \\(\\{y_t - y_{t-j}\\}\\) is a random walk with drift \\(c.\\)\nUnit root processes are non-stationary not because of the presence of a linear deterministic trend but because they are driven by a stochastic trend which makes their variance time dependent and are called Difference Stationary processes as opposed to Trend Stationary processes.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS02_AR-MA-representation.html#references",
    "href": "TS02_AR-MA-representation.html#references",
    "title": "8  AR(1) and Its MA representation",
    "section": "References",
    "text": "References\n\n§6.2, F. Hayashi (2021), Econometrics, Princeton University Press, IBSN: 9780691010182.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AR(1) and Its MA representation</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html",
    "href": "TS03_finite-property.html",
    "title": "9  Finite Sample Properties of OLS",
    "section": "",
    "text": "9.1 Finite Sample Properties of OLS under Classical Assumptions\nIn this section, we give a complete listing of the finite sample, or small sample, properties of OLS under standard assumptions.\nIn the notation \\(x_{tj}\\), \\(t\\) denotes the time period, and \\(j\\) is a label to indicate one of the \\(K\\) explanatory variables.\nFor example, a Finite Distributed Lag (FDL) model of order two\n\\[\ny_t = \\alpha_0 + \\delta_0 z_t + \\delta_1 z_{t-1} + \\delta_2 z_{t-2} + u_t,\n\\] is obtained by setting \\(x_{t1}=z_t,\\) \\(x_{t2}=z_{t-1},\\) and \\(x_{t3}=z_{t-2}.\\)\nFor the remaining assumptions, let \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) denote the set of all independent variables at time \\(t.\\) Further, \\(\\bX\\) denote the collection of all independent variables for all time periods. It is useful to think of \\(\\bX\\) as being an array, with \\(n\\) rows and \\(k\\) columns.\n\\[\n\\bX = \\begin{bmatrix}\n\\bx_1 \\\\\n\\bx_2 \\\\\n\\vdots \\\\\n\\bx_n\n\\end{bmatrix}\n= \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1K} \\\\\nx_{21} & x_{22} & \\cdots & x_{2K} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{nK} \\\\\n\\end{bmatrix}\n\\]\nAssumption 9.2 allows the explanatory variables to be correlated, but it rules out perfect correlation in the sample.\nAssumption 9.3 implies that the error at time \\(t\\), \\(u_t,\\) is uncorrelated with each explanatory in every time period. This is called the strict exogeneity.\nA relaxed version is contemporaneously exogenous, which only requires \\(u_t\\) to be uncorrelated with the explanatory variables dated at time \\(t\\): in conditional mean terms,\n\\[\n\\E(u_t\\mid x_{t1}, x_{t2}, \\ldots, x_{tK} ) = \\E(u_t\\mid \\bx_t) = 0.\n\\] Contemporaneously exogeneity is much weaker than strict exogeneity because it puts no restrictions on how \\(u_t\\) is related to the explanatory variables in other time periods.\nUnder Assumptions 9.1 through 9.3, the OLS estimators are unbiased:\n\\[\n\\E(\\hat{\\beta}_j\\mid \\bX) = \\E(\\hat{\\beta}_j) = \\beta_j, \\, j=0,1,\\ldots,K.\n\\]\nUnder Assumptions 9.1 through 9.5, the OLS estimators are the best linear unbiased estimators conditional on \\(\\bX.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finite Sample Properties of OLS</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#finite-sample-properties-of-ols-under-classical-assumptions",
    "href": "TS03_finite-property.html#finite-sample-properties-of-ols-under-classical-assumptions",
    "title": "9  Finite Sample Properties of OLS",
    "section": "",
    "text": "Assumption 9.1 (Linear in Parameters) The stochastic process \\(\\{(x_{t1}, x_{t2}, \\ldots, x_{tK}, y_t): t=1,2,\\ldots, n\\}\\) follows the linear model\n\\[\ny_t = \\beta_0 + \\beta_1x_{t1} + \\beta_2x_{t2} + \\cdots + + \\beta_Kx_{tK} + u_t,\n\\]\nwhere \\(\\{u_t: t=1,2,\\ldots, n\\}\\) is the sequence of errors or disturbances. Here \\(n\\) is the number of observations (time periods).\n\n\n\n\n\n\n\nAssumption 9.2 (No Perfect Collinearity) In the sample (and therefore in the underlying time series process), no independent variable is constant nor a perfect linear combination of the others.\n\n\n\nAssumption 9.3 (Zero Conditional Mean) For each \\(t\\), the expected value of the error \\(u_t,\\) given the explanatory variables for all time periods, is zero. Mathematically,\n\\[\n\\E(u_t\\mid \\bX) = 0,\\, t=1,2,\\ldots,n.\n\\]\n\n\n\n\n\nAssumption 9.4 (Homoskedasticity) Conditional on \\(\\bX,\\) the variance of \\(u_t\\) is the same for all \\(t:\\) \\(\\var(u_t\\mid \\bX)=\\var(u_t)=\\sigma^2,\\) \\(t=1,2,\\ldots,n.\\)\n\n\nAssumption 9.5 (No Serial Correlation) Conditional on \\(\\bX,\\) the errors in two different time periods are uncorrelated: \\(\\cor(u_t, u_s\\mid \\bX)=0,\\) for all \\(t\\ne s.\\)",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finite Sample Properties of OLS</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "href": "TS03_finite-property.html#violation-of-strict-exogeneity",
    "title": "9  Finite Sample Properties of OLS",
    "section": "9.2 Violation of Strict Exogeneity",
    "text": "9.2 Violation of Strict Exogeneity\nStrict exogeneity (Assumption 9.3) requires \\(u_t\\) to be uncorrelated with \\(\\bx_s\\), for \\(s=1,2,\\ldots,n.\\)\nIf the unobservables at time \\(t\\)are correlated with any of the explanatory variables in any time period, then Zero Conditional Mean fails. Two leading candidates for failure are\n\nomitted variables and\nmeasurement error in some of the regressors.\n\nOther more subtle causes:\n\nLagged effects\nEither the dependent variable of one of the independent variables is based on expectation.\nFor example, consider a static model to explain a city’s murder rate (\\(mrdrte_t\\)) in terms of police officers per capita (\\(polpc_t\\)):\n\\[\n  mrdrte_t = \\beta_0 + \\beta_1\\, polpc_t + u_t.\n  \\]\nSuppose that the city adjusts the size of its police force based on past values of the murder rate, such as\n\\[\n  polpc_{t+1} = \\delta_0 + \\delta_1 mrdrte_t + v_t.\n  \\]\nThis means that, say, \\(polpc_{t+1}\\) might be correlated with \\(u_t\\) (since a higher \\(u_t\\) leads to a higher \\(mrdrte_t\\)). If this is the case, strict exogeneity is violated.\nMathematically,\n\\[\n  \\begin{aligned}\n  \\E [u_t polpc_{t+1}] &= \\E [u_t (\\delta_0 + \\delta_1 mrdrte_t + v_t)] \\\\\n  &= \\delta_1 \\E[u_t\\, mrdrte_t] \\\\\n  &= \\delta_1 \\E[u_t (\\beta_0 + \\beta_1\\, polpc_t + u_t)] \\\\\n  &= \\delta_1 \\E[u_t^2] \\\\\n  &= \\delta_1 \\sigma^2_u.\n  \\end{aligned}\n  \\]\n\nA general representation with two explanatory variables:\n\\[\ny_t = \\beta_0 + \\beta_1z_{t1} + \\beta_2z_{t2} + u_t.\n\\] Under weak dependence, the condition sufficient for consistency of OLS is\n\\[\n\\E(u_t\\mid z_{t1}, z_{t2}) = 0\n\\] Importantly, the condition does NOT rule out correlation between, say, \\(u_{t-1}\\) and \\(z_{t1}\\). This type of correlation could arise if \\(z_{t1}\\) is related to past \\(y_{t-1}\\), such as\n\\[\nz_{t1} = \\delta_0 + \\delta_1y_{t-1} + v_t.\n\\] For example, \\(z_{t1}\\) might be a policy variable, such as monthly percentage change in the money supply, and this change might depend on last month’s rate of inflation (\\(y_{t-1}\\)). Such a mechanism generally causes \\(z_{t1}\\) and \\(u_{t-1}\\) to be correlated (as can be seen by plugging in for \\(y_{t-1}\\)). This kind of feedback is allowed under contemporaneous exogeneity.\nIn the social sciences, many explanatory variables may very well violate the strict exogeneity assumption. For example, the amount of labor input might not be strictly exogenous, as it is chosen by the farmer, and the farmer may adjust the amount of labor based on last year’s yield. Policy variables, such as growth in the money supply, expenditures on welfare, and highway speed limits, are often influenced by what has happened to the outcome variable in the past.\nThere are similar considerations in distributed lag models. Usually, we do not worry that \\(u_t\\) might be correlated with past \\(z\\) because we are controlling for past \\(z\\) in the model. But feedback from \\(u\\) to future \\(z\\) is always an issue.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finite Sample Properties of OLS</span>"
    ]
  },
  {
    "objectID": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "href": "TS03_finite-property.html#violation-of-no-serial-correlation",
    "title": "9  Finite Sample Properties of OLS",
    "section": "9.3 Violation of No Serial Correlation",
    "text": "9.3 Violation of No Serial Correlation\nWhen Assumption 9.5 is false, we say that the error suffers from serial correlation, or autocorrelation, because they are correlated across time.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Finite Sample Properties of OLS</span>"
    ]
  },
  {
    "objectID": "TS03_asymptotic-property.html",
    "href": "TS03_asymptotic-property.html",
    "title": "10  Asymptotic Properties of OLS",
    "section": "",
    "text": "Assumption 10.1 (Linear and Weak Dependence) We assume the model is exactly as in Assumption 9.1, but now we add the assumption that \\(\\{(\\bx_t, y_t): t=1, 2, \\ldots\\}\\) is stationary and weakly dependent. In particular, the law of large numbers and the central limit theorem can be applied to sample averages.\n\n\nAssumption 10.2 (No Perfect Collinearity) Same as Assumption 9.2.\n\n\nAssumption 10.3 (Zero Conditional Mean) The explanatory variables \\(\\bx_t=(x_{t1}, x_{t2}, \\ldots, x_{tk})\\) are contemporaneously exogenous:\n\\[\n\\E(u_t\\mid \\bx_t) = 0.\n\\]\n\nIt is equivalent to say that \\(u_t\\) has zero unconditional mean and is uncorrelated with each \\(x_{tj}, j=1,\\ldots,K\\):\n\\[\n\\E(u_t) = 0, \\cov(x_{tj}, u_t)=0, j=1,\\ldots,K.\n\\]\n\nAssumption 10.4 (Homoskedasticity) he errors are contemporaneously homoskedastic, that is, conditional on \\(\\bx_t,\\) the variance of \\(u_t\\) is the same for all \\(t:\\)\n\\[\n\\var(u_t\\mid \\bx_t)=\\var(u_t)=\\sigma^2,\\, t=1,2,\\ldots,n.\n\\]\n\n\nAssumption 10.5 (No Serial Correlation) Conditional on \\(\\bx_t\\) and \\(\\bx_s\\) the errors in two different time periods are uncorrelated:\n\\[\n\\cor(u_t, u_s\\mid \\bx_t, \\bx_s)=0, \\text{ for all } t\\ne s.\n\\]\n\n\nUnder Assumptions 10.1 through 10.3, the OLS estimators are consistent: \\(\\mathrm{plim}\\, \\hat{\\beta}_j = \\beta_j,\\) \\(j=1,\\ldots,K.\\)\n\nOLS estimators are consistent, but not necessarily unbiased.\nWe have weakened the sense in which the explanatory variables must be exogenous, but weak dependence is required in the underlying time series.\n\n\nUnder Assumptions 10.1 through 10.5, the OLS estimators are asymptotically normally distributed. Further, the usual OLS standard errors, \\(t\\) statistics, \\(F\\) statistics, and \\(LM\\) statistics are asymptotically valid.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Asymptotic Properties of OLS</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html",
    "href": "PA01_Hetero_lm.html",
    "title": "11  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "11.1 Linearity and Heterogeneity",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html#linearity-and-heterogeneity",
    "href": "PA01_Hetero_lm.html#linearity-and-heterogeneity",
    "title": "11  Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "11.1.1 Models with Homogeneous Slopes\nWe begin our journey where standard textbooks and first-year foundational courses in econometrics leave off. The “standard” linear models considered in such courses often assume homogeneity in individual responses to covariates (e.g., Hansen (2022)). A common cross-sectional specification is:\n\\[\ny_i = \\bbeta'\\bx_i + u_{i},\n\\tag{11.1}\\] where \\(i=1, \\dots, N\\) indexes cross-sectional units.\nIn panel data, models often include unit-specific \\((i)\\) and time-specific \\((t)\\) intercepts while maintaining a common slope vector \\(\\bbeta\\):\n\\[\ny_{it} = \\alpha_i + \\delta_t +  \\bbeta'\\bx_{it} + u_{it}.\n\\tag{11.2}\\]\n\n\n11.1.2 Heterogeneity in Slopes.\nHowever, modern economic theory rarely supports the assumption of homogeneous slopes \\(\\bbeta.\\) Theoretical models recognize that observationally identical individuals, firms, and countries can respond differently to the same stimulus. In a linear model, this requires us to consider more flexible models with heterogeneous coefficients:\n\nCross-sectional model (11.1) generalizes to\n\\[\ny_i = \\bbeta_{i}'\\bx + u_i.\n\\tag{11.3}\\]\nPanel data model (11.2) generalizes to\n\\[\ny_{it}  = \\bbeta_{it}'\\bx_{it} + u_{it}.\n\\tag{11.4}\\]\n\nSuch models are worth studying, as they naturally arise in a variety of contexts:\n\nStructural models with parametric restrictions: Certain parametric restrictions yield linear relationships in coefficients. An example is given by firm-level Cobb-Douglas production functions where firm-specific productivity differences induce heterogeneous coefficients (Combes et al. (2012); Sury (2011)).\nBinary covariates and interaction terms: if all covariates are binary and all interactions are included, a linear model encodes all treatment effects without loss of generality (see, e.g., Wooldridge (2005)).\nLog-linearized models: Nonlinear models may be approximated by linear models around a steady-state. For example, Heckman and Vytlacil (1998) demonstrate how the nonlinear Card (2001) education model simplifies to a heterogeneous linear specification after linearization.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "PA01_Hetero_lm.html#references",
    "href": "PA01_Hetero_lm.html#references",
    "title": "11  Linear Models with Heterogeneous Coefficients",
    "section": "References",
    "text": "References\n\nVladislav Morozov, Econometrics with Unobserved Heterogeneity, Course material, https://vladislav-morozov.github.io/econometrics-heterogeneity/linear/linear-introduction.html\nVladislav Morozov, GitHub course repository, https://github.com/vladislav-morozov/econometrics-heterogeneity/blob/fix/linear/src/linear/linear-introduction.qmd\n\n\n\n\n\nCard, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga, and Sébastien Roux. 2012. “The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random coefficient model.” Journal of Human Resources 33 (4): 974–87.\n\n\nSury, Tavneet. 2011. “Selection and Comparative Advantage in Technology Adoption.” Econometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "Panel Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Models with Heterogeneous Coefficients</span>"
    ]
  }
]