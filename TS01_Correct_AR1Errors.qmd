---
title: "Correcting for Serial Correaltion with Strictly Exogenous Regressors"
format: html
---

Note that the corrections mentioned below should not be used when the explanatory variables include lagged dependent variables.

The idea is to apply the generalized least-squares (GLS) method to estimate the parameters in a linear regression model in which the errors are serially correlated. Specifically, the errors are assumed to follow a first-order autoregressive process.

Assumes 

$$
y_t = \beta_1x_{t1} + \beta_2x_{t2} + \cdots + \beta_Kx_{tK} + u_t ,
$$ {#eq-model-setup}

where the errors follow the AR(1) model

$$
u_t = \rho u_{t-1} + e_t .
$$ {#eq-error}

Because the problem in (-@eq-model-setup) is serial correlation in $u_t,$ it makes sense to transform the equation to eliminate the serial correlation.

For $t\ge 2$

$$
y_{t-1} = \beta_1x_{t-1,1} + \beta_2x_{t-1,2} + \cdots + \beta_Kx_{t-1,K} + u_{t-1} ,
$$ {#eq-modelt-1}

Multiply (-@eq-modelt-1) by $\rho$ and subtract it from (-@eq-model-setup), we get

$$
y_t - \rho y_{t-1} = \beta_1(x_{t1} - \rho\, x_{t-1,1}) + \beta_2(x_{t2} - \rho\, x_{t-1,2}) + \cdots + \beta_K(x_{tK} - \rho\, x_{t-1,K}) + e_t ,
$$
where $e_t = u_t - \rho u_{t-1}.$ Hence, we have removed the serial correaltion in the error.

We rewrite the transformed equation as

$$
\tilde{y_t} = \beta_1\tilde{x}_{t1} + \beta_2\tilde{x}_{t2} + \cdots + \beta_K\tilde{x}_{tK} + e_t,\; t\ge 2,
$${#eq-tranformed-model}
where
$$
\begin{split}
\tilde{y}_t &= y_t - \rho y_{t-1} \\
\tilde{x}_{tj} &= x_{tj} - \rho x_{t-1,j},\; j=1,\ldots,K. \\
\end{split}
$$
Applying OLS to (-@eq-tranformed-model), the estimator is BLUE. Since the errors in the transformed equation are serially uncorrelated, we can apply the usual $t$ statistics and $F$ statistics for hypothesis testing.


In practice, since we don't know $\rho,$ we need to get a consistent estimator -- this is done by feasible GLS (FGLS).

Steps:

1. Run OLS on the original equation

    $$
    \text{Regress } y_t \text{ on } x_{t1},x_{t2},\ldots,x_{tK} \text{ and obtain } \hat{u}_t ,
    $$
    for $t=1,2,\ldots,n.$

2. Run the regression of 

    $$
    \hat{u}_t \text{ on } \hat{u}_{t-1} \text{ and obtain } \hat{\rho}
    $$
    for $t=2,\ldots,n$

3. Apply the transformation as in (-@eq-tranformed-model), where

    $$
    \begin{split}
    \tilde{y}_t &= y_t - \hat\rho y_{t-1}, \; t\ge 2 \\
    \tilde{x}_{tj} &= x_{tj} - \hat\rho x_{t-1,j},\; j=1,\ldots,K. \\
    \end{split}
    $$

4. Apply OLS to the transformed equation

    $$
    \text{Regress } \tilde{y}_t \text{ on } \tilde{x}_{t1}, \tilde{x}_{t2}, \ldots, \tilde{x}_{tK} ,
    $$
    and obtain new estimates of $\beta_1,$ $\ldots,$ $\beta_K.$ The usual standard errors, $t$ statistics, and $F$ statistics are asymptotically valid.


This procedure is called the Cochrane-Orcutt (CO) Estimation, which has one less observation after the transformation.


Prais-Winsten (PW) Estimation use a specific transformation at $t=1:$

$$
\sqrt{1-\rho^2} y_1  = \beta_1(\sqrt{1-\rho^2} x_{11}) + \beta_2(\sqrt{1-\rho^2} x_{12}) + \cdots + \beta_K(\sqrt{1-\rho^2} x_{1K}) + \sqrt{1-\rho^2}u_1 .
$$
Therefore the PW estimation has one more observation than the CO estimation. 

--------------------------------------------------------------------------------

In practice, both the Cochrane-Orcutt and Prais-Winsten methods are used in an **iterative scheme**. 

After we obtained the new estimates of $\beta_1,$ $\ldots,$ $\beta_K,$ after step 4, we can produce the fitted values

$$
\hat{y}_t = \hat\beta_1x_{t1} + \hat\beta_2x_{t2} + \cdots + \hat\beta_Kx_{tK} ,
$$

and then $\rho$ is reestimated using a new set of residuals based on the following regression:

$$
y_t - \hat y_t = \rho(y_{t-1}-\hat y_{t-1}) + v_t .
$$

Then transform the data using the newly obtained estimate of $\rho$ and estimate the transformed equation by OLS. 

We can repeat the whole process many times, until the estimate of $\rho$ changes by very little from the previous iteration.













